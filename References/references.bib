
@article{stevens_estimating_2013,
  title = {Estimating Absolute Methylation Levels at Single-{{CpG}} Resolution from Methylation Enrichment and Restriction Enzyme Sequencing Methods},
  volume = {23},
  issn = {1088-9051, 1549-5469},
  url = {http://genome.cshlp.org/content/23/9/1541},
  doi = {10.1101/gr.152231.112},
  abstract = {Recent advancements in sequencing-based DNA methylation profiling methods provide an unprecedented opportunity to map complete DNA methylomes. These include whole-genome bisulfite sequencing (WGBS, MethylC-seq, or BS-seq), reduced-representation bisulfite sequencing (RRBS), and enrichment-based methods such as MeDIP-seq, MBD-seq, and MRE-seq. These methods yield largely comparable results but differ significantly in extent of genomic CpG coverage, resolution, quantitative accuracy, and cost, at least while using current algorithms to interrogate the data. None of these existing methods provides single-CpG resolution, comprehensive genome-wide coverage, and cost feasibility for a typical laboratory. We introduce methylCRF, a novel conditional random fields–based algorithm that integrates methylated DNA immunoprecipitation (MeDIP-seq) and methylation-sensitive restriction enzyme (MRE-seq) sequencing data to predict DNA methylation levels at single-CpG resolution. Our method is a combined computational and experimental strategy to produce DNA methylomes of all 28 million CpGs in the human genome for a fraction ($<$10\%) of the cost of whole-genome bisulfite sequencing methods. methylCRF was benchmarked for accuracy against Infinium arrays, RRBS, WGBS sequencing, and locus-specific bisulfite sequencing performed on the same human embryonic stem cell line. methylCRF transformation of MeDIP-seq/MRE-seq was equivalent to a biological replicate of WGBS in quantification, coverage, and resolution. We used conventional bisulfite conversion, PCR, cloning, and sequencing to validate loci where our predictions do not agree with whole-genome bisulfite data, and in 11 out of 12 cases, methylCRF predictions of methylation level agree better with validated results than does whole-genome bisulfite sequencing. Therefore, methylCRF transformation of MeDIP-seq/MRE-seq data provides an accurate, inexpensive, and widely accessible strategy to create full DNA methylomes.},
  timestamp = {2015-10-06T12:13:55Z},
  langid = {english},
  number = {9},
  journaltitle = {Genome Research},
  shortjournal = {Genome Res.},
  author = {Stevens, Michael and Cheng, Jeffrey B. and Li, Daofeng and Xie, Mingchao and Hong, Chibo and Maire, Cécile L. and Ligon, Keith L. and Hirst, Martin and Marra, Marco A. and Costello, Joseph F. and Wang, Ting},
  urldate = {2014-12-09},
  date = {2013-01-09},
  pages = {1541--1553},
  keywords = {notes},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4SW5ZFHZ/Stevens et al. - 2013 - Estimating absolute methylation levels at single-C.pdf:application/pdf;methylcrf.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TTZSQDEM/methylcrf.pdf:application/pdf;crf_steven.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Z6EJP47E/crf_steven.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/J95VGB4D/1541.html:text/html},
  groups = {methods,methods},
  eprinttype = {pmid},
  eprint = {23804401}
}

@article{feng_prediction_2014,
  title = {Prediction of {{CpG}} Island Methylation Status by Integrating {{DNA}} Physicochemical Properties},
  volume = {104},
  issn = {0888-7543},
  url = {http://www.sciencedirect.com/science/article/pii/S0888754314001530},
  doi = {10.1016/j.ygeno.2014.08.011},
  abstract = {As an inheritable epigenetic modification, DNA methylation plays important roles in many biological processes. The non-uniform distribution of DNA methylation across the genome implies that characterizing genome-wide DNA methylation patterns is necessary to better understand the regulatory mechanisms of DNA methylation. Although a series of experimental technologies have been proposed, they are cost-ineffective for DNA methylation status detection. As complements to experimental techniques, computational methods will facilitate the identification of DNA methylation status. In the present study, we proposed a Naïve Bayes model to predict CpG island methylation status. In this model, DNA sequences are formulated by “pseudo trinucleotide composition” into which three DNA physicochemical properties were incorporated. It was observed by the jack-knife test that the overall success rate achieved by the proposed model in predicting the DNA methylation status was 88.22\%. This result indicates that the proposed model is a useful tool for DNA methylation status prediction.},
  timestamp = {2015-01-07T11:47:50Z},
  number = {4},
  journaltitle = {Genomics},
  shortjournal = {Genomics},
  author = {Feng, Pengmian and Chen, Wei and Lin, Hao},
  urldate = {2015-01-07},
  date = {2014-10},
  pages = {229--233},
  keywords = {CpG island,DNA Methylation,DNA physicochemical property,Naïve Bayes,notes,Pseudo trinucleotide composition},
  file = {ScienceDirect Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GQJRCWF9/Feng et al. - 2014 - Prediction of CpG island methylation status by int.pdf:application/pdf;ScienceDirect Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/EFBXESDW/S0888754314001530.html:text/html},
  groups = {CGI,CGI,CGI}
}

@article{wrzodek_linking_2012,
  title = {Linking the {{Epigenome}} to the {{Genome}}: {{Correlation}} of {{Different Features}} to {{DNA Methylation}} of {{CpG Islands}}},
  volume = {7},
  url = {http://dx.doi.org/10.1371/journal.pone.0035327},
  doi = {10.1371/journal.pone.0035327},
  shorttitle = {Linking the {{Epigenome}} to the {{Genome}}},
  abstract = {DNA methylation of CpG islands plays a crucial role in the regulation of gene expression. More than half of all human promoters contain CpG islands with a tissue-specific methylation pattern in differentiated cells. Still today, the whole process of how DNA methyltransferases determine which region should be methylated is not completely revealed. There are many hypotheses of which genomic features are correlated to the epigenome that have not yet been evaluated. Furthermore, many explorative approaches of measuring DNA methylation are limited to a subset of the genome and thus, cannot be employed, e.g., for genome-wide biomarker prediction methods. In this study, we evaluated the correlation of genetic, epigenetic and hypothesis-driven features to DNA methylation of CpG islands. To this end, various binary classifiers were trained and evaluated by cross-validation on a dataset comprising DNA methylation data for 190 CpG islands in HEPG2, HEK293, fibroblasts and leukocytes. We achieved an accuracy of up to 91\% with an MCC of 0.8 using ten-fold cross-validation and ten repetitions. With these models, we extended the existing dataset to the whole genome and thus, predicted the methylation landscape for the given cell types. The method used for these predictions is also validated on another external whole-genome dataset. Our results reveal features correlated to DNA methylation and confirm or disprove various hypotheses of DNA methylation related features. This study confirms correlations between DNA methylation and histone modifications, DNA structure, DNA sequence, genomic attributes and CpG island properties. Furthermore, the method has been validated on a genome-wide dataset from the ENCODE consortium. The developed software, as well as the predicted datasets and a web-service to compare methylation states of CpG islands are available at http://www.cogsys.cs.uni-tuebingen.de/software/dna-methylation/.},
  timestamp = {2015-01-07T12:57:45Z},
  number = {4},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  author = {Wrzodek, Clemens and Büchel, Finja and Hinselmann, Georg and Eichner, Johannes and Mittag, Florian and Zell, Andreas},
  urldate = {2015-01-07},
  date = {2012-04-30},
  pages = {e35327},
  keywords = {notes},
  file = {PLoS Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I9DDGE2T/Wrzodek et al. - 2012 - Linking the Epigenome to the Genome Correlation o.pdf:application/pdf;PLoS Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BMJFPZ2N/infodoi10.1371journal.pone.html:text/html},
  groups = {CGI,CGI,CGI}
}

@inproceedings{zheng_enhancement_2011,
  title = {Enhancement on the Predictive Power of the Prediction Model for Human Genomic {{DNA}} Methylation},
  url = {http://worldcomp-proceedings.com/proc/p2011/BIC4342.pdf},
  timestamp = {2015-01-07T13:03:24Z},
  booktitle = {International {{Conference}} on {{Bioinformatics}} and {{Computational Biology}} ({{BIOCOMP}})},
  author = {Zheng, Hao and Jiang, Shi-Wen and Wu, Hongwei},
  urldate = {2015-01-07},
  date = {2011},
  keywords = {notes},
  file = {BIC4342.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NBKXI7SU/BIC4342.pdf:application/pdf},
  groups = {CGI,CGI,CGI}
}

@article{fang_predicting_2006,
  title = {Predicting Methylation Status of {{CpG}} Islands in the Human Brain},
  volume = {22},
  issn = {1367-4803, 1460-2059},
  url = {http://bioinformatics.oxfordjournals.org/content/22/18/2204},
  doi = {10.1093/bioinformatics/btl377},
  abstract = {Motivation: Over 50\% of human genes contain CpG islands in their 5′-regions. Methylation patterns of CpG islands are involved in tissue-specific gene expression and regulation. Mis-epigenetic silencing associated with aberrant CpG island methylation is one mechanism leading to the loss of tumor suppressor functions in cancer cells. Large-scale experimental detection of DNA methylation is still both labor-intensive and time-consuming. Therefore, it is necessary to develop in silico approaches for predicting methylation status of CpG islands.
Results: Based on a recent genome-scale dataset of DNA methylation in human brain tissues, we developed a classifier called MethCGI for predicting methylation status of CpG islands using a support vector machine (SVM). Nucleotide sequence contents as well as transcription factor binding sites (TFBSs) are used as features for the classification. The method achieves specificity of 84.65\% and sensitivity of 84.32\% on the brain data, and can also correctly predict about two-third of the data from other tissues reported in the MethDB database.
Availability: An online predictor based on MethCGI is available at http://166.111.201.7/MethCGI.html
Contact:mzhang@cshl.edu
Supplementary Information: Supplementary data available at Bioinformatics online and http://166.111.201.7/help.html},
  timestamp = {2015-01-07T13:08:08Z},
  langid = {english},
  number = {18},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  author = {Fang, Fang and Fan, Shicai and Zhang, Xuegong and Zhang, Michael Q.},
  urldate = {2015-01-07},
  date = {2006-09-15},
  pages = {2204--2209},
  keywords = {notes},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/32VH4CMC/Fang et al. - 2006 - Predicting methylation status of CpG islands in th.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8G7JT9HX/2204.html:text/html},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {16837523}
}

@article{li_prediction_2014,
  title = {The Prediction of Methylation States in Human {{DNA}} Sequences Based on Hexanucleotide Composition and Feature Selection},
  volume = {6},
  issn = {1759-9660, 1759-9679},
  url = {http://xlink.rsc.org/?DOI=c3ay41962b},
  doi = {10.1039/c3ay41962b},
  timestamp = {2015-01-07T21:57:48Z},
  langid = {english},
  number = {6},
  journaltitle = {Analytical Methods},
  author = {Li, Zhanchao and Chen, Lili and Lai, Yanhua and Dai, Zong and Zou, Xiaoyong},
  urldate = {2015-01-07},
  date = {2014},
  pages = {1897},
  keywords = {notes},
  file = {doi\: - C3AY41962B:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5HUXZFMM/C3AY41962B.pdf:application/pdf},
  groups = {seq,seq,seq}
}

@article{yan_chromatin_2015,
  title = {Chromatin Modifications and Genomic Contexts Linked to Dynamic {{DNA}} Methylation Patterns across Human Cell Types},
  volume = {5},
  rights = {© 2015 Macmillan Publishers Limited. All rights reserved},
  url = {http://www.nature.com/srep/2015/150212/srep08410/full/srep08410.html},
  doi = {10.1038/srep08410},
  abstract = {DNA methylation is related closely to sequence contexts and chromatin modifications; however, their potential differences in different genomic regions across cell types remain largely unexplored. We used publicly available genome-scale DNA methylation and histone modification profiles to study their relationships among different genomic regions in human embryonic stem cells (H1), H1-derived neuronal progenitor cultured cells (NPC), and foetal fibroblasts (IMR90) using the Random forests classifier. Histone modifications achieved high accuracy in modelling DNA methylation patterns on a genome scale in the three cell types. The inclusion of sequence features helped improve accuracy only in non-promoter regions of IMR90. Furthermore, the top six feature combinations obtained by mean decrease Gini were important indicators of different DNA methylation patterns, suggesting that H3K4me2 and H3K4me3 are important indicators that are independent of genomic regions and cell types. H3K9me3 was IMR90-specific and exhibited a genomic region-specific correlation with DNA methylation. Variations of essential chromatin modification signals may effectively discriminate changes of DNA methylation between H1 and IMR90. Genes with different co-variations of epigenetic marks exhibited genomic region-specific biological relevance. This study provides an integrated strategy to identify systematically essential epigenetic and genetic elements of genomic region-specific and cell type-specific DNA methylation patterns.},
  timestamp = {2015-02-17T22:29:35Z},
  langid = {english},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci. Rep.},
  author = {Yan, Haidan and Zhang, Dongwei and Liu, Hongbo and Wei, Yanjun and Lv, Jie and Wang, Fang and Zhang, Chunlong and Wu, Qiong and Su, Jianzhong and Zhang, Yan},
  urldate = {2015-02-17},
  date = {2015-02-12},
  keywords = {Computational biology and bioinformatics,Developmental biology},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TTZ7TIJJ/Yan et al. - 2015 - Chromatin modifications and genomic contexts linke.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZWE8VPJP/srep08410.html:text/html},
  groups = {methods,methods}
}

@article{lu_predicting_2010,
  title = {Predicting {{DNA}} Methylation Status Using Word Composition},
  volume = {03},
  issn = {1937-6871, 1937-688X},
  url = {http://www.scirp.org/journal/PaperDownload.aspx?DOI=10.4236/jbise.2010.37091},
  doi = {10.4236/jbise.2010.37091},
  timestamp = {2015-01-09T08:24:55Z},
  issue = {07},
  journaltitle = {Journal of Biomedical Science and Engineering},
  author = {Lu, Lingyi},
  urldate = {2015-01-09},
  date = {2010},
  pages = {672--676},
  keywords = {notes},
  file = {JBiSE20100700004_30368053.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/K33U6U8Z/JBiSE20100700004_30368053.pdf:application/pdf},
  groups = {seq,seq,seq}
}

@article{bock_cpg_2006,
  title = {{{CpG Island Methylation}} in {{Human Lymphocytes Is Highly Correlated}} with {{DNA Sequence}}, {{Repeats}}, and {{Predicted DNA Structure}}},
  volume = {2},
  issn = {1553-7390},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1386721/},
  doi = {10.1371/journal.pgen.0020026},
  abstract = {CpG island methylation plays an important role in epigenetic gene control during mammalian development and is frequently altered in disease situations such as cancer. The majority of CpG islands is normally unmethylated, but a sizeable fraction is prone to become methylated in various cell types and pathological situations. The goal of this study is to show that a computational epigenetics approach can discriminate between CpG islands that are prone to methylation from those that remain unmethylated. We develop a bioinformatics scoring and prediction method on the basis of a set of 1,184 DNA attributes, which refer to sequence, repeats, predicted structure, CpG islands, genes, predicted binding sites, conservation, and single nucleotide polymorphisms. These attributes are scored on 132 CpG islands across the entire human Chromosome 21, whose methylation status was previously established for normal human lymphocytes. Our results show that three groups of DNA attributes, namely certain sequence patterns, specific DNA repeats, and a particular DNA structure, are each highly correlated with CpG island methylation (correlation coefficients of 0.64, 0.66, and 0.49, respectively). We predicted, and subsequently experimentally examined 12 CpG islands from human Chromosome 21 with unknown methylation patterns and found more than 90\% of our predictions to be correct. In addition, we applied our prediction method to analyzing Human Epigenome Project methylation data on human Chromosome 6 and again observed high prediction accuracy. In summary, our results suggest that DNA composition of CpG islands (sequence, repeats, and structure) plays a significant role in predisposing CpG islands for DNA methylation. This finding may have a strong impact on our understanding of changes in CpG island methylation in development and disease., DNA methylation is the only epigenetic mechanism in eukaryotes that is known to directly modify the DNA. It plays an important role for gene control during development and cell differentiation, and it is a promising therapeutic target in cancer research. While a genome-wide picture of DNA methylation patterns is currently emerging, we have only fragmentary knowledge about the linkage between DNA methylation and other genomic attributes such as DNA sequence and structure, repetitive elements, or sequence conservation. The authors fill this gap by reporting on a comprehensive bioinformatical analysis of DNA methylation on human Chromosome 21—and in part, extending to other regions of the human genome. They report new associations that will help elucidate the functions of DNA methylation along the human genome. Furthermore, the authors show that their findings can be applied to predicting DNA methylation patterns from genome sequence. Such predictions have the potential of speeding up genome-wide epigenetic profiling: It may be possible to focus experimental resources on a few selected areas while bioinformatics procedures are applied to the bulk of the genome.},
  timestamp = {2015-01-12T14:49:35Z},
  number = {3},
  journaltitle = {PLoS Genetics},
  shortjournal = {PLoS Genet},
  author = {Bock, Christoph and Paulsen, Martina and Tierling, Sascha and Mikeska, Thomas and Lengauer, Thomas and Walter, Jörn},
  urldate = {2015-01-12},
  date = {2006-03},
  keywords = {good,notes},
  file = {PubMed Central Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VJ3JQZJC/Bock et al. - 2006 - CpG Island Methylation in Human Lymphocytes Is Hig.pdf:application/pdf},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {16520826},
  pmcid = {PMC1386721}
}

@article{isse_detailed_2009,
  title = {Detailed Methylation Prediction of {{CpG}} Islands on Human Chromosome 21},
  url = {http://www.wseas.us/e-library/conferences/2009/prague/MCBC/MCBC21.pdf},
  timestamp = {2015-01-16T16:16:05Z},
  journaltitle = {10th WSEAS International Conference on Mathematics},
  author = {Isse, Ali and Huseyin, Seker},
  urldate = {2015-01-12},
  date = {2009},
  keywords = {bad,notes},
  file = {- MCBC21.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NM8HED67/MCBC21.pdf:application/pdf},
  groups = {CGI,CGI,CGI}
}

@article{fan_histone_2008,
  title = {Histone Methylation Marks Play Important Roles in Predicting the Methylation Status of {{CpG}} Islands},
  volume = {374},
  issn = {0006291X},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0006291X08014101},
  doi = {10.1016/j.bbrc.2008.07.077},
  timestamp = {2015-01-13T20:04:02Z},
  langid = {english},
  number = {3},
  journaltitle = {Biochemical and Biophysical Research Communications},
  author = {Fan, Shicai and Zhang, Michael Q. and Zhang, Xuegong},
  urldate = {2015-01-13},
  date = {2008-09},
  pages = {559--564},
  keywords = {notes},
  file = {1-s2.0-S0006291X08014101-main.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4STA9F57/1-s2.0-S0006291X08014101-main.pdf:application/pdf},
  groups = {CGI,CGI,CGI}
}

@article{bhasin_prediction_2005,
  title = {Prediction of Methylated {{CpGs}} in {{DNA}} Sequences Using a Support Vector Machine},
  volume = {579},
  issn = {00145793},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0014579305008185},
  doi = {10.1016/j.febslet.2005.07.002},
  timestamp = {2015-01-15T22:10:32Z},
  langid = {english},
  number = {20},
  journaltitle = {FEBS Letters},
  author = {Bhasin, Manoj and Zhang, Hong and Reinherz, Ellis L. and Reche, Pedro A.},
  urldate = {2015-01-15},
  date = {2005-08},
  pages = {4302--4308},
  keywords = {notes},
  file = {1-s2.0-S0014579305008185-main.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WKT45G6Z/1-s2.0-S0014579305008185-main.pdf:application/pdf},
  groups = {seq,seq,seq}
}

@article{previti_profile_2009,
  title = {Profile Analysis and Prediction of Tissue-Specific {{CpG}} Island Methylation Classes},
  volume = {10},
  rights = {2009 Previti et al; licensee BioMed Central Ltd.},
  issn = {1471-2105},
  url = {http://www.biomedcentral.com/1471-2105/10/116/abstract},
  doi = {10.1186/1471-2105-10-116},
  abstract = {The computational prediction of DNA methylation has become an important topic in the recent years due to its role in the epigenetic control of normal and cancer-related processes. While previous prediction approaches focused merely on differences between methylated and unmethylated DNA sequences, recent experimental results have shown the presence of much more complex patterns of methylation across tissues and time in the human genome. These patterns are only partially described by a binary model of DNA methylation. In this work we propose a novel approach, based on profile analysis of tissue-specific methylation that uncovers significant differences in the sequences of CpG islands (CGIs) that predispose them to a tissue- specific methylation pattern.
PMID: 19383127},
  timestamp = {2015-01-20T20:08:52Z},
  langid = {english},
  number = {1},
  journaltitle = {BMC Bioinformatics},
  author = {Previti, Christopher and Harari, Oscar and Zwir, Igor and del Val, Coral},
  urldate = {2015-01-20},
  date = {2009-04-21},
  pages = {116},
  keywords = {bad,notes},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/92BF6586/Previti et al. - 2009 - Profile analysis and prediction of tissue-specific.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/N9VC5IV9/116.html:text/html},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {19383127}
}

@article{zhou_prediction_2012,
  title = {Prediction of Methylation {{CpGs}} and Their Methylation Degrees in Human {{DNA}} Sequences},
  volume = {42},
  issn = {00104825},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0010482511002484},
  doi = {10.1016/j.compbiomed.2011.12.008},
  timestamp = {2015-01-20T20:04:34Z},
  langid = {english},
  number = {4},
  journaltitle = {Computers in Biology and Medicine},
  author = {Zhou, Xuan and Li, Zhanchao and Dai, Zong and Zou, Xiaoyong},
  urldate = {2015-01-20},
  date = {2012-04},
  pages = {408--413},
  keywords = {notes},
  file = {1-s2.0-S0010482511002484-main.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/X36PNVEN/1-s2.0-S0010482511002484-main.pdf:application/pdf},
  groups = {seq,seq,seq}
}

@article{liu_idna-methyl:_2015,
  title = {{{iDNA}}-{{Methyl}}: {{Identifying DNA}} Methylation Sites via Pseudo Trinucleotide Composition},
  volume = {474},
  issn = {0003-2697},
  url = {http://www.sciencedirect.com/science/article/pii/S0003269714005697},
  doi = {10.1016/j.ab.2014.12.009},
  shorttitle = {{{iDNA}}-{{Methyl}}},
  abstract = {Predominantly occurring on cytosine, DNA methylation is a process by which cells can modify their DNAs to change the expression of gene products. It plays very important roles in life development but also in forming nearly all types of cancer. Therefore, knowledge of DNA methylation sites is significant for both basic research and drug development. Given an uncharacterized DNA sequence containing many cytosine residues, which one can be methylated, and which one cannot? With the avalanche of DNA sequences generated in the postgenomic age, it is highly desired to develop computational methods for accurately identifying the methylation sites in DNA. Using the trinucleotide composition, pseudo amino acid components, as well as dataset-optimizing technique, we developed a new predictor called “iDNA-Methyl” that has achieved remarkably higher success rates in identifying the DNA methylation sites than the existing predictors. A user-friendly web-server for the new predictor has been established at http://www.jci-bioinfo.cn/iDNA-Methyl, by which users can easily get their desired results. We anticipate that the web-server predictor will become a very useful high throughput tool for basic research and drug development, and that the novel approach and technique can also be used to investigate many other DNA-related problems and genome analysis.},
  timestamp = {2016-05-18T23:41:59Z},
  journaltitle = {Analytical Biochemistry},
  shortjournal = {Analytical Biochemistry},
  author = {Liu, Zi and Xiao, Xuan and Qiu, Wang-Ren and Chou, Kuo-Chen},
  urldate = {2015-01-20},
  date = {2015-04-01},
  pages = {69--77},
  keywords = {3->1 codon conversion,bad,DNA Methylation,Neighborhood cleaning rule,notes,Pseudo amino acid components,Synthetic minority over-sampling technique,Target-jackknife cross-validation},
  file = {1-s2.0-S0010482511002484-main.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/SK7PRFPU/1-s2.0-S0010482511002484-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/6ECHIPNR/S0003269714005697.html:text/html},
  groups = {seq,seq,seq}
}

@article{ernst_large-scale_2015,
  title = {Large-Scale Imputation of Epigenomic Datasets for Systematic Annotation of Diverse Human Tissues},
  volume = {33},
  rights = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1087-0156},
  url = {http://www.nature.com/nbt/journal/v33/n4/abs/nbt.3157.html},
  doi = {10.1038/nbt.3157},
  abstract = {With hundreds of epigenomic maps, the opportunity arises to exploit the correlated nature of epigenetic signals, across both marks and samples, for large-scale prediction of additional datasets. Here, we undertake epigenome imputation by leveraging such correlations through an ensemble of regression trees. We impute 4,315 high-resolution signal maps, of which 26\% are also experimentally observed. Imputed signal tracks show overall similarity to observed signals and surpass experimental datasets in consistency, recovery of gene annotations and enrichment for disease-associated variants. We use the imputed data to detect low-quality experimental datasets, to find genomic sites with unexpected epigenomic signals, to define high-priority marks for new experiments and to delineate chromatin states in 127 reference epigenomes spanning diverse tissues and cell types. Our imputed datasets provide the most comprehensive human regulatory region annotation to date, and our approach and the ChromImpute software constitute a useful complement to large-scale experimental mapping of epigenomic information.},
  timestamp = {2016-05-18T23:30:52Z},
  langid = {english},
  number = {4},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotech},
  author = {Ernst, Jason and Kellis, Manolis},
  urldate = {2016-05-18},
  date = {2015-04},
  pages = {364--376},
  keywords = {notes},
  file = {Ernst and Kellis - 2015 - Large-scale imputation of epigenomic datasets for .pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VEKBVBE9/Ernst and Kellis - 2015 - Large-scale imputation of epigenomic datasets for .pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VWB7DNTF/nbt.3157.html:text/html},
  groups = {bio}
}

@article{jones_functions_2012,
  title = {Functions of {{DNA}} Methylation: Islands, Start Sites, Gene Bodies and Beyond},
  volume = {13},
  rights = {© 2012 Nature Publishing Group},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v13/n7/full/nrg3230.html},
  doi = {10.1038/nrg3230},
  shorttitle = {Functions of {{DNA}} Methylation},
  abstract = {DNA methylation is frequently described as a 'silencing' epigenetic mark, and indeed this function of 5-methylcytosine was originally proposed in the 1970s. Now, thanks to improved genome-scale mapping of methylation, we can evaluate DNA methylation in different genomic contexts: transcriptional start sites with or without CpG islands, in gene bodies, at regulatory elements and at repeat sequences. The emerging picture is that the function of DNA methylation seems to vary with context, and the relationship between DNA methylation and transcription is more nuanced than we realized at first. Improving our understanding of the functions of DNA methylation is necessary for interpreting changes in this mark that are observed in diseases such as cancer.},
  timestamp = {2015-03-23T19:42:18Z},
  langid = {english},
  number = {7},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Jones, Peter A.},
  urldate = {2015-03-23},
  date = {2012-07},
  pages = {484--492},
  keywords = {good},
  file = {notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/SF3KV796/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WWIXHW2T/Jones - 2012 - Functions of DNA methylation islands, start sites.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WB2WW64H/nrg3230.html:text/html},
  groups = {bio}
}

@article{posfai_predictive_1989,
  title = {Predictive Motifs Derived from Cytosine Methyltransferases},
  volume = {17},
  url = {http://nar.oxfordjournals.org/content/17/7/2421.short},
  timestamp = {2015-12-18T21:01:31Z},
  number = {7},
  journaltitle = {Nucleic Acids Research},
  author = {Pósfai, János and Bhagwat, Ashok S. and Pósfai, György and Roberts, Richard J.},
  urldate = {2015-12-18},
  date = {1989},
  pages = {2421--2435},
  file = {Nucl. Acids Res.-1989-Pósfai-2421-35.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/JXHS38JT/Nucl. Acids Res.-1989-Pósfai-2421-35.pdf:application/pdf},
  groups = {seq,seq,seq}
}

@article{zheng_cpgimethpred:_2013,
  title = {{{CpGIMethPred}}: Computational Model for Predicting Methylation Status of {{CpG}} Islands in Human Genome},
  volume = {6},
  rights = {2013 Zheng et al.; licensee BioMed Central Ltd.},
  issn = {1755-8794},
  url = {http://www.biomedcentral.com/1755-8794/6/S1/S13/abstract},
  doi = {10.1186/1755-8794-6-S1-S13},
  shorttitle = {{{CpGIMethPred}}},
  abstract = {DNA methylation is an inheritable chemical modification of cytosine, and represents one of the most important epigenetic events. Computational prediction of the DNA methylation status can be employed to speed up the genome-wide methylation profiling, and to identify the key features that are correlated with various methylation patterns. Here, we develop CpGIMethPred, the support vector machine-based models to predict the methylation status of the CpG islands in the human genome under normal conditions. The features for prediction include those that have been previously demonstrated effective (CpG island specific attributes, DNA sequence composition patterns, DNA structure patterns, distribution patterns of conserved transcription factor binding sites and conserved elements, and histone methylation status) as well as those that have not been extensively explored but are likely to contribute additional information from a biological point of view (nucleosome positioning propensities, gene functions, and histone acetylation status). Statistical tests are performed to identify the features that are significantly correlated with the methylation status of the CpG islands, and principal component analysis is then performed to decorrelate the selected features. Data from the Human Epigenome Project (HEP) are used to train, validate and test the predictive models. Specifically, the models are trained and validated by using the DNA methylation data obtained in the CD4 lymphocytes, and are then tested for generalizability using the DNA methylation data obtained in the other 11 normal tissues and cell types. Our experiments have shown that (1) an eight-dimensional feature space that is selected via the principal component analysis and that combines all categories of information is effective for predicting the CpG island methylation status, (2) by incorporating the information regarding the nucleosome positioning, gene functions, and histone acetylation, the models can achieve higher specificity and accuracy than the existing models while maintaining a comparable sensitivity measure, (3) the histone modification (methylation and acetylation) information contributes significantly to the prediction, without which the performance of the models deteriorate, and, (4) the predictive models generalize well to different tissues and cell types. The developed program CpGIMethPred is freely available at http://users.ece.gatech.edu/\textasciitilde{}hzheng7/CGIMetPred.zip.
PMID: 23369266},
  timestamp = {2014-08-30T19:16:23Z},
  langid = {english},
  issue = {Suppl 1},
  journaltitle = {BMC Medical Genomics},
  author = {Zheng, Hao and Wu, Hongwei and Li, Jinping and Jiang, Shi-Wen},
  urldate = {2014-08-30},
  date = {2013-01-23},
  pages = {S13},
  keywords = {good,notes},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/EBGDJSXE/Zheng et al. - 2013 - CpGIMethPred computational model for predicting m.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Z3FS3KDZ/S13.html:text/html},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {23369266}
}

@article{guo_single-cell_2013,
  title = {Single-Cell Methylome Landscapes of Mouse Embryonic Stem Cells and Early Embryos Analyzed Using Reduced Representation Bisulfite Sequencing},
  volume = {23},
  issn = {1088-9051, 1549-5469},
  url = {http://genome.cshlp.org/content/23/12/2126},
  doi = {10.1101/gr.161679.113},
  abstract = {DNA methylation is crucial for a wide variety of biological processes, yet no technique suitable for the methylome analysis of DNA methylation at single-cell resolution is available. Here, we describe a methylome analysis technique that enables single-cell and single-base resolution DNA methylation analysis based on reduced representation bisulfite sequencing (scRRBS). The technique is highly sensitive and can detect the methylation status of up to 1.5 million CpG sites within the genome of an individual mouse embryonic stem cell (mESC). Moreover, we show that the technique can detect the methylation status of individual CpG sites in a haploid sperm cell in a digitized manner as either unmethylated or fully methylated. Furthermore, we show that the demethylation dynamics of maternal and paternal genomes after fertilization can be traced within the individual pronuclei of mouse zygotes. The demethylation process of the genic regions is faster than that of the intergenic regions in both male and female pronuclei. Our method paves the way for the exploration of the dynamic methylome landscapes of individual cells at single-base resolution during physiological processes such as embryonic development, or during pathological processes such as tumorigenesis.},
  timestamp = {2016-04-14T12:06:12Z},
  langid = {english},
  number = {12},
  journaltitle = {Genome Research},
  shortjournal = {Genome Res.},
  author = {Guo, Hongshan and Zhu, Ping and Wu, Xinglong and Li, Xianlong and Wen, Lu and Tang, Fuchou},
  urldate = {2016-04-14},
  date = {2013-01-12},
  pages = {2126--2135},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RJTSMBHH/Guo et al. - 2013 - Single-cell methylome landscapes of mouse embryoni.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZACF4HSV/2126.html:text/html},
  groups = {seq,seq},
  eprinttype = {pmid},
  eprint = {24179143}
}

@article{xiao_mesic:_2015,
  title = {{{MeSiC}}: {{A Model}}-{{Based Method}} for {{Estimating}} 5 {{mC Levels}} at {{Single}}-{{CpG Resolution}} from {{MeDIP}}-Seq},
  volume = {5},
  issn = {2045-2322},
  url = {http://www.nature.com/doifinder/10.1038/srep14699},
  doi = {10.1038/srep14699},
  shorttitle = {{{MeSiC}}},
  timestamp = {2015-10-17T07:33:04Z},
  journaltitle = {Scientific Reports},
  author = {Xiao, Yun and Yu, Fulong and Pang, Lin and Zhao, Hongying and Liu, Ling and Zhang, Guanxiong and Liu, Tingting and Zhang, Hongyi and Fan, Huihui and Zhang, Yan and Pang, Bo and Li, Xia},
  urldate = {2015-10-17},
  date = {2015-10-01},
  pages = {14699},
  file = {srep14699.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Q6DKXI4D/srep14699.pdf:application/pdf;MeSIC.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RT5HE33K/MeSIC.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation},
  groups = {methods,methods}
}

@article{zhang_predicting_2015,
  title = {Predicting Genome-Wide {{DNA}} Methylation Using Methylation Marks, Genomic Position, and {{DNA}} Regulatory Elements},
  volume = {16},
  rights = {2015 Zhang et al.; licensee BioMed Central.},
  issn = {1465-6906},
  url = {http://genomebiology.com/2015/16/1/14/abstract},
  doi = {10.1186/s13059-015-0581-9},
  abstract = {Recent assays for individual-specific genome-wide DNA methylation profiles have enabled epigenome-wide association studies to identify specific CpG sites associated with a phenotype. Computational prediction of CpG site-specific methylation levels is critical to enable genome-wide analyses, but current approaches tackle average methylation within a locus and are often limited to specific genomic regions.},
  timestamp = {2016-02-03T08:09:17Z},
  langid = {english},
  number = {1},
  journaltitle = {Genome Biology},
  author = {Zhang, Weiwei and Spector, Tim D. and Deloukas, Panos and Bell, Jordana T. and Engelhardt, Barbara E.},
  urldate = {2016-02-03},
  date = {2015-01-24},
  pages = {14},
  keywords = {notes},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8WRVEA8S/Zhang et al. - 2015 - Predicting genome-wide DNA methylation using methy.pdf:application/pdf;s13059-015-0581-9-s1.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NPC9XKVX/s13059-015-0581-9-s1.pdf:application/pdf;cpg_engelhardt.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Z85RCXJ4/cpg_engelhardt.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RQJZ589W/14.html:text/html},
  groups = {methods,methods},
  eprinttype = {pmid},
  eprint = {25616342}
}

@article{wang_predicting_2016,
  title = {Predicting {{DNA Methylation State}} of {{CpG Dinucleotide Using Genome Topological Features}} and {{Deep Networks}}},
  volume = {6},
  issn = {2045-2322},
  url = {http://www.nature.com/articles/srep19598},
  doi = {10.1038/srep19598},
  timestamp = {2016-01-26T22:30:17Z},
  journaltitle = {Scientific Reports},
  author = {Wang, Yiheng and Liu, Tong and Xu, Dong and Shi, Huidong and Zhang, Chaoyang and Mo, Yin-Yuan and Wang, Zheng},
  urldate = {2016-01-26},
  date = {2016-01-22},
  pages = {19598},
  keywords = {notes},
  file = {srep19598.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5FCTMB4S/srep19598.pdf:application/pdf;DeepMeth.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GHP5B7ZJ/DeepMeth.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation},
  groups = {methods,methods}
}

@article{zeng_discovering_2016,
  title = {Discovering {{DNA}} Motifs and Genomic Variants Associated with {{DNA}} Methylation},
  rights = {© 2016, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url = {http://biorxiv.org/content/early/2016/09/06/073809},
  doi = {10.1101/073809},
  abstract = {DNA methylation plays a crucial role in establishing tissue-specific gene expression. However, our incomplete understanding of the cis elements that regulate DNA methylation prevents us from interpreting the functional effects of non-coding variants. We present CpGenie (http://cpgenie.csail.mit.edu), a deep convolutional neural network that learns a regulatory sequence code of DNA methylation and enables allele-specific DNA methylation prediction with single-nucleotide sensitivity. Variant annotations from CpGenie accurately identify methylation quantitative trait loci (meQTL) and contribute to the prioritization of functional non-coding variants including expression quantitative trait loci (eQTL) and disease-associated mutations.},
  timestamp = {2016-09-13T22:05:47Z},
  langid = {english},
  journaltitle = {bioRxiv},
  author = {Zeng, Haoyang and Gifford, David K.},
  urldate = {2016-09-13},
  date = {2016-09-06},
  pages = {073809},
  keywords = {good},
  file = {notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3P9VAC9Q/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;tab1_models.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4ETZSNUJ/tab1.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;tab6.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/A87GRPQ2/tab6.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;fig1.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GSDCZK2A/fig1.pdf:application/pdf;Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I9GVVWHP/Zeng and Gifford - 2016 - Discovering DNA motifs and genomic variants associ.pdf:application/pdf;tab5.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/JWGAEXUV/tab5.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;tab2_motifs.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KSBD2IAX/tab2.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;fig3.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QSE96MQ6/fig3.pdf:application/pdf;tab4.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TRMIBN9F/tab4.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;fig2.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/UJXCW4M4/fig2.pdf:application/pdf;fig4.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/USQTE7GF/fig44.pdf:application/pdf;tab3.xlsx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WQKEVBJR/tab3.xlsx:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KQ2MGD38/073809.html:text/html},
  groups = {methods,seq,methods,seq,seq}
}

@article{tan_adage-based_2016,
  title = {{{ADAGE}}-{{Based Integration}} of {{Publicly Available Pseudomonas}} Aeruginosa {{Gene Expression Data}} with {{Denoising Autoencoders Illuminates Microbe}}-{{Host Interactions}}},
  volume = {1},
  rights = {Copyright © 2016 Tan et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license.},
  issn = {2379-5077},
  url = {http://msystems.asm.org/content/1/1/e00025-15},
  doi = {10.1128/mSystems.00025-15},
  abstract = {The increasing number of genome-wide assays of gene expression available from public databases presents opportunities for computational methods that facilitate hypothesis generation and biological interpretation of these data. We present an unsupervised machine learning approach, ADAGE (analysis using denoising autoencoders of gene expression), and apply it to the publicly available gene expression data compendium for Pseudomonas aeruginosa. In this approach, the machine-learned ADAGE model contained 50 nodes which we predicted would correspond to gene expression patterns across the gene expression compendium. While no biological knowledge was used during model construction, cooperonic genes had similar weights across nodes, and genes with similar weights across nodes were significantly more likely to share KEGG pathways. By analyzing newly generated and previously published microarray and transcriptome sequencing data, the ADAGE model identified differences between strains, modeled the cellular response to low oxygen, and predicted the involvement of biological processes based on low-level gene expression differences. ADAGE compared favorably with traditional principal component analysis and independent component analysis approaches in its ability to extract validated patterns, and based on our analyses, we propose that these approaches differ in the types of patterns they preferentially identify. We provide the ADAGE model with analysis of all publicly available P. aeruginosa GeneChip experiments and open source code for use with other species and settings. Extraction of consistent patterns across large-scale collections of genomic data using methods like ADAGE provides the opportunity to identify general principles and biologically important patterns in microbial biology. This approach will be particularly useful in less-well-studied microbial species.
IMPORTANCE The quantity and breadth of genome-scale data sets that examine RNA expression in diverse bacterial and eukaryotic species are increasing more rapidly than for curated knowledge. Our ADAGE method integrates such data without requiring gene function, gene pathway, or experiment labeling, making practical its application to any large gene expression compendium. We built a Pseudomonas aeruginosa ADAGE model from a diverse set of publicly available experiments without any prespecified biological knowledge, and this model was accurate and predictive. We provide ADAGE results for the complete P. aeruginosa GeneChip compendium for use by researchers studying P. aeruginosa and source code that facilitates ADAGE’s application to other species and data types.
Author Video: An author video summary of this article is available.},
  timestamp = {2016-10-06T21:55:11Z},
  langid = {english},
  number = {1},
  journaltitle = {mSystems},
  author = {Tan, Jie and Hammond, John H. and Hogan, Deborah A. and Greene, Casey S.},
  urldate = {2016-10-06},
  date = {2016-02-25},
  pages = {e00025--15},
  keywords = {medium},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/A86IFSXK/Tan et al. - 2016 - ADAGE-Based Integration of Publicly Available Pseu.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/FPS5ENW6/e00025-15.html:text/html},
  groups = {review}
}

@article{stricker_profiles_2016,
  title = {From Profiles to Function in Epigenomics},
  volume = {advance online publication},
  rights = {© 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/vaop/ncurrent/full/nrg.2016.138.html},
  doi = {10.1038/nrg.2016.138},
  abstract = {Myriads of epigenomic features have been comprehensively profiled in health and disease across cell types, tissues and individuals. Although current epigenomic approaches can infer function for chromatin marks through correlation, it remains challenging to establish which marks actually have causative roles in gene regulation and other processes. After revisiting how classical approaches have addressed this question in the past, we discuss the current state of epigenomic profiling and how functional information can be indirectly inferred. We also present new approaches that promise definitive functional answers, which are collectively referred to as 'epigenome editing'. In particular, we explore CRISPR-based technologies for single-locus and multi-locus manipulation. Finally, we discuss which level of function can be achieved with each approach and introduce emerging strategies for high-throughput progression from profiles to function.},
  timestamp = {2016-11-26T17:42:07Z},
  langid = {english},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Stricker, Stefan H. and Köferle, Anna and Beck, Stephan},
  urldate = {2016-11-26},
  date = {2016-11-21},
  keywords = {Chromatin analysis,CRISPR-Cas systems,DNA Methylation,epigenetics,Epigenomics,gene regulation,Genetic engineering,Histone post-translational modifications,medium},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/9Q8JZPGK/Stricker et al. - 2016 - From profiles to function in epigenomics.pdf:application/pdf;notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/FXZ86CNV/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BZZJVMN8/nrg.2016.138.html:text/html},
  groups = {bio}
}

@article{malousi_predictive_2014,
  title = {A {{Predictive Model}} for {{Genomic Methylation Targets}} in {{Humans}}},
  rights = {Bowen Publishing},
  url = {http://www.bowenpublishing.com/jbi/paperInfo.aspx?paperid=15046},
  timestamp = {2017-01-30T17:16:07Z},
  langid = {english},
  journaltitle = {Journal of Bioinformatics},
  author = {Malousi, Andigoni and Chouvarda, Ioanna and Kouidou, Sofia and Maglaveras, Nicos},
  urldate = {2017-01-30},
  date = {2014-01},
  keywords = {good,notes},
  file = {JBIB10003-20140123-090257-2655-39964.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4WXW5T6C/JBIB10003-20140123-090257-2655-39964.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5HTPNHPQ/paperInfo.html:text/html},
  groups = {seq,seq,seq}
}

@article{das_computational_2006,
  title = {Computational Prediction of Methylation Status in Human Genomic Sequences},
  volume = {103},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/103/28/10713},
  doi = {10.1073/pnas.0602949103},
  abstract = {Epigenetic effects in mammals depend largely on heritable genomic methylation patterns. We describe a computational pattern recognition method that is used to predict the methylation landscape of human brain DNA. This method can be applied both to CpG islands and to non-CpG island regions. It computes the methylation propensity for an 800-bp region centered on a CpG dinucleotide based on specific sequence features within the region. We tested several classifiers for classification performance, including K means clustering, linear discriminant analysis, logistic regression, and support vector machine. The best performing classifier used the support vector machine approach. Our program (called hdfinder) presently has a prediction accuracy of 86\%, as validated with CpG regions for which methylation status has been experimentally determined. Using hdfinder, we have depicted the entire genomic methylation patterns for all 22 human autosomes.},
  timestamp = {2017-01-30T18:33:43Z},
  langid = {english},
  number = {28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  author = {Das, Rajdeep and Dimitrova, Nevenka and Xuan, Zhenyu and Rollins, Robert A. and Haghighi, Fatemah and Edwards, John R. and Ju, Jingyue and Bestor, Timothy H. and Zhang, Michael Q.},
  urldate = {2017-01-30},
  date = {2006-11-07},
  pages = {10713--10716},
  keywords = {CpG islands,DNA Methylation,Epigenomics,good,methylation prediction},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/66HZSCZ2/Das et al. - 2006 - Computational prediction of methylation status in .pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3AB7T7CR/10713.html:text/html},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {16818882}
}

@article{feltus_predicting_2003,
  title = {Predicting Aberrant {{CpG}} Island Methylation},
  volume = {100},
  issn = {0027-8424, 1091-6490},
  url = {http://www.pnas.org/content/100/21/12253},
  doi = {10.1073/pnas.2037852100},
  abstract = {Epigenetic silencing associated with aberrant methylation of promoter region CpG islands is one mechanism leading to loss of tumor suppressor function in human cancer. Profiling of CpG island methylation indicates that some genes are more frequently methylated than others, and that each tumor type is associated with a unique set of methylated genes. However, little is known about why certain genes succumb to this aberrant event. To address this question, we used Restriction Landmark Genome Scanning to analyze the susceptibility of 1,749 unselected CpG islands to de novo methylation driven by overexpression of DNA cytosine-5-methyltransferase 1 (DNMT1). We found that although the overall incidence of CpG island methylation was increased in cells overexpressing DNMT1, not all loci were equally affected. The majority of CpG islands (69.9\%) were resistant to de novo methylation, regardless of DNMT1 overexpression. In contrast, we identified a subset of methylation-prone CpG islands (3.8\%) that were consistently hypermethylated in multiple DNMT1 overexpressing clones. Methylation-prone and methylation-resistant CpG islands were not significantly different with respect to size, C+G content, CpG frequency, chromosomal location, or promoter association. We used DNA pattern recognition and supervised learning techniques to derive a classification function based on the frequency of seven novel sequence patterns that was capable of discriminating methylation-prone from methylation-resistant CpG islands with 82\% accuracy. The data indicate that CpG islands differ in their intrinsic susceptibility to de novo methylation, and suggest that the propensity for a CpG island to become aberrantly methylated can be predicted based on its sequence context.},
  timestamp = {2017-01-30T18:41:38Z},
  langid = {english},
  number = {21},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  author = {Feltus, F. A. and Lee, E. K. and Costello, J. F. and Plass, C. and Vertino, P. M.},
  urldate = {2017-01-30},
  date = {2003-10-14},
  pages = {12253--12258},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/UTZ6RF9U/Feltus et al. - 2003 - Predicting aberrant CpG island methylation.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WTG2F2PF/12253.html:text/html},
  groups = {CGI,CGI,CGI},
  eprinttype = {pmid},
  eprint = {14519846}
}

@article{laird_principles_2010,
  title = {Principles and Challenges of Genome-Wide {{DNA}} Methylation Analysis},
  volume = {11},
  rights = {© 2010 Nature Publishing Group},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v11/n3/abs/nrg2732.html},
  doi = {10.1038/nrg2732},
  abstract = {Methylation of cytosine bases in DNA provides a layer of epigenetic control in many eukaryotes that has important implications for normal biology and disease. Therefore, profiling DNA methylation across the genome is vital to understanding the influence of epigenetics. There has been a revolution in DNA methylation analysis technology over the past decade: analyses that previously were restricted to specific loci can now be performed on a genome-scale and entire methylomes can be characterized at single-base-pair resolution. However, there is such a diversity of DNA methylation profiling techniques that it can be challenging to select one. This Review discusses the different approaches and their relative merits and introduces considerations for data analysis.},
  timestamp = {2017-01-30T22:30:05Z},
  langid = {english},
  number = {3},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Laird, Peter W.},
  urldate = {2017-01-30},
  date = {2010-02-02},
  pages = {191--203},
  file = {notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BG2SPUG3/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GB226NCE/Laird - 2010 - Principles and challenges of genome-wide DNA methy.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I4KJ5IX5/nrg2732.html:text/html},
  groups = {bio}
}

@article{bock_analysing_2012,
  title = {Analysing and Interpreting {{DNA}} Methylation Data},
  volume = {13},
  rights = {© 2012 Nature Publishing Group},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v13/n10/full/nrg3273.html},
  doi = {10.1038/nrg3273},
  abstract = {DNA methylation is an epigenetic mark that has suspected regulatory roles in a broad range of biological processes and diseases. The technology is now available for studying DNA methylation genome-wide, at a high resolution and in a large number of samples. This Review discusses relevant concepts, computational methods and software tools for analysing and interpreting DNA methylation data. It focuses not only on the bioinformatic challenges of large epigenome-mapping projects and epigenome-wide association studies but also highlights software tools that make genome-wide DNA methylation mapping more accessible for laboratories with limited bioinformatics experience.},
  timestamp = {2017-01-30T22:31:42Z},
  langid = {english},
  number = {10},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Bock, Christoph},
  urldate = {2017-01-30},
  date = {2012-10},
  pages = {705--719},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WKNSJAHB/Bock - 2012 - Analysing and interpreting DNA methylation data.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3J47RXMF/nrg3273.html:text/html},
  groups = {bio}
}

@article{bird_dna_2002,
  title = {{{DNA}} Methylation Patterns and Epigenetic Memory},
  volume = {16},
  issn = {0890-9369, 1549-5477},
  url = {http://genesdev.cshlp.org/content/16/1/6},
  doi = {10.1101/gad.947102},
  abstract = {A biweekly scientific journal publishing high-quality research in molecular biology and genetics, cancer biology, biochemistry, and related fields},
  timestamp = {2017-01-30T22:50:14Z},
  langid = {english},
  number = {1},
  journaltitle = {Genes \& Development},
  shortjournal = {Genes Dev.},
  author = {Bird, Adrian},
  urldate = {2017-01-30},
  date = {2002-01-01},
  pages = {6--21},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/C39DXBBT/Bird - 2002 - DNA methylation patterns and epigenetic memory.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Q3JNHHE7/6.html:text/html},
  groups = {bio},
  eprinttype = {pmid},
  eprint = {11782440}
}

@article{jones_rethinking_2009,
  title = {Rethinking How {{DNA}} Methylation Patterns Are Maintained},
  volume = {10},
  rights = {© 2009 Nature Publishing Group},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v10/n11/full/nrg2651.html},
  doi = {10.1038/nrg2651},
  abstract = {DNA methylation patterns are set up early in mammalian development and are then copied during the division of somatic cells. A long-established model for the maintenance of these patterns explains some, but not all, of the data that are now available. We propose a new model that suggests that the maintenance of DNA methylation relies not only on the recognition of hemimethylated DNA by DNA methyltransferase 1 (DNMT1) but also on the localization of the DNMT3A and DNMT3B enzymes to specific chromatin regions that contain methylated DNA.},
  timestamp = {2017-02-01T22:34:14Z},
  langid = {english},
  number = {11},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Jones, Peter A. and Liang, Gangning},
  urldate = {2017-02-01},
  date = {2009-11},
  pages = {805--811},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KH33IHF5/Jones and Liang - 2009 - Rethinking how DNA methylation patterns are mainta.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/CEEWKA2T/nrg2651.html:text/html},
  groups = {bio}
}

@article{angermueller_accurate_2017,
  title = {Accurate Prediction of Single-Cell {{DNA}} Methylation States Using Deep Learning},
  rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url = {http://biorxiv.org/content/early/2017/02/01/055715},
  doi = {10.1101/055715},
  abstract = {Recent technological advances have enabled assaying DNA methylation at single-cell resolution. Current protocols are limited by incomplete CpG coverage and hence methods to predict missing methylation states are critical to enable genome-wide analyses. Here, we report DeepCpG, a computational approach based on deep neural networks to predict DNA methylation states from DNA sequence and incomplete methylation profiles in single cells. We evaluated DeepCpG on single-cell methylation data from five cell types generated using alternative sequencing protocols, finding that DeepCpG yields substantially more accurate predictions than previous methods. Additionally, we show that the parameters of our model can be interpreted, thereby providing insights into the effect of sequence composition on methylation variability.},
  timestamp = {2017-02-02T23:00:29Z},
  langid = {english},
  journaltitle = {bioRxiv},
  author = {Angermueller, Christof and Lee, Heather and Reik, Wolf and Stegle, Oliver},
  urldate = {2017-02-02},
  date = {2017-02-01},
  pages = {055715},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4NIHGBUT/Angermueller et al. - 2017 - Accurate prediction of single-cell DNA methylation.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VM5IAAPQ/055715.html:text/html},
  groups = {review}
}

@article{robertson_dna_2005,
  title = {{{DNA}} Methylation and Human Disease},
  volume = {6},
  rights = {© 2005 Nature Publishing Group},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v6/n8/full/nrg1655.html},
  doi = {10.1038/nrg1655},
  abstract = {DNA methylation is a crucial epigenetic modification of the genome that is involved in regulating many cellular processes. These include embryonic development, transcription, chromatin structure, X chromosome inactivation, genomic imprinting and chromosome stability. Consistent with these important roles, a growing number of human diseases have been found to be associated with aberrant DNA methylation. The study of these diseases has provided new and fundamental insights into the roles that DNA methylation and other epigenetic modifications have in development and normal cellular homeostasis.},
  timestamp = {2017-02-04T20:19:50Z},
  langid = {english},
  number = {8},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Robertson, Keith D.},
  urldate = {2017-02-04},
  date = {2005-08},
  pages = {597--610},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KM86UHJ6/Robertson - 2005 - DNA methylation and human disease.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MXITPB3G/nrg1655.html:text/html},
  groups = {bio}
}

@article{lim_dna_2010,
  title = {{{DNA}} Methylation: A Form of Epigenetic Control of Gene Expression},
  volume = {12},
  issn = {14672561},
  url = {http://doi.wiley.com/10.1576/toag.12.1.037.27556},
  doi = {10.1576/toag.12.1.037.27556},
  shorttitle = {{{DNA}} Methylation},
  timestamp = {2017-02-04T20:22:26Z},
  langid = {english},
  number = {1},
  journaltitle = {The Obstetrician \& Gynaecologist},
  author = {Lim, Derek H K and Maher, Eamonn R},
  urldate = {2017-02-04},
  date = {2010-01},
  pages = {37--42},
  file = {toag.12.1.037.27556.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GSP37KPI/toag.12.1.037.27556.pdf:application/pdf},
  groups = {bio}
}

@article{moore_dna_2013,
  title = {{{DNA Methylation}} and {{Its Basic Function}}},
  volume = {38},
  issn = {0893-133X, 1740-634X},
  url = {http://www.nature.com/doifinder/10.1038/npp.2012.112},
  doi = {10.1038/npp.2012.112},
  timestamp = {2017-02-04T20:24:01Z},
  number = {1},
  journaltitle = {Neuropsychopharmacology},
  author = {Moore, Lisa D and Le, Thuc and Fan, Guoping},
  urldate = {2017-02-04},
  date = {2013-01},
  pages = {23--38},
  keywords = {good},
  file = {notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/9E2HKSZ7/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;npp2012112a.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Q339JB6T/npp2012112a.pdf:application/pdf},
  groups = {bio}
}

@article{koh_denoising_2017,
  title = {Denoising Genome-Wide Histone {{ChIP}}-Seq with Convolutional Neural Networks},
  rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  url = {http://biorxiv.org/content/early/2017/01/27/052118},
  doi = {10.1101/052118},
  abstract = {Motivation: Chromatin immunoprecipitation sequencing (ChIP-seq) experiments are commonly used to obtain genome-wide profiles of histone modifications associated with different types of functional genomic elements. However, the quality of histone ChIP-seq data is affected by a myriad of experimental parameters such as the amount of input DNA, antibody specificity, ChIP enrichment, and sequencing depth. Making accurate inferences from chromatin profiling experiments that involve diverse experimental parameters is challenging. Results: We introduce a convolutional denoising algorithm, Coda, that uses convolutional neural networks to learn a mapping from suboptimal to high-quality histone ChIP-seq data. This overcomes various sources of noise and variability, substantially enhancing and recovering signal when applied to low-quality chromatin profiling datasets across individuals, cell types, and species. Our method has the potential to improve data quality at reduced costs. More broadly, this approach -- using a high-dimensional discriminative model to encode a generative noise process -- is generally applicable to other biological domains where it is easy to generate noisy data but difficult to analytically characterize the noise or underlying data distribution. Availability: https://github.com/kundajelab/coda},
  timestamp = {2017-02-05T17:42:53Z},
  langid = {english},
  journaltitle = {bioRxiv},
  author = {Koh, Pang Wei and Pierson, Emma and Kundaje, Anshul},
  urldate = {2017-02-05},
  date = {2017-01-27},
  pages = {052118},
  keywords = {notes},
  file = {supplement.pdf:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/23EGI642/ICML_denoising_supplement.pdf:application/pdf;Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/CP4CAGS9/Koh et al. - 2017 - Denoising genome-wide histone ChIP-seq with convol.pdf:application/pdf;notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MEQS7IHF/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/F9MEPXK4/052118.html:text/html},
  groups = {review}
}

@online{_deep_????,
  title = {Deep {{Learning Book}}},
  shorttitle = {Deep {{Learning Book}}},
  timestamp = {2017-02-09T19:10:22Z},
  groups = {review}
}

@article{_cs231_????,
  title = {{{CS231}}},
  shorttitle = {{{CS231}}},
  timestamp = {2017-02-09T19:10:23Z},
  groups = {review}
}

@online{_cs231n_????,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  shorttitle = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  timestamp = {2017-02-09T19:10:23Z},
  keywords = {notes},
  groups = {review}
}

@online{_recently_????,
  title = {Recently We Launched an Update to {{Google Handwriting Input}} (Goo.Gl/{{qWWhDU}}),...},
  shorttitle = {Recently We Launched an Update to {{Google Handwriting Input}} (Goo.Gl/{{qWWhDU}}),...},
  abstract = {Recently we launched an update to Google Handwriting Input (goo.gl/qWWhDU), which adds support for Arabic, Hebrew, Persian, Urdu and Myanmar. ~In addition...  -  Research at Google - Google+},
  timestamp = {2017-02-09T19:10:23Z},
  groups = {review}
}

@online{_inceptionism:_????,
  title = {Inceptionism: {{Going Deeper}} into {{Neural Networks}}},
  shorttitle = {Inceptionism},
  timestamp = {2017-02-09T19:10:23Z},
  journaltitle = {Research Blog},
  groups = {review}
}

@online{_how_????,
  title = {How Convolutional Neural Networks See the World},
  shorttitle = {How Convolutional Neural Networks See the World},
  timestamp = {2017-02-09T19:10:23Z},
  groups = {review}
}

@article{_tensorflow_????,
  title = {{{TensorFlow}}},
  shorttitle = {{{TensorFlow}}},
  timestamp = {2017-02-09T19:10:23Z},
  keywords = {notes},
  file = {whitepaper2015:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/IRZK7P9J/whitepaper2015.pdf:application/pdf},
  groups = {review}
}

@online{_awentzonline/image-analogies_????,
  title = {Awentzonline/Image-Analogies},
  shorttitle = {Awentzonline/Image-Analogies},
  abstract = {image-analogies - Generate image analogies using neural matching and blending.},
  timestamp = {2017-02-09T19:10:24Z},
  journaltitle = {GitHub},
  groups = {review}
}

@online{_notes_????,
  title = {Notes on {{Pixel Recurrent Neural Networks}}},
  shorttitle = {Notes on {{Pixel Recurrent Neural Networks}}},
  abstract = {Link:~http://arxiv.org/abs/1601.06759 Summary This paper explores the use of convolutional (PixelCNN) and recurrent units (PixelRNN) for modeling the distribution of images, in the framework of autore...},
  timestamp = {2017-02-09T19:10:24Z},
  journaltitle = {Evernote},
  groups = {review}
}

@article{_icml_2016_rnn_dropout.pdf_????,
  title = {{{ICML}}\_2016\_{{RNN}}\_dropout.Pdf},
  shorttitle = {{{ICML}}\_2016\_{{RNN}}\_dropout.Pdf},
  timestamp = {2017-02-09T19:10:24Z},
  file = {ICML_2016_RNN_dropout:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/K9KUFN79/ICML_2016_RNN_dropout.pdf:application/pdf},
  groups = {review}
}

@online{_pointers_????,
  title = {Pointers},
  shorttitle = {Pointers},
  abstract = {awesome-rnn - Recurrent Neural Network - A curated list of resources dedicated to RNN},
  timestamp = {2017-02-09T19:10:24Z},
  journaltitle = {GitHub},
  groups = {review}
}

@online{_recurrent_????,
  title = {Recurrent {{Net Dreams Up Fake Chinese Characters}} in {{Vector Format}} with {{TensorFlow}} – 大トロ},
  shorttitle = {Recurrent {{Net Dreams Up Fake Chinese Characters}} in {{Vector Format}} with {{TensorFlow}} – 大トロ},
  timestamp = {2017-02-09T19:10:24Z},
  groups = {review}
}

@online{_deep_????-1,
  title = {Deep {{Learning}} for {{Visual Question Answering}}},
  shorttitle = {Deep {{Learning}} for {{Visual Question Answering}}},
  abstract = {Homepage and blog},
  timestamp = {2017-02-09T19:10:24Z},
  journaltitle = {Avi Singh},
  groups = {review}
}

@online{_ryankiros/neural-storyteller_????,
  title = {Ryankiros/Neural-Storyteller},
  shorttitle = {Ryankiros/Neural-Storyteller},
  abstract = {neural-storyteller - A recurrent neural network for generating little stories about images},
  timestamp = {2017-02-09T19:10:24Z},
  journaltitle = {GitHub},
  groups = {review}
}

@article{_cs231n_cnn_tricks_????,
  title = {{{CS231n}}\_cnn\_tricks},
  shorttitle = {{{CS231n}}\_cnn\_tricks},
  timestamp = {2017-02-09T19:10:24Z},
  file = {CS231n_cnn_tricks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Q8M6QT5D/CS231n_cnn_tricks.pdf:application/pdf},
  groups = {review}
}

@online{_newmu/dcgan_code_????,
  title = {Newmu/Dcgan\_code},
  shorttitle = {Newmu/Dcgan\_code},
  abstract = {dcgan\_code - Deep Convolutional Generative Adversarial Networks},
  timestamp = {2017-02-09T19:10:25Z},
  journaltitle = {GitHub},
  groups = {review}
}

@article{abadi_tensorflow:_2016,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  timestamp = {2017-02-09T19:10:25Z},
  journaltitle = {arXiv preprint arXiv:1603.04467},
  author = {Abadi, Martın and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu},
  date = {2016},
  groups = {review}
}

@incollection{agathocleous_protein_2010,
  title = {Protein Secondary Structure Prediction with Bidirectional Recurrent Neural Nets: {{Can}} Weight Updating for Each Residue Enhance Performance?},
  isbn = {3-642-16238-X},
  shorttitle = {Protein Secondary Structure Prediction with Bidirectional Recurrent Neural Nets: {{Can}} Weight Updating for Each Residue Enhance Performance?},
  timestamp = {2017-02-09T19:10:25Z},
  booktitle = {Artificial {{Intelligence Applications}} and {{Innovations}}},
  publisher = {{Springer}},
  author = {Agathocleous, Michalis and Christodoulou, Georgia and Promponas, Vasilis and Christodoulou, Chris and Vassiliades, Vassilis and Antoniou, Antonis},
  date = {2010},
  pages = {128--137},
  groups = {review}
}

@inproceedings{alain_regularized_2012,
  title = {Regularized Auto-Encoders Estimate Local Statistics},
  shorttitle = {Regularized Auto-Encoders Estimate Local Statistics},
  eventtitle = {Proc. CoRR},
  timestamp = {2017-02-09T19:10:25Z},
  author = {Alain, Guillaume and Bengio, Yoshua and Rifai, Salah},
  date = {2012},
  pages = {1--17},
  groups = {review}
}

@article{albert_genetics_2014,
  title = {Genetics of Single-Cell Protein Abundance Variation in Large Yeast Populations},
  volume = {506},
  issn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  doi = {10.1038/nature12904},
  shorttitle = {Genetics of Single-Cell Protein Abundance Variation in Large Yeast Populations},
  abstract = {Variation among individuals arises in part from differences in DNA sequences, but the genetic basis for variation in most traits, including common diseases, remains only partly understood. Many DNA variants influence phenotypes by altering the expression level of one or several genes. The effects of such variants can be detected as expression quantitative trait loci (eQTL). Traditional eQTL mapping requires large-scale genotype and gene expression data for each individual in the study sample, which limits sample sizes to hundreds of individuals in both humans and model organisms and reduces statistical power. Consequently, many eQTL are probably missed, especially those with smaller effects. Furthermore, most studies use messenger RNA rather than protein abundance as the measure of gene expression. Studies that have used mass-spectrometry proteomics reported unexpected differences between eQTL and protein QTL (pQTL) for the same genes, but these studies have been even more limited in scope. Here we introduce a powerful method for identifying genetic loci that influence protein expression in the yeast Saccharomyces cerevisiae. We measure single-cell protein abundance through the use of green fluorescent protein tags in very large populations of genetically variable cells, and use pooled sequencing to compare allele frequencies across the genome in thousands of individuals with high versus low protein abundance. We applied this method to 160 genes and detected many more loci per gene than previous studies. We also observed closer correspondence between loci that influence protein abundance and loci that influence mRNA abundance of a given gene. Most loci that we detected were clustered in 'hotspots' that influence multiple proteins, and some hotspots were found to influence more than half of the proteins that we examined. The variants that underlie these hotspots have profound effects on the gene regulatory network and provide insights into genetic variation in cell physiology between yeast strains.},
  timestamp = {2017-02-09T19:10:25Z},
  eprinttype = {pubmed},
  eprint = {24402228},
  langid = {english},
  number = {7489},
  journaltitle = {Nature},
  shortjournal = {Nature},
  author = {Albert, F. W. and Treusch, S. and Shockley, A. H. and Bloom, J. S. and Kruglyak, L.},
  date = {2014-02-27},
  pages = {494--7},
  keywords = {*Single-Cell Analysis,Gene Expression Profiling,Gene Expression Regulation; Fungal/*genetics,Gene Frequency,Gene Regulatory Networks/genetics,Genes; Fungal/genetics,Genetic Variation/*genetics,Genome; Fungal/genetics,Genotype,Green Fluorescent Proteins/analysis/genetics,Multigene Family/genetics,Proteomics,Quantitative Trait Loci/genetics,RNA; Fungal/genetics/metabolism,RNA; Messenger/genetics/metabolism,Saccharomyces cerevisiae/cytology/*genetics/*metabolism,Saccharomyces cerevisiae Proteins/analysis/genetics/*metabolism,Sequence Analysis; DNA},
  groups = {review}
}

@article{alipanahi_predicting_2015,
  title = {Predicting the Sequence Specificities of {{DNA}}- and {{RNA}}-Binding Proteins by Deep Learning},
  volume = {33},
  issn = {1087-0156},
  doi = {10.1038/nbt.3300},
  shorttitle = {Predicting the Sequence Specificities of {{DNA}}- and {{RNA}}-Binding Proteins by Deep Learning},
  abstract = {Knowing the sequence specificities of DNA- and RNA-binding proteins is essential for developing models of the regulatory processes in biological systems and for identifying causal disease variants. Here we show that sequence specificities can be ascertained from experimental data with 'deep learning' techniques, which offer a scalable, flexible and unified computational approach for pattern discovery. Using a diverse array of experimental data and evaluation metrics, we find that deep learning outperforms other state-of-the-art methods, even when training on in vitro data and testing on in vivo data. We call this approach DeepBind and have built a stand-alone software tool that is fully automatic and handles millions of sequences per experiment. Specificities determined by DeepBind are readily visualized as a weighted ensemble of position weight matrices or as a 'mutation map' that indicates how variations affect binding within a specific sequence.},
  timestamp = {2017-02-09T19:10:25Z},
  langid = {english},
  journaltitle = {Nature Biotechnology},
  author = {Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T. and Frey, Brendan J.},
  date = {2015-08},
  pages = {831--838},
  keywords = {notes},
  file = {nbt.3300-S2:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2UHHMDVD/nbt.3300-S2.pdf:application/pdf;Alipanahi et al. - 2015 - Predicting the sequence specificities of DNA- and:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/UIXIHF5K/Alipanahi et al. - 2015 - Predicting the sequence specificities of DNA- and .pdf:application/pdf},
  groups = {review}
}

@article{allamanis_convolutional_2016,
  title = {A {{Convolutional Attention Network}} for {{Extreme Summarization}} of {{Source Code}}},
  shorttitle = {A {{Convolutional Attention Network}} for {{Extreme Summarization}} of {{Source Code}}},
  abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
  timestamp = {2017-02-09T19:10:26Z},
  journaltitle = {arXiv:1602.03001 [cs]},
  author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  date = {2016-02-09},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Software Engineering,notes},
  file = {Allamanis et al. - 2016 - A Convolutional Attention Network for Extreme Summ:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5EI85W9B/Allamanis et al. - 2016 - A Convolutional Attention Network for Extreme Summ.pdf:application/pdf},
  groups = {review}
}

@article{andrychowicz_learning_2016,
  title = {Learning {{Efficient Algorithms}} with {{Hierarchical Attentive Memory}}},
  shorttitle = {Learning {{Efficient Algorithms}} with {{Hierarchical Attentive Memory}}},
  abstract = {In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.},
  timestamp = {2017-02-09T19:10:26Z},
  journaltitle = {arXiv:1602.03218 [cs]},
  author = {Andrychowicz, Marcin and Kurach, Karol},
  date = {2016-02-09},
  keywords = {Computer Science - Learning,notes},
  file = {Andrychowicz and Kurach - 2016 - Learning Efficient Algorithms with Hierarchical At:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BM4ABQS7/Andrychowicz and Kurach - 2016 - Learning Efficient Algorithms with Hierarchical At.pdf:application/pdf},
  groups = {review}
}

@article{asgari_protvec:_2015,
  title = {{{ProtVec}}: {{A Continuous Distributed Representation}} of {{Biological Sequences}}},
  volume = {10},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0141287},
  shorttitle = {{{ProtVec}}},
  abstract = {We propose a new approach for representing biological sequences. This method, named protein-vectors or ProtVec for short, can be utilized in bioinformatics applications such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. Using the Skip-gram neural networks, protein sequences are represented with a single dense n-dimensional vector. This method was evaluated by classifying protein sequences obtained from Swiss-Prot belonging to 7,027 protein families where an average family classification accuracy of \$94$\backslash$\%$\backslash$pm 0.03$\backslash$\%\$ was obtained, outperforming existing family classification methods. In addition, our model was used to predict disordered proteins from structured proteins. Two databases of disordered sequences were used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences were distinguished from structured Protein Data Bank (PDB) sequences with 99.81$\backslash$\% accuracy, and unstructured DisProt sequences from structured DisProt sequences with 100.0$\backslash$\% accuracy. These results indicate that by only providing sequence data for various proteins into this model, information about protein structure can be determined with high accuracy. This so-called embedding model needs to be trained only once and can then be used to ascertain a diverse set of information regarding the proteins of interest. In addition, this representation can be considered as pre-training for various applications of deep learning in bioinformatics.},
  timestamp = {2017-02-09T19:10:28Z},
  journaltitle = {PLOS ONE},
  author = {Asgari, Ehsaneddin and Mofrad, Mohammad R. K.},
  date = {2015-11-10},
  pages = {e0141287},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,notes,Quantitative Biology - Genomics,Quantitative Biology - Quantitative Methods},
  file = {Asgari and Mofrad - 2015 - ProtVec A Continuous Distributed Representation o:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/T4EI933H/Asgari and Mofrad - 2015 - ProtVec A Continuous Distributed Representation o.pdf:application/pdf},
  groups = {review}
}

@article{bach_pixel-wise_2015,
  title = {On {{Pixel}}-{{Wise Explanations}} for {{Non}}-{{Linear Classifier Decisions}} by {{Layer}}-{{Wise Relevance Propagation}}},
  volume = {10},
  issn = {1932-6203 (Electronic) 1932-6203 (Linking)},
  doi = {10.1371/journal.pone.0130140},
  shorttitle = {On {{Pixel}}-{{Wise Explanations}} for {{Non}}-{{Linear Classifier Decisions}} by {{Layer}}-{{Wise Relevance Propagation}}},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  timestamp = {2017-02-09T19:10:28Z},
  eprinttype = {pubmed},
  eprint = {26161953},
  langid = {english},
  number = {7},
  journaltitle = {PloS one},
  shortjournal = {PLoS One},
  author = {Bach, S. and Binder, A. and Montavon, G. and Klauschen, F. and Muller, K. R. and Samek, W.},
  date = {2015},
  pages = {e0130140},
  keywords = {Algorithms,Artificial Intelligence,Humans,Image Processing; Computer-Assisted/*methods,Pattern Recognition; Automated/*methods},
  groups = {review}
}

@article{baek_learning_????,
  title = {Learning {{Deep Architectures}} for {{Protein Structure Prediction}}},
  shorttitle = {Learning {{Deep Architectures}} for {{Protein Structure Prediction}}},
  timestamp = {2017-02-09T19:10:28Z},
  author = {Baek, Kyungim},
  file = {baek_bicob15:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8A3P2CQX/baek_bicob15.pdf:application/pdf},
  groups = {review}
}

@article{bahdanau_neural_2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  shorttitle = {Neural Machine Translation by Jointly Learning to Align and Translate},
  timestamp = {2017-02-09T19:10:29Z},
  journaltitle = {arXiv preprint arXiv:1409.0473},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2014},
  keywords = {notes},
  file = {1409.0473v6:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QUJ7NU2K/1409.0473v6.pdf:application/pdf},
  groups = {review}
}

@article{bastien_theano:_2012,
  title = {Theano: New Features and Speed Improvements},
  shorttitle = {Theano: New Features and Speed Improvements},
  timestamp = {2017-02-09T19:10:29Z},
  journaltitle = {arXiv preprint arXiv:1211.5590},
  author = {Bastien, Frédéric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian and Bergeron, Arnaud and Bouchard, Nicolas and Warde-Farley, David and Bengio, Yoshua},
  date = {2012},
  groups = {review}
}

@article{battle_genomic_2015,
  title = {Genomic Variation. {{Impact}} of Regulatory Variation from {{RNA}} to Protein},
  volume = {347},
  issn = {1095-9203 (Electronic) 0036-8075 (Linking)},
  doi = {10.1126/science.1260793},
  shorttitle = {Genomic Variation. {{Impact}} of Regulatory Variation from {{RNA}} to Protein},
  abstract = {The phenotypic consequences of expression quantitative trait loci (eQTLs) are presumably due to their effects on protein expression levels. Yet the impact of genetic variation, including eQTLs, on protein levels remains poorly understood. To address this, we mapped genetic variants that are associated with eQTLs, ribosome occupancy (rQTLs), or protein abundance (pQTLs). We found that most QTLs are associated with transcript expression levels, with consequent effects on ribosome and protein levels. However, eQTLs tend to have significantly reduced effect sizes on protein levels, which suggests that their potential impact on downstream phenotypes is often attenuated or buffered. Additionally, we identified a class of cis QTLs that affect protein abundance with little or no effect on messenger RNA or ribosome levels, which suggests that they may arise from differences in posttranslational regulation.},
  timestamp = {2017-02-09T19:10:29Z},
  eprinttype = {pubmed},
  eprint = {25657249},
  number = {6222},
  journaltitle = {Science},
  shortjournal = {Science},
  author = {Battle, A. and Khan, Z. and Wang, S. H. and Mitrano, A. and Ford, M. J. and Pritchard, J. K. and Gilad, Y.},
  date = {2015-02-06},
  pages = {664--7},
  keywords = {*Gene Expression Regulation,*Genetic Variation,*Quantitative Trait Loci,*Transcription; Genetic,3' Flanking Region,5' Flanking Region,Cell Line,Exons,Humans,Phenotype,Protein Biosynthesis/*genetics,Ribosomes/metabolism,RNA; Messenger/*genetics},
  groups = {review}
}

@article{bayer_learning_2014,
  title = {Learning {{Stochastic Recurrent Networks}}},
  shorttitle = {Learning {{Stochastic Recurrent Networks}}},
  abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
  timestamp = {2017-02-09T19:10:29Z},
  journaltitle = {arXiv:1411.7610 [cs, stat]},
  author = {Bayer, Justin and Osendorfer, Christian},
  date = {2014-11-27},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Bayer and Osendorfer - 2014 - Learning Stochastic Recurrent Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GTG7JGM5/Bayer and Osendorfer - 2014 - Learning Stochastic Recurrent Networks.pdf:application/pdf},
  groups = {review}
}

@article{bell_dna_2011,
  title = {{{DNA}} Methylation Patterns Associate with Genetic and Gene Expression Variation in {{HapMap}} Cell Lines},
  volume = {12},
  shorttitle = {{{DNA}} Methylation Patterns Associate with Genetic and Gene Expression Variation in {{HapMap}} Cell Lines},
  timestamp = {2017-02-09T19:10:30Z},
  number = {1},
  journaltitle = {Genome Biol},
  author = {Bell, Jordana T and Pai, Athma A and Pickrell, Joseph K and Gaffney, Daniel J and Pique-Regi, Roger and Degner, Jacob F and Gilad, Yoav and Pritchard, Jonathan K},
  date = {2011},
  pages = {R10},
  groups = {review}
}

@article{bengio_scheduled_2015,
  title = {Scheduled {{Sampling}} for {{Sequence Prediction}} with {{Recurrent Neural Networks}}},
  shorttitle = {Scheduled {{Sampling}} for {{Sequence Prediction}} with {{Recurrent Neural Networks}}},
  abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
  timestamp = {2017-02-09T19:10:30Z},
  journaltitle = {arXiv:1506.03099 [cs]},
  author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  date = {2015-06-09},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,notes},
  file = {Bengio et al. - 2015 - Scheduled Sampling for Sequence Prediction with Re:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RXCQ436D/Bengio et al. - 2015 - Scheduled Sampling for Sequence Prediction with Re.pdf:application/pdf},
  groups = {review}
}

@incollection{bengio_practical_2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  isbn = {3-642-35288-X},
  shorttitle = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  timestamp = {2017-02-09T19:10:30Z},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer}},
  author = {Bengio, Yoshua},
  date = {2012},
  pages = {437--478},
  groups = {review}
}

@article{bengio_representation_2013,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  volume = {35},
  issn = {0162-8828},
  shorttitle = {Representation Learning: {{A}} Review and New Perspectives},
  timestamp = {2017-02-09T19:10:30Z},
  number = {8},
  journaltitle = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pierre},
  date = {2013},
  pages = {1798--1828},
  groups = {review}
}

@inproceedings{bengio_greedy_2007,
  title = {Greedy Layer-Wise Training of Deep Networks},
  volume = {19},
  isbn = {1049-5258},
  shorttitle = {Greedy Layer-Wise Training of Deep Networks},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:10:30Z},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  date = {2007},
  pages = {153},
  groups = {review}
}

@article{berglund_bidirectional_2015,
  title = {Bidirectional {{Recurrent Neural Networks}} as {{Generative Models}} - {{Reconstructing Gaps}} in {{Time Series}}},
  shorttitle = {Bidirectional {{Recurrent Neural Networks}} as {{Generative Models}} - {{Reconstructing Gaps}} in {{Time Series}}},
  abstract = {Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model is difficult. As an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible.},
  timestamp = {2017-02-09T19:10:30Z},
  journaltitle = {arXiv:1504.01575 [cs]},
  author = {Berglund, Mathias and Raiko, Tapani and Honkala, Mikko and Kärkkäinen, Leo and Vetek, Akos and Karhunen, Juha},
  date = {2015-04-07},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Berglund et al. - 2015 - Bidirectional Recurrent Neural Networks as Generat:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/G6Q46B3A/Berglund et al. - 2015 - Bidirectional Recurrent Neural Networks as Generat.pdf:application/pdf},
  groups = {review}
}

@article{bergstra_hyperparameter_2013,
  title = {Hyperparameter {{Optimization}} and {{Boosting}} for {{Classifying Facial Expressions}}: {{How}} Good Can a "{{Null}}" {{Model}} Be?},
  shorttitle = {Hyperparameter {{Optimization}} and {{Boosting}} for {{Classifying Facial Expressions}}},
  timestamp = {2017-02-09T19:10:31Z},
  journaltitle = {arXiv preprint arXiv:1306.3476},
  author = {Bergstra, James and Cox, David D.},
  date = {2013},
  file = {1306.3476v1:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TZUA4CHU/1306.3476v1.pdf:application/pdf},
  groups = {review}
}

@inproceedings{bergstra_making_2013,
  title = {Making a Science of Model Search: {{Hyperparameter}} Optimization in Hundreds of Dimensions for Vision Architectures},
  shorttitle = {Making a Science of Model Search},
  timestamp = {2017-02-09T19:10:31Z},
  booktitle = {Proceedings of {{The}} 30th {{International Conference}} on {{Machine Learning}}},
  author = {Bergstra, James and Yamins, Daniel and Cox, David},
  date = {2013},
  pages = {115--123},
  file = {bergstra13:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/XU2PZMAW/bergstra13.pdf:application/pdf},
  groups = {review}
}

@book{bergstra_hyperopt:_2013,
  title = {Hyperopt: {{A}} Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms},
  shorttitle = {Hyperopt},
  timestamp = {2017-02-09T19:10:32Z},
  publisher = {{SciPy}},
  author = {Bergstra, James and Yamins, Dan and Cox, David D.},
  date = {2013},
  file = {bergstra_hyperopt:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8WMGUZVD/bergstra_hyperopt.pdf:application/pdf},
  groups = {review}
}

@inproceedings{bergstra_algorithms_2011,
  title = {Algorithms for Hyper-Parameter Optimization},
  shorttitle = {Algorithms for Hyper-Parameter Optimization},
  timestamp = {2017-02-09T19:10:32Z},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bergstra, James S. and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
  date = {2011},
  pages = {2546--2554},
  keywords = {notes},
  file = {4443-algorithms-for-hyper-parameter-optimization:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/UUGDUABJ/4443-algorithms-for-hyper-parameter-optimization.pdf:application/pdf},
  groups = {review}
}

@article{boulanger-lewandowski_modeling_2012,
  title = {Modeling {{Temporal Dependencies}} in {{High}}-{{Dimensional Sequences}}: {{Application}} to {{Polyphonic Music Generation}} and {{Transcription}}},
  shorttitle = {Modeling {{Temporal Dependencies}} in {{High}}-{{Dimensional Sequences}}},
  abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
  timestamp = {2017-02-09T19:10:33Z},
  journaltitle = {arXiv:1206.6392 [cs, stat]},
  author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  date = {2012-06-27},
  keywords = {Computer Science - Learning,Computer Science - Sound,notes,Statistics - Machine Learning},
  file = {Boulanger-Lewandowski et al. - 2012 - Modeling Temporal Dependencies in High-Dimensional:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/69Z8P59D/Boulanger-Lewandowski et al. - 2012 - Modeling Temporal Dependencies in High-Dimensional.pdf:application/pdf},
  groups = {review}
}

@inproceedings{boulanger-lewandowski_high-dimensional_2013,
  title = {High-Dimensional Sequence Transduction},
  shorttitle = {High-Dimensional Sequence Transduction},
  timestamp = {2017-02-09T19:10:33Z},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2013 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pierre},
  date = {2013},
  pages = {3178--3182},
  keywords = {notes},
  file = {ICASSP2013:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/IS5SIRAB/ICASSP2013.pdf:application/pdf},
  groups = {review}
}

@article{bowman_generating_2015,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  shorttitle = {Generating {{Sentences}} from a {{Continuous Space}}},
  abstract = {The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.},
  timestamp = {2017-02-09T19:10:34Z},
  journaltitle = {arXiv:1511.06349 [cs]},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  date = {2015-11-19},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,notes},
  file = {Bowman et al. - 2015 - Generating Sentences from a Continuous Space:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I4N44EMU/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:application/pdf},
  groups = {review}
}

@article{brakel_training_2013,
  title = {Training {{Energy}}-{{Based Models}} for {{Time}}-{{Series Imputation}}},
  volume = {14},
  shorttitle = {Training {{Energy}}-{{Based Models}} for {{Time}}-{{Series Imputation}}},
  timestamp = {2017-02-09T19:10:34Z},
  journaltitle = {Journal of Machine Learning Research},
  author = {Brakel, Philémon and Stroobandt, Dirk and Schrauwen, Benjamin},
  date = {2013},
  pages = {2771--2797},
  keywords = {notes},
  file = {Brakel et al. - 2013 - Training Energy-Based Models for Time-Series Imput:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WHWH7N5M/Brakel et al. - 2013 - Training Energy-Based Models for Time-Series Imput.pdf:application/pdf},
  groups = {review}
}

@article{buitinck_api_2013,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  shorttitle = {{{API}} Design for Machine Learning Software},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  timestamp = {2017-02-09T19:10:36Z},
  journaltitle = {arXiv:1309.0238 [cs]},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Gaël},
  date = {2013-09-01},
  keywords = {Computer Science - Learning,Computer Science - Mathematical Software},
  groups = {review}
}

@article{burda_importance_2015,
  title = {Importance {{Weighted Autoencoders}}},
  shorttitle = {Importance {{Weighted Autoencoders}}},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It makes two strong assumptions about posterior inference: that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  timestamp = {2017-02-09T19:10:36Z},
  journaltitle = {arXiv:1509.00519 [cs, stat]},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  date = {2015-09-01},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Burda et al. - 2015 - Importance Weighted Autoencoders:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I2A6TXXV/Burda et al. - 2015 - Importance Weighted Autoencoders.pdf:application/pdf},
  groups = {review}
}

@inproceedings{castro_predicting_2015,
  location = {{New York, NY, USA}},
  title = {Predicting {{Daily Activities}} from {{Egocentric Images Using Deep Learning}}},
  isbn = {978-1-4503-3578-2},
  doi = {10.1145/2802083.2808398},
  shorttitle = {Predicting {{Daily Activities}} from {{Egocentric Images Using Deep Learning}}},
  abstract = {We present a method to analyze images taken from a passive egocentric wearable camera along with the contextual information, such as time and day of week, to learn and predict everyday activities of an individual. We collected a dataset of 40,103 egocentric images over a 6 month period with 19 activity classes and demonstrate the benefit of state-of-the-art deep learning techniques for learning and predicting daily activities. Classification is conducted using a Convolutional Neural Network (CNN) with a classification method we introduce called a late fusion ensemble. This late fusion ensemble incorporates relevant contextual information and increases our classification accuracy. Our technique achieves an overall accuracy of 83.07\% in predicting a person's activity across the 19 activity classes. We also demonstrate some promising results from two additional users by fine-tuning the classifier with one day of training data.},
  timestamp = {2017-02-09T19:10:36Z},
  booktitle = {Proceedings of the 2015 {{ACM International Symposium}} on {{Wearable Computers}}},
  series = {ISWC '15},
  publisher = {{ACM}},
  author = {Castro, Daniel and Hickson, Steven and Bettadapura, Vinay and Thomaz, Edison and Abowd, Gregory and Christensen, Henrik and Essa, Irfan},
  date = {2015},
  pages = {75--82},
  keywords = {activity prediction,convolutional neural networks,Deep learning,egocentric vision,health,late fusion ensemble,wearable computing},
  groups = {review}
}

@article{chandar_correlational_2015,
  title = {Correlational {{Neural Networks}}},
  shorttitle = {Correlational {{Neural Networks}}},
  abstract = {Common Representation Learning (CRL), wherein different descriptions (or views) of the data are embedded in a common subspace, is receiving a lot of attention recently. Two popular paradigms here are Canonical Correlation Analysis (CCA) based approaches and Autoencoder (AE) based approaches. CCA based approaches learn a joint representation by maximizing correlation of the views when projected to the common subspace. AE based methods learn a common representation by minimizing the error of reconstructing the two views. Each of these approaches has its own advantages and disadvantages. For example, while CCA based approaches outperform AE based approaches for the task of transfer learning, they are not as scalable as the latter. In this work we propose an AE based approach called Correlational Neural Network (CorrNet), that explicitly maximizes correlation among the views when projected to the common subspace. Through a series of experiments, we demonstrate that the proposed CorrNet is better than the above mentioned approaches with respect to its ability to learn correlated common representations. Further, we employ CorrNet for several cross language tasks and show that the representations learned using CorrNet perform better than the ones learned using other state of the art approaches.},
  timestamp = {2017-02-09T19:10:36Z},
  journaltitle = {arXiv:1504.07225 [cs, stat]},
  author = {Chandar, Sarath and Khapra, Mitesh M. and Larochelle, Hugo and Ravindran, Balaraman},
  date = {2015-04-27},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes,Statistics - Machine Learning},
  file = {Chandar et al. - 2015 - Correlational Neural Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RKH8JH8W/Chandar et al. - 2015 - Correlational Neural Networks.pdf:application/pdf},
  groups = {review}
}

@article{che_distilling_2015,
  title = {Distilling {{Knowledge}} from {{Deep Networks}} with {{Applications}} to {{Healthcare Domain}}},
  shorttitle = {Distilling {{Knowledge}} from {{Deep Networks}} with {{Applications}} to {{Healthcare Domain}}},
  timestamp = {2017-02-09T19:10:38Z},
  journaltitle = {arXiv preprint arXiv:1512.03542},
  author = {Che, Zhengping and Purushotham, Sanjay and Khemani, Robinder and Liu, Yan},
  date = {2015},
  groups = {review}
}

@article{cheng_statistical_2011,
  title = {A Statistical Framework for Modeling Gene Expression Using Chromatin Features and Application to {{modENCODE}} Datasets},
  volume = {12},
  issn = {1474-760X (Electronic) 1474-7596 (Linking)},
  doi = {10.1186/gb-2011-12-2-r15},
  shorttitle = {A Statistical Framework for Modeling Gene Expression Using Chromatin Features and Application to {{modENCODE}} Datasets},
  abstract = {We develop a statistical framework to study the relationship between chromatin features and gene expression. This can be used to predict gene expression of protein coding genes, as well as microRNAs. We demonstrate the prediction in a variety of contexts, focusing particularly on the modENCODE worm datasets. Moreover, our framework reveals the positional contribution around genes (upstream or downstream) of distinct chromatin features to the overall prediction of expression levels.},
  timestamp = {2017-02-09T19:10:38Z},
  eprinttype = {pubmed},
  eprint = {21324173},
  number = {2},
  journaltitle = {Genome Biol},
  shortjournal = {Genome biology},
  author = {Cheng, C. and Yan, K. K. and Yip, K. Y. and Rozowsky, J. and Alexander, R. and Shou, C. and Gerstein, M.},
  date = {2011},
  pages = {R15},
  keywords = {*Gene Expression Profiling,Algorithms,Animals,Caenorhabditis elegans/*genetics,Chromatin/*genetics/metabolism,Cluster Analysis,Computational Biology/*methods,Data Mining/*methods,Gene Expression,Gene Expression Regulation,Histones/*genetics/metabolism,Humans,MicroRNAs/*genetics/metabolism,Models; Statistical},
  groups = {review}
}

@article{cheung_discovering_2014,
  title = {Discovering {{Hidden Factors}} of {{Variation}} in {{Deep Networks}}},
  shorttitle = {Discovering {{Hidden Factors}} of {{Variation}} in {{Deep Networks}}},
  abstract = {Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal.},
  timestamp = {2017-02-09T19:10:38Z},
  journaltitle = {arXiv:1412.6583 [cs]},
  author = {Cheung, Brian and Livezey, Jesse A. and Bansal, Arjun K. and Olshausen, Bruno A.},
  date = {2014-12-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Cheung et al. - 2014 - Discovering Hidden Factors of Variation in Deep Ne:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/787EGZHV/Cheung et al. - 2014 - Discovering Hidden Factors of Variation in Deep Ne.pdf:application/pdf},
  groups = {review}
}

@article{cho_describing_2015,
  title = {Describing {{Multimedia Content}} Using {{Attention}}-Based {{Encoder}}--{{Decoder Networks}}},
  shorttitle = {Describing {{Multimedia Content}} Using {{Attention}}-Based {{Encoder}}--{{Decoder Networks}}},
  abstract = {Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. We focus in this paper on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.},
  timestamp = {2017-02-09T19:10:38Z},
  journaltitle = {arXiv:1507.01053 [cs]},
  author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
  date = {2015-07-03},
  keywords = {attention,cnn,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes,rnn},
  file = {Cho et al. - 2015 - Describing Multimedia Content using Attention-base:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VJAEPJPT/Cho et al. - 2015 - Describing Multimedia Content using Attention-base.pdf:application/pdf},
  groups = {review}
}

@article{chollet_keras:_2015,
  title = {Keras: {{Theano}}-Based Deep Learning Library},
  shorttitle = {Keras: {{Theano}}-Based Deep Learning Library},
  timestamp = {2017-02-09T19:10:39Z},
  journaltitle = {Code: https://github. com/fchollet. Documentation: http://keras. io},
  author = {Chollet, François},
  date = {2015},
  groups = {review}
}

@article{chung_empirical_2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  shorttitle = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  timestamp = {2017-02-09T19:10:39Z},
  journaltitle = {arXiv:1412.3555 [cs]},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  date = {2014-12-11},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3HD8R2P8/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf},
  groups = {review}
}

@article{chung_gated_2015,
  title = {Gated {{Feedback Recurrent Neural Networks}}},
  shorttitle = {Gated {{Feedback Recurrent Neural Networks}}},
  abstract = {In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.},
  timestamp = {2017-02-09T19:10:39Z},
  journaltitle = {arXiv:1502.02367 [cs, stat]},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2015-02-09},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {Chung et al. - 2015 - Gated Feedback Recurrent Neural Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8KGPHQBB/Chung et al. - 2015 - Gated Feedback Recurrent Neural Networks.pdf:application/pdf},
  groups = {review}
}

@article{chung_recurrent_2015,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  shorttitle = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  abstract = {In this paper, we explore the inclusion of random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic hidden state.},
  timestamp = {2017-02-09T19:10:39Z},
  journaltitle = {arXiv:1506.02216 [cs]},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
  date = {2015-06-07},
  keywords = {notes},
  file = {Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KBTNC6DH/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf},
  groups = {review}
}

@inproceedings{ciresan_deep_2012,
  title = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  shorttitle = {Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:10:40Z},
  author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, Jürgen},
  date = {2012},
  pages = {2843--2851},
  groups = {review}
}

@incollection{ciresan_mitosis_2013,
  title = {Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks},
  isbn = {3-642-40762-5},
  shorttitle = {Mitosis Detection in Breast Cancer Histology Images with Deep Neural Networks},
  timestamp = {2017-02-09T19:10:40Z},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}–{{MICCAI}} 2013},
  publisher = {{Springer}},
  author = {Cireşan, Dan C and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, Jürgen},
  date = {2013},
  pages = {411--418},
  groups = {review}
}

@inproceedings{ciresan_transfer_2012,
  title = {Transfer Learning for {{Latin}} and {{Chinese}} Characters with Deep Neural Networks},
  shorttitle = {Transfer Learning for {{Latin}} and {{Chinese}} Characters with Deep Neural Networks},
  timestamp = {2017-02-09T19:10:40Z},
  booktitle = {Neural {{Networks}} ({{IJCNN}}), {{The}} 2012 {{International Joint Conference}} On},
  publisher = {{IEEE}},
  author = {Cireşan, Dan C. and Meier, Ueli and Schmidhuber, Jürgen},
  date = {2012},
  pages = {1--6},
  keywords = {notes},
  groups = {review}
}

@article{clevert_fast_2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  shorttitle = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers ($>$= 5). Using ELUs, we obtained the best published single-crop result on CIFAR-100 and CIFAR-10. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with similar classification performance, obtaining less than 10\% classification error for a single crop, single model network.},
  timestamp = {2017-02-09T19:10:40Z},
  journaltitle = {arXiv:1511.07289 [cs]},
  author = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  date = {2015-11-23},
  keywords = {Computer Science - Learning,notes},
  file = {Clevert et al. - 2015 - Fast and Accurate Deep Network Learning by Exponen:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/6VS7RBSQ/Clevert et al. - 2015 - Fast and Accurate Deep Network Learning by Exponen.pdf:application/pdf},
  groups = {review}
}

@inproceedings{collobert_torch7:_2011,
  title = {Torch7: {{A}} Matlab-like Environment for Machine Learning},
  shorttitle = {Torch7: {{A}} Matlab-like Environment for Machine Learning},
  eventtitle = {BigLearn, NIPS Workshop},
  timestamp = {2017-02-09T19:10:41Z},
  author = {Collobert, Ronan and Kavukcuoglu, Koray and Farabet, Clément},
  date = {2011},
  note = {EPFL-CONF-192376},
  groups = {review}
}

@article{dahl_multi-task_2014,
  title = {Multi-Task {{Neural Networks}} for {{QSAR Predictions}}},
  shorttitle = {Multi-Task {{Neural Networks}} for {{QSAR Predictions}}},
  abstract = {Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural networks in a recent QSAR competition, we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time. We conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature. We compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance.},
  timestamp = {2017-02-09T19:10:41Z},
  journaltitle = {arXiv preprint arXiv:1406.1231},
  author = {Dahl, George E. and Jaitly, Navdeep and Salakhutdinov, Ruslan},
  date = {2014-06-04},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {Dahl et al. - 2014 - Multi-task Neural Networks for QSAR Predictions:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/HTE7TFFF/Dahl et al. - 2014 - Multi-task Neural Networks for QSAR Predictions.pdf:application/pdf},
  groups = {review}
}

@inproceedings{dauphin_identifying_2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  shorttitle = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:10:41Z},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  pages = {2933--2941},
  groups = {review}
}

@article{deng_deep_2014,
  title = {Deep {{Learning}}: {{Methods}} and {{Applications}}},
  volume = {7},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000039},
  shorttitle = {Deep {{Learning}}},
  timestamp = {2017-02-09T19:10:41Z},
  langid = {english},
  journaltitle = {Foundations and Trends® in Signal Processing},
  author = {Deng, Li},
  date = {2014},
  pages = {197--387},
  file = {DeepLearning-NowPublishing-Vol7-SIG-039:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/536X5MCF/DeepLearning-NowPublishing-Vol7-SIG-039.pdf:application/pdf},
  groups = {review}
}

@incollection{deng_deep_2015,
  title = {Deep Dynamic Models for Learning Hidden Representations of Speech Features},
  shorttitle = {Deep Dynamic Models for Learning Hidden Representations of Speech Features},
  timestamp = {2017-02-09T19:10:43Z},
  booktitle = {Speech and {{Audio Processing}} for {{Coding}}, {{Enhancement}} and {{Recognition}}},
  publisher = {{Springer}},
  author = {Deng, Li and Togneri, Roberto},
  date = {2015},
  pages = {153--195},
  file = {Final_DNN_Chapter_20140314_Deng:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/RGKKKMF5/Final_DNN_Chapter_20140314_Deng.pdf:application/pdf},
  groups = {review}
}

@article{di_lena_deep_2012,
  title = {Deep Architectures for Protein Contact Map Prediction},
  volume = {28},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/bts475},
  shorttitle = {Deep Architectures for Protein Contact Map Prediction},
  abstract = {MOTIVATION: Residue-residue contact prediction is important for protein structure prediction and other applications. However, the accuracy of current contact predictors often barely exceeds 20\% on long-range contacts, falling short of the level required for ab initio structure prediction.
RESULTS: Here, we develop a novel machine learning approach for contact map prediction using three steps of increasing resolution. First, we use 2D recursive neural networks to predict coarse contacts and orientations between secondary structure elements. Second, we use an energy-based method to align secondary structure elements and predict contact probabilities between residues in contacting alpha-helices or strands. Third, we use a deep neural network architecture to organize and progressively refine the prediction of contacts, integrating information over both space and time. We train the architecture on a large set of non-redundant proteins and test it on a large set of non-homologous domains, as well as on the set of protein domains used for contact prediction in the two most recent CASP8 and CASP9 experiments. For long-range contacts, the accuracy of the new CMAPpro predictor is close to 30\%, a significant increase over existing approaches.
AVAILABILITY: CMAPpro is available as part of the SCRATCH suite at http://scratch.proteomics.ics.uci.edu/.
CONTACT: pfbaldi@uci.edu
SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  timestamp = {2017-02-09T19:10:43Z},
  langid = {english},
  journaltitle = {Bioinformatics (Oxford, England)},
  author = {Di Lena, Pietro and Nagata, Ken and Baldi, Pierre},
  date = {2012-10-01},
  pages = {2449--2457},
  keywords = {Algorithms,Artificial Intelligence,Computational Biology,Neural Networks (Computer),Proteins,Protein Structure; Secondary,Protein Structure; Tertiary},
  file = {Lena et al. - 2012 - Deep architectures for protein contact map predict:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/FWQN5QDP/Lena et al. - 2012 - Deep architectures for protein contact map predict.pdf:application/pdf},
  groups = {review}
}

@article{dieleman_lasagne:_2015,
  title = {Lasagne: {{First}} Release},
  shorttitle = {Lasagne: {{First}} Release},
  timestamp = {2017-02-09T19:10:45Z},
  journaltitle = {Zenodo: Geneva, Switzerland},
  author = {Dieleman, Sander and Schlüter, Jan and Raffel, Colin and Olson, Eben and Sønderby, SK and Nouri, D and Maturana, D and Thoma, M and Battenberg, E and Kelly, J},
  date = {2015},
  groups = {review}
}

@article{donahue_decaf:_2013,
  title = {Decaf: {{A}} Deep Convolutional Activation Feature for Generic Visual Recognition},
  shorttitle = {Decaf: {{A}} Deep Convolutional Activation Feature for Generic Visual Recognition},
  timestamp = {2017-02-09T19:10:45Z},
  journaltitle = {arXiv preprint arXiv:1310.1531},
  author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
  date = {2013},
  groups = {review}
}

@article{eduati_prediction_2015,
  title = {Prediction of Human Population Responses to Toxic Compounds by a Collaborative Competition},
  volume = {33},
  issn = {1546-1696 (Electronic) 1087-0156 (Linking)},
  doi = {10.1038/nbt.3299},
  shorttitle = {Prediction of Human Population Responses to Toxic Compounds by a Collaborative Competition},
  abstract = {The ability to computationally predict the effects of toxic compounds on humans could help address the deficiencies of current chemical safety testing. Here, we report the results from a community-based DREAM challenge to predict toxicities of environmental compounds with potential adverse health effects for human populations. We measured the cytotoxicity of 156 compounds in 884 lymphoblastoid cell lines for which genotype and transcriptional data are available as part of the Tox21 1000 Genomes Project. The challenge participants developed algorithms to predict interindividual variability of toxic response from genomic profiles and population-level cytotoxicity data from structural attributes of the compounds. 179 submitted predictions were evaluated against an experimental data set to which participants were blinded. Individual cytotoxicity predictions were better than random, with modest correlations (Pearson's r $<$ 0.28), consistent with complex trait genomic prediction. In contrast, predictions of population-level response to different compounds were higher (r $<$ 0.66). The results highlight the possibility of predicting health risks associated with unknown compounds, although risk estimation accuracy remains suboptimal.},
  timestamp = {2017-02-09T19:10:45Z},
  eprinttype = {pubmed},
  eprint = {26258538},
  number = {9},
  journaltitle = {Nat Biotechnol},
  shortjournal = {Nature biotechnology},
  author = {Eduati, F. and Mangravite, L. M. and Wang, T. and Tang, H. and Bare, J. C. and Huang, R. and Norman, T. and Kellen, M. and Menden, M. P. and Yang, J. and Zhan, X. and Zhong, R. and Xiao, G. and Xia, M. and Abdo, N. and Kosyk, O. and Collaboration, Niehs-Ncats-Unc Dream Toxicogenetics and Friend, S. and Dearry, A. and Simeonov, A. and Tice, R. R. and Rusyn, I. and Wright, F. A. and Stolovitzky, G. and Xie, Y. and Saez-Rodriguez, J.},
  date = {2015-09},
  pages = {933--40},
  groups = {review}
}

@inproceedings{eggensperger_towards_2013,
  title = {Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters},
  shorttitle = {Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters},
  timestamp = {2017-02-09T19:10:45Z},
  booktitle = {{{NIPS}} Workshop on {{Bayesian Optimization}} in {{Theory}} and {{Practice}}},
  author = {Eggensperger, Katharina and Feurer, Matthias and Hutter, Frank and Bergstra, James and Snoek, Jasper and Hoos, Holger and Leyton-Brown, Kevin},
  date = {2013},
  file = {13-BayesOpt_EmpiricalFoundation:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/6IEXUN68/13-BayesOpt_EmpiricalFoundation.pdf:application/pdf},
  groups = {review}
}

@article{eickholt_predicting_2012,
  title = {Predicting Protein Residue-Residue Contacts Using Deep Networks and Boosting},
  volume = {28},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bts598},
  shorttitle = {Predicting Protein Residue-Residue Contacts Using Deep Networks and Boosting},
  abstract = {Motivation: Protein residue–residue contacts continue to play a larger and larger role in protein tertiary structure modeling and evaluation. Yet, while the importance of contact information increases, the performance of sequence-based contact predictors has improved slowly. New approaches and methods are needed to spur further development and progress in the field., Results: Here we present DNCON, a new sequence-based residue–residue contact predictor using deep networks and boosting techniques. Making use of graphical processing units and CUDA parallel computing technology, we are able to train large boosted ensembles of residue–residue contact predictors achieving state-of-the-art performance., Availability: The web server of the prediction method (DNCON) is available at http://iris.rnet.missouri.edu/dncon/., Contact:
chengji@missouri.edu, Supplementary information:
Supplementary data are available at Bioinformatics online.},
  timestamp = {2017-02-09T19:10:45Z},
  journaltitle = {Bioinformatics},
  author = {Eickholt, Jesse and Cheng, Jianlin},
  date = {2012-12-01},
  pages = {3066--3072},
  file = {Eickholt and Cheng - 2012 - Predicting protein residue-residue contacts using:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/38RZPJ4M/Eickholt and Cheng - 2012 - Predicting protein residue-residue contacts using .pdf:application/pdf},
  groups = {review}
}

@article{eickholt_dndisorder:_2013,
  title = {{{DNdisorder}}: Predicting Protein Disorder Using Boosting and Deep Networks},
  volume = {14},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-14-88},
  shorttitle = {{{DNdisorder}}},
  abstract = {A number of proteins contain regions which do not adopt a stable tertiary structure in their native state. Such regions known as disordered regions have been shown to participate in many vital cell functions and are increasingly being examined as drug targets.
PMID: 23497251},
  timestamp = {2017-02-09T19:10:45Z},
  langid = {english},
  journaltitle = {BMC Bioinformatics},
  author = {Eickholt, Jesse and Cheng, Jianlin},
  date = {2013-03-06},
  pages = {88},
  keywords = {Deep learning,Deep networks,Disordered regions,Protein disorder prediction},
  file = {Eickholt and Cheng - 2013 - DNdisorder predicting protein disorder using boos:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QIKZGAB9/Eickholt and Cheng - 2013 - DNdisorder predicting protein disorder using boos.pdf:application/pdf;dncon:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/REUHUW8B/dncon.pdf:application/pdf},
  groups = {review}
}

@article{fabius_variational_2014,
  title = {Variational {{Recurrent Auto}}-{{Encoders}}},
  shorttitle = {Variational {{Recurrent Auto}}-{{Encoders}}},
  abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
  timestamp = {2017-02-09T19:10:47Z},
  journaltitle = {arXiv:1412.6581 [cs, stat]},
  author = {Fabius, Otto and van Amersfoort, Joost R.},
  date = {2014-12-19},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,nodes,notes,Statistics - Machine Learning},
  options = {useprefix=true},
  file = {Fabius and van Amersfoort - 2014 - Variational Recurrent Auto-Encoders:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/R7NFJEWK/Fabius and van Amersfoort - 2014 - Variational Recurrent Auto-Encoders.pdf:application/pdf},
  groups = {review}
}

@inproceedings{fakoor_using_2013,
  title = {Using Deep Learning to Enhance Cancer Diagnosis and Classification},
  shorttitle = {Using Deep Learning to Enhance Cancer Diagnosis and Classification},
  eventtitle = {Proceedings of the ICML Workshop on the Role of Machine Learning in Transforming Healthcare. Atlanta, Georgia: JMLR: W\&CP},
  timestamp = {2017-02-09T19:10:47Z},
  author = {Fakoor, Rasool and Ladhak, Faisal and Nazi, Azade and Huber, Manfred},
  date = {2013},
  groups = {review}
}

@article{farley_simulation_1954,
  title = {Simulation of Self-Organizing Systems by Digital Computer},
  volume = {4},
  issn = {2168-2690},
  shorttitle = {Simulation of Self-Organizing Systems by Digital Computer},
  timestamp = {2017-02-09T19:10:47Z},
  number = {4},
  journaltitle = {Transactions of the IRE Professional Group on Information Theory},
  author = {Farley, BWAC and Clark, W},
  date = {1954},
  pages = {76--84},
  groups = {review}
}

@inproceedings{ferrari_bacterial_2015,
  title = {Bacterial Colony Counting by {{Convolutional Neural Networks}}},
  shorttitle = {Bacterial Colony Counting by {{Convolutional Neural Networks}}},
  eventtitle = {Engineering in Medicine and Biology Society (EMBC), 2015 37th Annual International Conference of the IEEE},
  timestamp = {2017-02-09T19:10:47Z},
  publisher = {{IEEE}},
  author = {Ferrari, Alessandro and Lombardi, Stefano and Signoroni, Alberto},
  date = {2015},
  pages = {7458--7461},
  groups = {review}
}

@article{from_embeds_deep_????,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}}},
  timestamp = {2017-02-09T19:10:47Z},
  author = {from Embeds, Views and Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and {others}},
  options = {useprefix=true},
  file = {38131:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2T4IC8VZ/38131.pdf:application/pdf},
  groups = {review}
}

@article{gal_theoretically_2015,
  title = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  shorttitle = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  abstract = {A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.},
  timestamp = {2017-02-09T19:10:47Z},
  journaltitle = {arXiv:1512.05287 [stat]},
  author = {Gal, Yarin},
  date = {2015-12-16},
  keywords = {notes,Statistics - Machine Learning},
  file = {Gal - 2015 - A Theoretically Grounded Application of Dropout in:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2Q32HGQD/Gal - 2015 - A Theoretically Grounded Application of Dropout in.pdf:application/pdf},
  groups = {review}
}

@article{gal_dropout_2015,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  abstract = {Deep learning tools have recently gained much attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in neural networks (NNs) can be interpreted as a Bayesian approximation. As a direct result we obtain tools for modelling uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational complexity or test accuracy. We perform an exploratory study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of extrapolation, interpolation, and classification. We show that model uncertainty is indispensable for classification using MNIST as an example, and use the model's uncertainty in a Bayesian pipeline with deep reinforcement learning as a practical task.},
  timestamp = {2017-02-09T19:10:48Z},
  journaltitle = {arXiv:1506.02142 [cs, stat]},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2015-06-06},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Gal and Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MMW9USBV/Gal and Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf},
  groups = {review}
}

@article{gan_deep_2015,
  title = {Deep {{Temporal Sigmoid Belief Networks}} for {{Sequence Modeling}}},
  shorttitle = {Deep {{Temporal Sigmoid Belief Networks}} for {{Sequence Modeling}}},
  abstract = {Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.},
  timestamp = {2017-02-09T19:10:48Z},
  journaltitle = {arXiv:1509.07087 [cs, stat]},
  author = {Gan, Zhe and Li, Chunyuan and Henao, Ricardo and Carlson, David and Carin, Lawrence},
  date = {2015-09-23},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Gan et al. - 2015 - Deep Temporal Sigmoid Belief Networks for Sequence:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/XSKNBE2Q/Gan et al. - 2015 - Deep Temporal Sigmoid Belief Networks for Sequence.pdf:application/pdf},
  groups = {review}
}

@article{gardner_deep_2015,
  title = {Deep {{Manifold Traversal}}: {{Changing Labels}} with {{Convolutional Features}}},
  shorttitle = {Deep {{Manifold Traversal}}},
  abstract = {Machine learning is increasingly used in high impact applications such as prediction of hospital re-admission, cancer screening or bio-medical research applications. As predictions become increasingly accurate, practitioners may be interested in identifying actionable changes to inputs in order to alter their class membership. For example, a doctor might want to know what changes to a patient's status would predict him/her to not be re-admitted to the hospital soon. Szegedy et al. (2013b) demonstrated that identifying such changes can be very hard in image classification tasks. In fact, tiny, imperceptible changes can result in completely different predictions without any change to the true class label of the input. In this paper we ask the question if we can make small but meaningful changes in order to truly alter the class membership of images from a source class to a target class. To this end we propose deep manifold traversal, a method that learns the manifold of natural images and provides an effective mechanism to move images from one area (dominated by the source class) to another (dominated by the target class).The resulting algorithm is surprisingly effective and versatile. It allows unrestricted movements along the image manifold and only requires few images from source and target to identify meaningful changes. We demonstrate that the exact same procedure can be used to change an individual's appearance of age, facial expressions or even recolor black and white images.},
  timestamp = {2017-02-09T19:10:49Z},
  journaltitle = {arXiv:1511.06421 [cs, stat]},
  author = {Gardner, Jacob R. and Kusner, Matt J. and Li, Yixuan and Upchurch, Paul and Weinberger, Kilian Q. and Hopcroft, John E.},
  date = {2015-11-19},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Gardner et al. - 2015 - Deep Manifold Traversal Changing Labels with Conv:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VBMHBP4X/Gardner et al. - 2015 - Deep Manifold Traversal Changing Labels with Conv.pdf:application/pdf},
  groups = {review}
}

@article{gatys_neural_2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  shorttitle = {A {{Neural Algorithm}} of {{Artistic Style}}},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  timestamp = {2017-02-09T19:10:49Z},
  journaltitle = {arXiv:1508.06576 [cs, q-bio]},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015-08-26},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,notes,Quantitative Biology - Neurons and Cognition},
  file = {Gatys et al. - 2015 - A Neural Algorithm of Artistic Style:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8N5XD4U8/Gatys et al. - 2015 - A Neural Algorithm of Artistic Style.pdf:application/pdf},
  groups = {review}
}

@article{gawehn_deep_2016,
  title = {Deep {{Learning}} in {{Drug Discovery}}},
  volume = {35},
  issn = {1868-1751},
  doi = {10.1002/minf.201501008},
  shorttitle = {Deep {{Learning}} in {{Drug Discovery}}},
  abstract = {Artificial neural networks had their first heyday in molecular informatics and drug discovery approximately two decades ago. Currently, we are witnessing renewed interest in adapting advanced neural network architectures for pharmaceutical research by borrowing from the field of “deep learning”. Compared with some of the other life sciences, their application in drug discovery is still limited. Here, we provide an overview of this emerging field of molecular informatics, present the basic concepts of prominent deep learning methods and offer motivation to explore these techniques for their usefulness in computer-assisted drug discovery and design. We specifically emphasize deep neural networks, restricted Boltzmann machine networks and convolutional networks.},
  timestamp = {2017-02-09T19:10:51Z},
  langid = {english},
  number = {1},
  journaltitle = {Molecular Informatics},
  author = {Gawehn, Erik and Hiss, Jan A. and Schneider, Gisbert},
  date = {2016-01-01},
  pages = {3--14},
  keywords = {bioinformatics,cheminformatics,drug design,machine-learning,neural network,virtual screening},
  file = {Gawehn et al. - 2016 - Deep Learning in Drug Discovery:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TVF5HBBP/Gawehn et al. - 2016 - Deep Learning in Drug Discovery.pdf:application/pdf},
  groups = {review}
}

@article{gibbs_abundant_2010,
  title = {Abundant Quantitative Trait Loci Exist for {{DNA}} Methylation and Gene Expression in Human Brain},
  volume = {6},
  issn = {1553-7404},
  shorttitle = {Abundant Quantitative Trait Loci Exist for {{DNA}} Methylation and Gene Expression in Human Brain},
  timestamp = {2017-02-09T19:10:51Z},
  number = {5},
  journaltitle = {PLoS Genet},
  author = {Gibbs, J Raphael and van der Brug, Marcel P and Hernandez, Dena G and Traynor, Bryan J and Nalls, Michael A and Lai, Shiao-Lin and Arepalli, Sampath and Dillman, Allissa and Rafferty, Ian P and Troncoso, Juan},
  date = {2010},
  pages = {e1000952},
  options = {useprefix=true},
  groups = {review}
}

@article{girshick_fast_2015,
  title = {Fast {{R}}-{{CNN}}},
  shorttitle = {Fast {{R}}-{{CNN}}},
  abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  timestamp = {2017-02-09T19:10:51Z},
  journaltitle = {arXiv:1504.08083 [cs]},
  author = {Girshick, Ross},
  date = {2015-04-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notes},
  file = {Girshick - 2015 - Fast R-CNN:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MXBTF5E2/Girshick - 2015 - Fast R-CNN.pdf:application/pdf},
  groups = {review}
}

@inproceedings{girshick_rich_2014,
  title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  shorttitle = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
  eventtitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  timestamp = {2017-02-09T19:10:52Z},
  author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  date = {2014},
  pages = {580--587},
  groups = {review}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  shorttitle = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  eventtitle = {International conference on artificial intelligence and statistics},
  timestamp = {2017-02-09T19:10:52Z},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010},
  pages = {249--256},
  groups = {review}
}

@inproceedings{glorot_deep_2011,
  title = {Deep Sparse Rectifier Neural Networks},
  shorttitle = {Deep Sparse Rectifier Neural Networks},
  eventtitle = {International Conference on Artificial Intelligence and Statistics},
  timestamp = {2017-02-09T19:10:52Z},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  date = {2011},
  pages = {315--323},
  groups = {review}
}

@article{goel_learning_2014,
  title = {Learning {{Temporal Dependencies}} in {{Data Using}} a {{DBN}}-{{BLSTM}}},
  shorttitle = {Learning {{Temporal Dependencies}} in {{Data Using}} a {{DBN}}-{{BLSTM}}},
  abstract = {Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.},
  timestamp = {2017-02-09T19:10:52Z},
  journaltitle = {arXiv:1412.6093 [cs]},
  author = {Goel, Kratarth and Vohra, Raunaq},
  date = {2014-12-18},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Goel and Vohra - 2014 - Learning Temporal Dependencies in Data Using a DBN:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GAS95IJZ/Goel and Vohra - 2014 - Learning Temporal Dependencies in Data Using a DBN.pdf:application/pdf},
  groups = {review}
}

@article{goodfellow_deep_2016,
  title = {Deep {{Learning}}},
  url = {http://www.deeplearningbook.org},
  shorttitle = {Deep {{Learning}}},
  timestamp = {2017-02-09T19:10:52Z},
  journaltitle = {Book in preparation for MIT Press},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016},
  groups = {review}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  timestamp = {2017-02-09T19:10:52Z},
  journaltitle = {arXiv:1406.2661 [cs, stat]},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Goodfellow et al. - 2014 - Generative Adversarial Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WUDCBUWG/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf},
  groups = {review}
}

@article{goodfellow_maxout_2013,
  title = {Maxout {{Networks}}},
  shorttitle = {Maxout {{Networks}}},
  abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
  timestamp = {2017-02-09T19:10:52Z},
  journaltitle = {arXiv:1302.4389 [cs, stat]},
  author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  date = {2013-02-18},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Goodfellow et al. - 2013 - Maxout Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NNQ3HR37/Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf},
  groups = {review}
}

@article{graves_generating_2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  shorttitle = {Generating {{Sequences With Recurrent Neural Networks}}},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  timestamp = {2017-02-09T19:10:56Z},
  journaltitle = {arXiv:1308.0850 [cs]},
  author = {Graves, Alex},
  date = {2013-08-04},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Graves - 2013 - Generating Sequences With Recurrent Neural Network:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3UBPNJTS/Graves - 2013 - Generating Sequences With Recurrent Neural Network.pdf:application/pdf},
  groups = {review}
}

@inproceedings{graves_speech_2013,
  title = {Speech Recognition with Deep Recurrent Neural Networks},
  doi = {10.1109/ICASSP.2013.6638947},
  shorttitle = {Speech Recognition with Deep Recurrent Neural Networks},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  eventtitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  timestamp = {2017-02-09T19:10:57Z},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Graves, A. and Mohamed, A.-R. and Hinton, G.},
  date = {2013-05},
  pages = {6645--6649},
  keywords = {Acoustics,connectionist temporal classification,deep neural networks,deep recurrent neural networks,end-to-end training methods,long short-term memory RNN architecture,Noise,Recurrent neural networks,sequential data,speech recognition,Training,Vectors},
  file = {06638947:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/US5AFF2D/06638947.pdf:application/pdf},
  groups = {review}
}

@article{graves_neural_2014,
  title = {Neural {{Turing Machines}}},
  shorttitle = {Neural {{Turing Machines}}},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  timestamp = {2017-02-09T19:10:57Z},
  journaltitle = {arXiv:1410.5401 [cs]},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  date = {2014-10-20},
  keywords = {Computer Science - Neural and Evolutionary Computing,notes},
  file = {Graves et al. - 2014 - Neural Turing Machines:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/TRVXCJQJ/Graves et al. - 2014 - Neural Turing Machines.pdf:application/pdf},
  groups = {review}
}

@article{greff_lstm:_2015,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (about 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  timestamp = {2017-02-09T19:10:58Z},
  journaltitle = {arXiv:1503.04069 [cs]},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  date = {2015-03-13},
  keywords = {68T10,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1,notes},
  file = {Greff et al. - 2015 - LSTM A Search Space Odyssey:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2GHZW2M7/Greff et al. - 2015 - LSTM A Search Space Odyssey.pdf:application/pdf},
  groups = {review}
}

@article{gregor_draw:_2015,
  title = {{{DRAW}}: {{A Recurrent Neural Network For Image Generation}}},
  shorttitle = {{{DRAW}}},
  abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  timestamp = {2017-02-09T19:11:00Z},
  journaltitle = {arXiv:1502.04623 [cs]},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  date = {2015-02-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Gregor et al. - 2015 - DRAW A Recurrent Neural Network For Image Generat:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KS3N4BMN/Gregor et al. - 2015 - DRAW A Recurrent Neural Network For Image Generat.pdf:application/pdf},
  groups = {review}
}

@article{grubert_genetic_2015,
  title = {Genetic {{Control}} of {{Chromatin States}} in {{Humans Involves Local}} and {{Distal Chromosomal Interactions}}},
  volume = {162},
  issn = {1097-4172 (Electronic) 0092-8674 (Linking)},
  doi = {10.1016/j.cell.2015.07.048},
  shorttitle = {Genetic {{Control}} of {{Chromatin States}} in {{Humans Involves Local}} and {{Distal Chromosomal Interactions}}},
  abstract = {Deciphering the impact of genetic variants on gene regulation is fundamental to understanding human disease. Although gene regulation often involves long-range interactions, it is unknown to what extent non-coding genetic variants influence distal molecular phenotypes. Here, we integrate chromatin profiling for three histone marks in lymphoblastoid cell lines (LCLs) from 75 sequenced individuals with LCL-specific Hi-C and ChIA-PET-based chromatin contact maps to uncover one of the largest collections of local and distal histone quantitative trait loci (hQTLs). Distal QTLs are enriched within topologically associated domains and exhibit largely concordant variation of chromatin state coordinated by proximal and distal non-coding genetic variants. Histone QTLs are enriched for common variants associated with autoimmune diseases and enable identification of putative target genes of disease-associated variants from genome-wide association studies. These analyses provide insights into how genetic variation can affect human disease phenotypes by coordinated changes in chromatin at interacting regulatory elements.},
  timestamp = {2017-02-09T19:11:00Z},
  eprinttype = {pubmed},
  eprint = {26300125},
  number = {5},
  journaltitle = {Cell},
  shortjournal = {Cell},
  author = {Grubert, F. and Zaugg, J. B. and Kasowski, M. and Ursu, O. and Spacek, D. V. and Martin, A. R. and Greenside, P. and Srivas, R. and Phanstiel, D. H. and Pekowska, A. and Heidari, N. and Euskirchen, G. and Huber, W. and Pritchard, J. K. and Bustamante, C. D. and Steinmetz, L. M. and Kundaje, A. and Snyder, M.},
  date = {2015-08-27},
  pages = {1051--65},
  keywords = {*Human Genome Project,Cell Line,Chromatin/*metabolism,Chromosomes; Human/chemistry/*metabolism,Cohort Studies,Female,Gene Regulatory Networks,Genome-Wide Association Study,Histones/metabolism,Humans,Lymphocytes/metabolism,Male,Quantitative Trait Loci,Regulatory Elements; Transcriptional},
  groups = {review}
}

@article{hastie_elements_2005,
  title = {The Elements of Statistical Learning: Data Mining, Inference and Prediction},
  volume = {27},
  issn = {0343-6993},
  shorttitle = {The Elements of Statistical Learning: Data Mining, Inference and Prediction},
  timestamp = {2017-02-09T19:11:00Z},
  number = {2},
  journaltitle = {The Mathematical Intelligencer},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome and Franklin, James},
  date = {2005},
  pages = {83--85},
  groups = {review}
}

@article{he_deep_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  shorttitle = {Deep {{Residual Learning}} for {{Image Recognition}}},
  timestamp = {2017-02-09T19:11:00Z},
  journaltitle = {arXiv preprint arXiv:1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  groups = {review}
}

@inproceedings{he_delving_2015,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  shorttitle = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  eventtitle = {Proceedings of the IEEE International Conference on Computer Vision},
  timestamp = {2017-02-09T19:11:00Z},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  pages = {1026--1034},
  groups = {review}
}

@article{he_deep_2015-1,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  shorttitle = {Deep {{Residual Learning}} for {{Image Recognition}}},
  timestamp = {2017-02-09T19:11:00Z},
  journaltitle = {arXiv preprint arXiv:1512.03385},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015},
  groups = {review}
}

@article{hermann_teaching_2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  shorttitle = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  timestamp = {2017-02-09T19:11:00Z},
  journaltitle = {arXiv:1506.03340 [cs]},
  author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  date = {2015-06-10},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {Hermann et al. - 2015 - Teaching Machines to Read and Comprehend:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/G84DDNRB/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf},
  groups = {review}
}

@article{hinton_deep_2012,
  title = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: {{The}} Shared Views of Four Research Groups},
  volume = {29},
  issn = {1053-5888},
  shorttitle = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: {{The}} Shared Views of Four Research Groups},
  timestamp = {2017-02-09T19:11:01Z},
  number = {6},
  journaltitle = {Signal Processing Magazine, IEEE},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N},
  date = {2012},
  pages = {82--97},
  groups = {review}
}

@incollection{hinton_practical_2012,
  title = {A Practical Guide to Training Restricted Boltzmann Machines},
  isbn = {3-642-35288-X},
  shorttitle = {A Practical Guide to Training Restricted Boltzmann Machines},
  timestamp = {2017-02-09T19:11:01Z},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  publisher = {{Springer}},
  author = {Hinton, Geoffrey E},
  date = {2012},
  pages = {599--619},
  groups = {review}
}

@article{hinton_fast_2006,
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  volume = {18},
  shorttitle = {A Fast Learning Algorithm for Deep Belief Nets},
  timestamp = {2017-02-09T19:11:01Z},
  number = {7},
  journaltitle = {Neural computation},
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  date = {2006},
  pages = {1527--1554},
  groups = {review}
}

@article{hinton_reducing_2006,
  title = {Reducing the Dimensionality of Data with Neural Networks},
  volume = {313},
  issn = {0036-8075},
  shorttitle = {Reducing the Dimensionality of Data with Neural Networks},
  timestamp = {2017-02-09T19:11:01Z},
  number = {5786},
  journaltitle = {Science},
  author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  date = {2006},
  pages = {504--507},
  groups = {review}
}

@article{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  shorttitle = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  timestamp = {2017-02-09T19:11:01Z},
  journaltitle = {arXiv preprint arXiv:1207.0580},
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  date = {2012},
  groups = {review}
}

@article{hubel_shape_1963,
  title = {Shape and Arrangement of Columns in Cat's Striate Cortex},
  volume = {165},
  shorttitle = {Shape and Arrangement of Columns in Cat's Striate Cortex},
  timestamp = {2017-02-09T19:11:01Z},
  number = {3},
  journaltitle = {The Journal of physiology},
  author = {Hubel, DH and Wiesel, TN},
  date = {1963},
  pages = {559},
  groups = {review}
}

@article{hubel_period_1970,
  title = {The Period of Susceptibility to the Physiological Effects of Unilateral Eye Closure in Kittens},
  volume = {206},
  shorttitle = {The Period of Susceptibility to the Physiological Effects of Unilateral Eye Closure in Kittens},
  timestamp = {2017-02-09T19:11:01Z},
  number = {2},
  journaltitle = {The Journal of physiology},
  author = {Hubel, David H and Wiesel, Torsten N},
  date = {1970},
  pages = {419},
  groups = {review}
}

@incollection{hutter_sequential_2011,
  title = {Sequential Model-Based Optimization for General Algorithm Configuration},
  isbn = {3-642-25565-5},
  shorttitle = {Sequential Model-Based Optimization for General Algorithm Configuration},
  timestamp = {2017-02-09T19:11:01Z},
  booktitle = {Learning and {{Intelligent Optimization}}},
  publisher = {{Springer}},
  author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  date = {2011},
  pages = {507--523},
  groups = {review}
}

@article{im_generating_2016,
  title = {Generating Images with Recurrent Adversarial Networks},
  shorttitle = {Generating Images with Recurrent Adversarial Networks},
  abstract = {Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other.},
  timestamp = {2017-02-09T19:11:01Z},
  journaltitle = {arXiv:1602.05110 [cs]},
  author = {Im, Daniel Jiwoong and Kim, Chris Dongjoo and Jiang, Hui and Memisevic, Roland},
  date = {2016-02-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Im et al. - 2016 - Generating images with recurrent adversarial netwo:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NIZHQHRF/Im et al. - 2016 - Generating images with recurrent adversarial netwo.pdf:application/pdf},
  groups = {review}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  timestamp = {2017-02-09T19:11:01Z},
  journaltitle = {arXiv preprint arXiv:1502.03167},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-02-10},
  keywords = {Computer Science - Learning,notes},
  file = {Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/JQ5Q26DH/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
  groups = {review}
}

@article{jaderberg_spatial_2015,
  title = {Spatial {{Transformer Networks}}},
  shorttitle = {Spatial {{Transformer Networks}}},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  timestamp = {2017-02-09T19:11:02Z},
  journaltitle = {arXiv:1506.02025 [cs]},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  date = {2015-06-05},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notes},
  file = {Jaderberg et al. - 2015 - Spatial Transformer Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/7EVIEZTK/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:application/pdf},
  groups = {review}
}

@inproceedings{jain_supervised_2007,
  title = {Supervised Learning of Image Restoration with Convolutional Networks},
  isbn = {1-4244-1630-2},
  shorttitle = {Supervised Learning of Image Restoration with Convolutional Networks},
  eventtitle = {Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on},
  timestamp = {2017-02-09T19:11:02Z},
  publisher = {{IEEE}},
  author = {Jain, Viren and Murray, Joseph F and Roth, Fabian and Turaga, Srinivas and Zhigulin, Valentin and Briggman, Kevin L and Helmstaedter, Moritz N and Denk, Winfried and Seung, H Sebastian},
  date = {2007},
  pages = {1--8},
  groups = {review}
}

@inproceedings{jarrett_what_2009,
  title = {What Is the Best Multi-Stage Architecture for Object Recognition?},
  doi = {10.1109/ICCV.2009.5459469},
  shorttitle = {What Is the Best Multi-Stage Architecture for Object Recognition?},
  abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63\% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ($>$ 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%).},
  eventtitle = {2009 IEEE 12th International Conference on Computer Vision},
  timestamp = {2017-02-09T19:11:02Z},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M. and LeCun, Y.},
  date = {2009-09},
  pages = {2146--2153},
  keywords = {Brain modeling,Caltech-101,Error analysis,feature extraction,feature pooling layer,feature rectification,filter bank,Gabor filters,Histograms,Image edge detection,Learning systems,local contrast normalization,multistage architecture,nonlinear transformation,NORB dataset,object recognition,Refining,supervised learning,unprocessed MNIST dataset,unsupervised learning},
  file = {05459469:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WTU2TUX7/05459469.pdf:application/pdf},
  groups = {review}
}

@inproceedings{jia_caffe:_2014,
  title = {Caffe: {{Convolutional}} Architecture for Fast Feature Embedding},
  isbn = {1-4503-3063-0},
  shorttitle = {Caffe: {{Convolutional}} Architecture for Fast Feature Embedding},
  eventtitle = {Proceedings of the ACM International Conference on Multimedia},
  timestamp = {2017-02-09T19:11:04Z},
  publisher = {{ACM}},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  date = {2014},
  pages = {675--678},
  groups = {review}
}

@article{joulin_inferring_2015,
  title = {Inferring {{Algorithmic Patterns}} with {{Stack}}-{{Augmented Recurrent Nets}}},
  shorttitle = {Inferring {{Algorithmic Patterns}} with {{Stack}}-{{Augmented Recurrent Nets}}},
  abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  timestamp = {2017-02-09T19:11:04Z},
  journaltitle = {arXiv:1503.01007 [cs]},
  author = {Joulin, Armand and Mikolov, Tomas},
  date = {2015-03-03},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Joulin and Mikolov - 2015 - Inferring Algorithmic Patterns with Stack-Augmente:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NF5REKHZ/Joulin and Mikolov - 2015 - Inferring Algorithmic Patterns with Stack-Augmente.pdf:application/pdf},
  groups = {review}
}

@article{jozefowicz_exploring_2016,
  title = {Exploring the {{Limits}} of {{Language Modeling}}},
  shorttitle = {Exploring the {{Limits}} of {{Language Modeling}}},
  abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
  timestamp = {2017-02-09T19:11:04Z},
  journaltitle = {arXiv:1602.02410 [cs]},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  date = {2016-02-07},
  keywords = {Computer Science - Computation and Language,notes},
  file = {Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QJJ77S8X/Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf:application/pdf},
  groups = {review}
}

@article{kalchbrenner_grid_2015,
  title = {Grid {{Long Short}}-{{Term Memory}}},
  shorttitle = {Grid {{Long Short}}-{{Term Memory}}},
  abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.},
  timestamp = {2017-02-09T19:11:04Z},
  journaltitle = {arXiv:1507.01526 [cs]},
  author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
  date = {2015-07-06},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Kalchbrenner et al. - 2015 - Grid Long Short-Term Memory:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8Q9IT83I/Kalchbrenner et al. - 2015 - Grid Long Short-Term Memory.pdf:application/pdf},
  groups = {review}
}

@article{kang_accurate_2008,
  title = {Accurate Discovery of Expression Quantitative Trait Loci under Confounding from Spurious and Genuine Regulatory Hotspots},
  volume = {180},
  issn = {0016-6731 (Print) 0016-6731 (Linking)},
  doi = {10.1534/genetics.108.094201},
  shorttitle = {Accurate Discovery of Expression Quantitative Trait Loci under Confounding from Spurious and Genuine Regulatory Hotspots},
  abstract = {In genomewide mapping of expression quantitative trait loci (eQTL), it is widely believed that thousands of genes are trans-regulated by a small number of genomic regions called "regulatory hotspots," resulting in "trans-regulatory bands" in an eQTL map. As several recent studies have demonstrated, technical confounding factors such as batch effects can complicate eQTL analysis by causing many spurious associations including spurious regulatory hotspots. Yet little is understood about how these technical confounding factors affect eQTL analyses and how to correct for these factors. Our analysis of data sets with biological replicates suggests that it is this intersample correlation structure inherent in expression data that leads to spurious associations between genetic loci and a large number of transcripts inducing spurious regulatory hotspots. We propose a statistical method that corrects for the spurious associations caused by complex intersample correlation of expression measurements in eQTL mapping. Applying our intersample correlation emended (ICE) eQTL mapping method to mouse, yeast, and human identifies many more cis associations while eliminating most of the spurious trans associations. The concordances of cis and trans associations have consistently increased between different replicates, tissues, and populations, demonstrating the higher accuracy of our method to identify real genetic effects.},
  timestamp = {2017-02-09T19:11:06Z},
  eprinttype = {pubmed},
  eprint = {18791227},
  number = {4},
  journaltitle = {Genetics},
  shortjournal = {Genetics},
  author = {Kang, H. M. and Ye, C. and Eskin, E.},
  date = {2008-12},
  pages = {1909--25},
  keywords = {*Gene Expression,*Gene Regulatory Networks,Animals,Computer Simulation,Databases; Genetic,Genome,Humans,Mice,Mice; Inbred Strains,Quantitative Trait Loci/*genetics,Saccharomyces cerevisiae/genetics},
  groups = {review}
}

@article{karlic_histone_2010,
  title = {Histone Modification Levels Are Predictive for Gene Expression},
  volume = {107},
  issn = {1091-6490 (Electronic) 0027-8424 (Linking)},
  doi = {10.1073/pnas.0909344107},
  shorttitle = {Histone Modification Levels Are Predictive for Gene Expression},
  abstract = {Histones are frequently decorated with covalent modifications. These histone modifications are thought to be involved in various chromatin-dependent processes including transcription. To elucidate the relationship between histone modifications and transcription, we derived quantitative models to predict the expression level of genes from histone modification levels. We found that histone modification levels and gene expression are very well correlated. Moreover, we show that only a small number of histone modifications are necessary to accurately predict gene expression. We show that different sets of histone modifications are necessary to predict gene expression driven by high CpG content promoters (HCPs) or low CpG content promoters (LCPs). Quantitative models involving H3K4me3 and H3K79me1 are the most predictive of the expression levels in LCPs, whereas HCPs require H3K27ac and H4K20me1. Finally, we show that the connections between histone modifications and gene expression seem to be general, as we were able to predict gene expression levels of one cell type using a model trained on another one.},
  timestamp = {2017-02-09T19:11:06Z},
  eprinttype = {pubmed},
  eprint = {20133639},
  number = {7},
  journaltitle = {Proc Natl Acad Sci U S A},
  shortjournal = {Proceedings of the National Academy of Sciences of the United States of America},
  author = {Karlic, R. and Chung, H. R. and Lasserre, J. and Vlahovicek, K. and Vingron, M.},
  date = {2010-02-16},
  pages = {2926--31},
  keywords = {*Models; Biological,CD4-Positive T-Lymphocytes/*metabolism,Computational Biology,CpG Islands/genetics,Gene Expression Regulation/*physiology,Histones/*metabolism,Humans,Promoter Regions; Genetic/genetics,Regression Analysis},
  groups = {review}
}

@inproceedings{karpathy_deep_2015,
  title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  shorttitle = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  eventtitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  timestamp = {2017-02-09T19:11:06Z},
  author = {Karpathy, Andrej and Fei-Fei, Li},
  date = {2015},
  pages = {3128--3137},
  groups = {review}
}

@online{karpathy_cs231n_2016,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  url = {http://cs231n.github.io/},
  shorttitle = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  timestamp = {2017-02-09T19:11:06Z},
  author = {Karpathy, Andrej and Johnson, Justin},
  urldate = {2016-01-03},
  date = {2016},
  groups = {review}
}

@article{kell_metabolomics_2005,
  title = {Metabolomics, Machine Learning and Modelling: Towards an Understanding of the Language of Cells},
  volume = {33},
  issn = {0300-5127 (Print) 0300-5127 (Linking)},
  doi = {10.1042/BST0330520},
  shorttitle = {Metabolomics, Machine Learning and Modelling: Towards an Understanding of the Language of Cells},
  abstract = {In answering the question 'Systems Biology--will it work?' (which it self-evidently has already), it is appropriate to highlight advances in philosophy, in new technique development and in novel findings. In terms of philosophy, we see that systems biology involves an iterative interplay between linked activities--instance, between theory and experiment, between induction and deduction and between measurements of parameters and variables--with more emphasis than has perhaps been common now being focused on the first in each of these pairs. In technique development, we highlight closed loop machine learning and its use in the optimization of scientific instrumentation, and the ability to effect high-quality and quasi-continuous optical images of cells. This leads to many important and novel findings. In the first case, these may involve new biomarkers for disease, whereas in the second case, we have determined that many biological signals may be frequency-rather than amplitude-encoded. This leads to a very different view of how signalling 'works' (equations such as that of Michaelis and Menten which use only amplitudes, i.e. concentrations, are inadequate descriptors), lays emphasis on the signal processing network elements that lie 'downstream' of what are traditionally considered the signals, and allows one simply to understand how cross-talk may be avoided between pathways which nevertheless use common signalling elements. The language of cells is much richer than we had supposed, and we are now well placed to decode it.},
  timestamp = {2017-02-09T19:11:06Z},
  eprinttype = {pubmed},
  eprint = {15916555},
  issue = {Pt 3},
  journaltitle = {Biochem Soc Trans},
  shortjournal = {Biochemical Society transactions},
  author = {Kell, D. B.},
  date = {2005-06},
  pages = {520--4},
  keywords = {*Cell Physiological Phenomena,*Computer Simulation,*Metabolism,*Models; Biological,*Systems Biology,Genomics,Signal Transduction},
  groups = {review}
}

@article{kelley_basset:_2016,
  title = {Basset: {{Learning}} the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks},
  volume = {Advance online},
  issn = {1549-5469 (Electronic) 1088-9051 (Linking)},
  doi = {10.1101/gr.200535.115},
  shorttitle = {Basset: {{Learning}} the Regulatory Code of the Accessible Genome with Deep Convolutional Neural Networks},
  abstract = {The complex language of eukaryotic gene expression remains incompletely understood. Despite the importance suggested by many noncoding variants statistically associated with human disease, nearly all such variants have unknown mechanism. Here, we address this challenge using an approach based on a recent machine learning advance - deep convolutional neural networks (CNNs). We introduce an open source package Basset to apply CNNs to learn the functional activity of DNA sequences from genomics data. We trained Basset on a compendium of accessible genomic sites mapped in 164 cell types by DNase-seq and demonstrate greater predictive accuracy than previous methods. Basset predictions for the change in accessibility between variant alleles were far greater for GWAS SNPs that are likely to be causal relative to nearby SNPs in linkage disequilibrium with them. With Basset, a researcher can perform a single sequencing assay in their cell type of interest and simultaneously learn that cell's chromatin accessibility code and annotate every mutation in the genome with its influence on present accessibility and latent potential for accessibility. Thus, Basset offers a powerful computational approach to annotate and interpret the noncoding genome.},
  timestamp = {2017-02-09T19:11:06Z},
  eprinttype = {pubmed},
  eprint = {27197224},
  langid = {english},
  journaltitle = {Genome research},
  shortjournal = {Genome Res},
  author = {Kelley, D. R. and Snoek, J. and Rinn, J.},
  date = {2016-05-03},
  groups = {review}
}

@article{kingma_adam:_2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam: {{A}} Method for Stochastic Optimization},
  timestamp = {2017-02-09T19:11:06Z},
  journaltitle = {arXiv preprint arXiv:1412.6980},
  author = {Kingma, Diederik and Ba, Jimmy},
  date = {2014},
  groups = {review}
}

@article{kingma_auto-encoding_2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  shorttitle = {Auto-{{Encoding Variational Bayes}}},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  timestamp = {2017-02-09T19:11:06Z},
  journaltitle = {arXiv preprint  arXiv:1312.6114},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2013-12-20},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Kingma and Welling - 2013 - Auto-Encoding Variational Bayes:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/74GUG7CX/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf},
  groups = {review}
}

@article{kiros_skip-thought_2015,
  title = {Skip-{{Thought Vectors}}},
  shorttitle = {Skip-{{Thought Vectors}}},
  abstract = {We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.},
  timestamp = {2017-02-09T19:11:06Z},
  journaltitle = {arXiv preprint arXiv:1506.06726},
  author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S. and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  date = {2015-06-22},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,notes},
  file = {Kiros et al. - 2015 - Skip-Thought Vectors:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BRQFRI3I/Kiros et al. - 2015 - Skip-Thought Vectors.pdf:application/pdf},
  groups = {review}
}

@thesis{koch_siamese_2015,
  title = {Siamese {{Neural Networks}} for {{One}}-{{Shot Image Recognition}}},
  shorttitle = {Siamese {{Neural Networks}} for {{One}}-{{Shot Image Recognition}}},
  timestamp = {2017-02-09T19:11:08Z},
  institution = {{University of Toronto}},
  author = {Koch, Gregory},
  date = {2015},
  keywords = {notes},
  file = {msc-thesis:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2PPK4ZQ3/msc-thesis.pdf:application/pdf},
  groups = {review}
}

@article{korattikara_bayesian_2015,
  title = {Bayesian {{Dark Knowledge}}},
  shorttitle = {Bayesian {{Dark Knowledge}}},
  abstract = {We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.},
  timestamp = {2017-02-09T19:11:08Z},
  journaltitle = {arXiv:1506.04416 [cs, stat]},
  author = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},
  date = {2015-06-14},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {Korattikara et al. - 2015 - Bayesian Dark Knowledge:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5675UZUF/Korattikara et al. - 2015 - Bayesian Dark Knowledge.pdf:application/pdf},
  groups = {review}
}

@article{kraus_classifying_2015,
  title = {Classifying and {{Segmenting Microscopy Images Using Convolutional Multiple Instance Learning}}},
  shorttitle = {Classifying and {{Segmenting Microscopy Images Using Convolutional Multiple Instance Learning}}},
  timestamp = {2017-02-09T19:11:09Z},
  journaltitle = {arXiv preprint arXiv:1511.05286},
  author = {Kraus, Oren Z and Ba, Lei Jimmy and Frey, Brendan},
  date = {2015},
  groups = {review}
}

@article{kraus_computer_2016,
  title = {Computer Vision for High Content Screening},
  volume = {0},
  issn = {1040-9238},
  doi = {10.3109/10409238.2015.1135868},
  shorttitle = {Computer Vision for High Content Screening},
  abstract = {High Content Screening (HCS) technologies that combine automated fluorescence microscopy with high throughput biotechnology have become powerful systems for studying cell biology and drug screening. These systems can produce more than 100 000 images per day, making their success dependent on automated image analysis. In this review, we describe the steps involved in quantifying microscopy images and different approaches for each step. Typically, individual cells are segmented from the background using a segmentation algorithm. Each cell is then quantified by extracting numerical features, such as area and intensity measurements. As these feature representations are typically high dimensional ($>$500), modern machine learning algorithms are used to classify, cluster and visualize cells in HCS experiments. Machine learning algorithms that learn feature representations, in addition to the classification or clustering task, have recently advanced the state of the art on several benchmarking tasks in the computer vision community. These techniques have also recently been applied to HCS image analysis.},
  timestamp = {2017-02-09T19:11:09Z},
  journaltitle = {Critical Reviews in Biochemistry and Molecular Biology},
  author = {Kraus, Oren Z. and Frey, Brendan J.},
  date = {2016-01-24},
  pages = {1--8},
  file = {Kraus and Frey - 2016 - Computer vision for high content screening:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ACBN2RTG/Kraus and Frey - 2016 - Computer vision for high content screening.pdf:application/pdf},
  groups = {review}
}

@article{krishnan_deep_2015,
  title = {Deep {{Kalman Filters}}},
  shorttitle = {Deep {{Kalman Filters}}},
  abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing MNIST" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
  timestamp = {2017-02-09T19:11:09Z},
  journaltitle = {arXiv:1511.05121 [cs, stat]},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  date = {2015-11-16},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Krishnan et al. - 2015 - Deep Kalman Filters:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/JCVZ5ZN8/Krishnan et al. - 2015 - Deep Kalman Filters.pdf:application/pdf},
  groups = {review}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  shorttitle = {Imagenet Classification with Deep Convolutional Neural Networks},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:11:10Z},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  pages = {1097--1105},
  groups = {review}
}

@inproceedings{krizhevsky_imagenet_2012-1,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  shorttitle = {Imagenet Classification with Deep Convolutional Neural Networks},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:11:10Z},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  pages = {1097--1105},
  groups = {review}
}

@article{kumar_ask_2015,
  title = {Ask {{Me Anything}}: {{Dynamic Memory Networks}} for {{Natural Language Processing}}},
  shorttitle = {Ask {{Me Anything}}},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.},
  timestamp = {2017-02-09T19:11:10Z},
  journaltitle = {arXiv:1506.07285 [cs]},
  author = {Kumar, Ankit and Irsoy, Ozan and Su, Jonathan and Bradbury, James and English, Robert and Pierce, Brian and Ondruska, Peter and Iyyer, Mohit and Gulrajani, Ishaan and Socher, Richard},
  date = {2015-06-24},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natur:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WEIXETNK/Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natur.pdf:application/pdf},
  groups = {review}
}

@article{lake_human-level_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  volume = {350},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3050},
  shorttitle = {Human-Level Concept Learning through Probabilistic Program Induction},
  abstract = {Handwritten characters drawn by a model
Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce.
Science, this issue p. 1332
People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
  timestamp = {2017-02-09T19:11:10Z},
  langid = {english},
  journaltitle = {Science},
  author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
  date = {2015-11-12},
  pages = {1332--1338},
  keywords = {notes},
  file = {Lake et al. - 2015 - Human-level concept learning through probabilistic:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/588PPEVK/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:application/pdf},
  groups = {review}
}

@inproceedings{larochelle_neural_2011,
  title = {The Neural Autoregressive Distribution Estimator},
  shorttitle = {The Neural Autoregressive Distribution Estimator},
  timestamp = {2017-02-09T19:11:11Z},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Larochelle, Hugo and Murray, Iain},
  date = {2011},
  pages = {29--37},
  keywords = {notes},
  file = {AISTATS2011_LarochelleM11:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/FKCUN8QT/AISTATS2011_LarochelleM11.pdf:application/pdf;deeplearning2015_larochelle_deep_learning_01:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/HQZ785B8/deeplearning2015_larochelle_deep_learning_01.pdf:application/pdf},
  groups = {review}
}

@article{le_simple_2015,
  title = {A {{Simple Way}} to {{Initialize Recurrent Networks}} of {{Rectified Linear Units}}},
  shorttitle = {A {{Simple Way}} to {{Initialize Recurrent Networks}} of {{Rectified Linear Units}}},
  abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
  timestamp = {2017-02-09T19:11:14Z},
  journaltitle = {arXiv:1504.00941 [cs]},
  author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
  date = {2015-04-03},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Le et al. - 2015 - A Simple Way to Initialize Recurrent Networks of R:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WG6ZS6HP/Le et al. - 2015 - A Simple Way to Initialize Recurrent Networks of R.pdf:application/pdf},
  groups = {review}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  volume = {521},
  issn = {0028-0836},
  doi = {10.1038/nature14539},
  shorttitle = {Deep Learning},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  timestamp = {2017-02-09T19:11:14Z},
  langid = {english},
  journaltitle = {Nature},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05-28},
  pages = {436--444},
  keywords = {Computer science,Mathematics and computing},
  file = {LeCun et al. - 2015 - Deep learning:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BXIWTVIK/LeCun et al. - 2015 - Deep learning.pdf:application/pdf},
  groups = {review}
}

@article{lecun_backpropagation_1989,
  title = {Backpropagation Applied to Handwritten Zip Code Recognition},
  volume = {1},
  shorttitle = {Backpropagation Applied to Handwritten Zip Code Recognition},
  timestamp = {2017-02-09T19:11:14Z},
  number = {4},
  journaltitle = {Neural computation},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  date = {1989},
  pages = {541--551},
  groups = {review}
}

@article{lee_dna-level_2015,
  title = {{{DNA}}-{{Level Splice Junction Prediction}} Using {{Deep Recurrent Neural Networks}}},
  shorttitle = {{{DNA}}-{{Level Splice Junction Prediction}} Using {{Deep Recurrent Neural Networks}}},
  timestamp = {2017-02-09T19:11:14Z},
  journaltitle = {arXiv preprint arXiv:1512.05135},
  author = {Lee, Byunghan and Lee, Taehoon and Na, Byunggook and Yoon, Sungroh},
  date = {2015},
  groups = {review}
}

@article{leung_machine_2016,
  title = {Machine {{Learning}} in {{Genomic Medicine}}: {{A Review}} of {{Computational Problems}} and {{Data Sets}}},
  issn = {0018-9219},
  shorttitle = {Machine {{Learning}} in {{Genomic Medicine}}: {{A Review}} of {{Computational Problems}} and {{Data Sets}}},
  timestamp = {2017-02-09T19:11:14Z},
  author = {Leung, Michael KK and Delong, Andrew and Alipanahi, Babak and Frey, Brendan J},
  date = {2016},
  groups = {review}
}

@article{leung_deep_2014,
  title = {Deep Learning of the Tissue-Regulated Splicing Code},
  volume = {30},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btu277},
  shorttitle = {Deep Learning of the Tissue-Regulated Splicing Code},
  abstract = {Motivation: Alternative splicing (AS) is a regulated process that directs the generation of different transcripts from single genes. A computational model that can accurately predict splicing patterns based on genomic features and cellular context is highly desirable, both in understanding this widespread phenomenon, and in exploring the effects of genetic variations on AS.
Methods: Using a deep neural network, we developed a model inferred from mouse RNA-Seq data that can predict splicing patterns in individual tissues and differences in splicing patterns across tissues. Our architecture uses hidden variables that jointly represent features in genomic sequences and tissue types when making predictions. A graphics processing unit was used to greatly reduce the training time of our models with millions of parameters.
Results: We show that the deep architecture surpasses the performance of the previous Bayesian method for predicting AS patterns. With the proper optimization procedure and selection of hyperparameters, we demonstrate that deep architectures can be beneficial, even with a moderately sparse dataset. An analysis of what the model has learned in terms of the genomic features is presented.
Contact: frey@psi.toronto.edu
Supplementary information: Supplementary data are available at Bioinformatics online.},
  timestamp = {2017-02-09T19:11:14Z},
  langid = {english},
  journaltitle = {Bioinformatics},
  author = {Leung, Michael K. K. and Xiong, Hui Yuan and Lee, Leo J. and Frey, Brendan J.},
  date = {2014-06-15},
  pages = {i121--i129},
  keywords = {notes},
  file = {Leung et al. - 2014 - Deep learning of the tissue-regulated splicing cod:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5WHD6M8B/Leung et al. - 2014 - Deep learning of the tissue-regulated splicing cod.pdf:application/pdf},
  groups = {review}
}

@article{li_visualizing_2015,
  title = {Visualizing and {{Understanding Neural Models}} in {{NLP}}},
  shorttitle = {Visualizing and {{Understanding Neural Models}} in {{NLP}}},
  abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve \{$\backslash$em compositionality\}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's \{$\backslash$em salience\}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
  timestamp = {2017-02-09T19:11:15Z},
  journaltitle = {arXiv:1506.01066 [cs]},
  author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
  date = {2015-06-02},
  keywords = {Computer Science - Computation and Language},
  groups = {review}
}

@article{li_using_2015,
  title = {Using Epigenomics Data to Predict Gene Expression in Lung Cancer},
  volume = {16 Suppl 5},
  issn = {1471-2105 (Electronic) 1471-2105 (Linking)},
  doi = {10.1186/1471-2105-16-S5-S10},
  shorttitle = {Using Epigenomics Data to Predict Gene Expression in Lung Cancer},
  abstract = {BACKGROUND: Epigenetic alterations are known to correlate with changes in gene expression among various diseases including cancers. However, quantitative models that accurately predict the up or down regulation of gene expression are currently lacking. METHODS: A new machine learning-based method of gene expression prediction is developed in the context of lung cancer. This method uses the Illumina Infinium HumanMethylation450K Beadchip CpG methylation array data from paired lung cancer and adjacent normal tissues in The Cancer Genome Atlas (TCGA) and histone modification marker CHIP-Seq data from the ENCODE project, to predict the differential expression of RNA-Seq data in TCGA lung cancers. It considers a comprehensive list of 1424 features spanning the four categories of CpG methylation, histone H3 methylation modification, nucleotide composition, and conservation. Various feature selection and classification methods are compared to select the best model over 10-fold cross-validation in the training data set. RESULTS: A best model comprising 67 features is chosen by ReliefF based feature selection and random forest classification method, with AUC = 0.864 from the 10-fold cross-validation of the training set and AUC = 0.836 from the testing set. The selected features cover all four data types, with histone H3 methylation modification (32 features) and CpG methylation (15 features) being most abundant. Among the dropping-off tests of individual data-type based features, removal of CpG methylation feature leads to the most reduction in model performance. In the best model, 19 selected features are from the promoter regions (TSS200 and TSS1500), highest among all locations relative to transcripts. Sequential dropping-off of CpG methylation features relative to different regions on the protein coding transcripts shows that promoter regions contribute most significantly to the accurate prediction of gene expression. CONCLUSIONS: By considering a comprehensive list of epigenomic and genomic features, we have constructed an accurate model to predict transcriptomic differential expression, exemplified in lung cancer.},
  timestamp = {2017-02-09T19:11:15Z},
  eprinttype = {pubmed},
  eprint = {25861082},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC bioinformatics},
  author = {Li, J. and Ching, T. and Huang, S. and Garmire, L. X.},
  date = {2015},
  pages = {S10},
  keywords = {*DNA Methylation,*Gene Expression Regulation; Neoplastic,*Genome; Human,Artificial Intelligence,CpG Islands/*genetics,Epigenomics/*methods,Genomics/methods,High-Throughput Nucleotide Sequencing,Histones/genetics,Humans,Lung Neoplasms/*genetics},
  groups = {review}
}

@book{li_markov_2009,
  title = {Markov Random Field Modeling in Image Analysis},
  isbn = {1-84800-279-3},
  shorttitle = {Markov Random Field Modeling in Image Analysis},
  timestamp = {2017-02-09T19:11:15Z},
  publisher = {{Springer Science \& Business Media}},
  author = {Li, Stan Z},
  date = {2009},
  groups = {review}
}

@article{li_generative_2015,
  title = {Generative {{Moment Matching Networks}}},
  shorttitle = {Generative {{Moment Matching Networks}}},
  abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
  timestamp = {2017-02-09T19:11:15Z},
  journaltitle = {arXiv:1502.02761 [cs, stat]},
  author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
  date = {2015-02-09},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Li et al. - 2015 - Generative Moment Matching Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KJ7ICWUJ/Li et al. - 2015 - Generative Moment Matching Networks.pdf:application/pdf},
  groups = {review}
}

@inproceedings{liang_recurrent_2015,
  title = {Recurrent {{Convolutional Neural Network}} for {{Object Recognition}}},
  shorttitle = {Recurrent {{Convolutional Neural Network}} for {{Object Recognition}}},
  timestamp = {2017-02-09T19:11:15Z},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liang, Ming and Hu, Xiaolin},
  date = {2015},
  pages = {3367--3375},
  file = {Liang15-cvpr:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/D2RWTZQF/Liang15-cvpr.pdf:application/pdf},
  groups = {review}
}

@article{libbrecht_machine_2015,
  title = {Machine Learning Applications in Genetics and Genomics},
  volume = {16},
  issn = {1471-0064 (Electronic) 1471-0056 (Linking)},
  doi = {10.1038/nrg3920},
  shorttitle = {Machine Learning Applications in Genetics and Genomics},
  abstract = {The field of machine learning, which aims to develop computer algorithms that improve with experience, holds promise to enable computers to assist humans in the analysis of large, complex data sets. Here, we provide an overview of machine learning applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data. We present considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised machine learning methods, as well as of generative and discriminative modelling approaches. We provide general guidelines to assist in the selection of these machine learning methods and their practical application for the analysis of genetic and genomic data sets.},
  timestamp = {2017-02-09T19:11:16Z},
  eprinttype = {pubmed},
  eprint = {25948244},
  number = {6},
  journaltitle = {Nat Rev Genet},
  shortjournal = {Nature reviews. Genetics},
  author = {Libbrecht, M. W. and Noble, W. S.},
  date = {2015-06},
  pages = {321--32},
  keywords = {*Artificial Intelligence,*Models; Genetic,Amino Acid Sequence,Animals,Base Sequence,Computer Simulation,Discriminant Analysis,Genetics; Medical,Genomics,Humans,Molecular Sequence Annotation},
  groups = {review}
}

@article{lin_neural_2015,
  title = {Neural {{Networks}} with {{Few Multiplications}}},
  shorttitle = {Neural {{Networks}} with {{Few Multiplications}}},
  abstract = {For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardware-friendly training of neural networks.},
  timestamp = {2017-02-09T19:11:16Z},
  journaltitle = {arXiv:1510.03009 [cs]},
  author = {Lin, Zhouhan and Courbariaux, Matthieu and Memisevic, Roland and Bengio, Yoshua},
  date = {2015-10-11},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Lin et al. - 2015 - Neural Networks with Few Multiplications:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8T64A6RX/Lin et al. - 2015 - Neural Networks with Few Multiplications.pdf:application/pdf},
  groups = {review}
}

@article{lipton_critical_2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  shorttitle = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  abstract = {Countless learning tasks require awareness of time. Image captioning, speech synthesis, and video game playing all require that a model generate sequences of outputs. In other domains, such as time series prediction, video analysis, and music information retrieval, a model must learn from sequences of inputs. Significantly more interactive tasks, such as natural language translation, engaging in dialogue, and robotic control, often demand both. Recurrent neural networks (RNNs) are a powerful family of connectionist models that capture time dynamics via cycles in the graph. Unlike feedforward neural networks, recurrent networks can process examples one at a time, retaining a state, or memory, that reflects an arbitrarily long context window. While these networks have long been difficult to train and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled large-scale learning with recurrent nets. Over the past few years, systems based on state of the art long short-term memory (LSTM) and bidirectional recurrent neural network (BRNN) architectures have demonstrated record-setting performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this review of the literature we synthesize the body of research that over the past three decades has yielded and reduced to practice these powerful models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a mostly self-contained explication of state of the art systems, together with a historical perspective and ample references to the primary research.},
  timestamp = {2017-02-09T19:11:16Z},
  journaltitle = {arXiv preprint  arXiv:1506.00019},
  author = {Lipton, Zachary C.},
  date = {2015-05-29},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Lipton - 2015 - A Critical Review of Recurrent Neural Networks for:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QGDRDDBN/Lipton - 2015 - A Critical Review of Recurrent Neural Networks for.pdf:application/pdf},
  groups = {review}
}

@article{lipton_learning_2015,
  title = {Learning to {{Diagnose}} with {{LSTM Recurrent Neural Networks}}},
  shorttitle = {Learning to {{Diagnose}} with {{LSTM Recurrent Neural Networks}}},
  timestamp = {2017-02-09T19:11:17Z},
  journaltitle = {arXiv preprint arXiv:1511.03677},
  author = {Lipton, Zachary C and Kale, David C and Elkan, Charles and Wetzell, Randall},
  date = {2015},
  groups = {review}
}

@article{lusci_deep_2013,
  title = {Deep {{Architectures}} and {{Deep Learning}} in {{Chemoinformatics}}: {{The Prediction}} of {{Aqueous Solubility}} for {{Drug}}-{{Like Molecules}}},
  volume = {53},
  issn = {1549-9596},
  doi = {10.1021/ci400187y},
  shorttitle = {Deep {{Architectures}} and {{Deep Learning}} in {{Chemoinformatics}}},
  abstract = {Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal (cdb.ics.uci.edu) together with additional material.},
  timestamp = {2017-02-09T19:11:17Z},
  journaltitle = {Journal of Chemical Information and Modeling},
  author = {Lusci, Alessandro and Pollastri, Gianluca and Baldi, Pierre},
  date = {2013-07-22},
  pages = {1563--1575},
  file = {ci400187y:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/P7WC782H/ci400187y.pdf:application/pdf},
  groups = {review}
}

@article{lyons_predicting_2014,
  title = {Predicting Backbone {{Cα}} Angles and Dihedrals from Protein Sequences by Stacked Sparse Auto‐encoder Deep Neural Network},
  volume = {35},
  issn = {1096-987X},
  shorttitle = {Predicting Backbone {{Cα}} Angles and Dihedrals from Protein Sequences by Stacked Sparse Auto‐encoder Deep Neural Network},
  timestamp = {2017-02-09T19:11:19Z},
  number = {28},
  journaltitle = {Journal of computational chemistry},
  author = {Lyons, James and Dehzangi, Abdollah and Heffernan, Rhys and Sharma, Alok and Paliwal, Kuldip and Sattar, Abdul and Zhou, Yaoqi and Yang, Yuedong},
  date = {2014},
  pages = {2040--2046},
  groups = {review}
}

@article{ma_deep_2015,
  title = {Deep {{Neural Nets}} as a {{Method}} for {{Quantitative Structure}}–{{Activity Relationships}}},
  volume = {55},
  issn = {1549-9596},
  doi = {10.1021/ci500747n},
  shorttitle = {Deep {{Neural Nets}} as a {{Method}} for {{Quantitative Structure}}–{{Activity Relationships}}},
  abstract = {Neural networks were widely used for quantitative structure?activity relationships (QSAR) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine (SVM) and random forest (RF), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets (DNNs), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that DNNs can routinely make better prospective predictions than RF on a set of large diverse QSAR data sets that are taken from Merck?s drug discovery effort. The number of adjustable parameters needed for DNNs is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than RF for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training DNNs is still computationally intensive, using graphical processing units (GPUs) can make this issue manageable.},
  timestamp = {2017-02-09T19:11:19Z},
  journaltitle = {Journal of Chemical Information and Modeling},
  author = {Ma, Junshui and Sheridan, Robert P. and Liaw, Andy and Dahl, George E. and Svetnik, Vladimir},
  date = {2015-02-23},
  pages = {263--274},
  file = {ci500747n:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/M2FRUKUD/ci500747n.pdf:application/pdf},
  groups = {review}
}

@article{mahendran_visualizing_2015,
  title = {Visualizing {{Deep Convolutional Neural Networks Using Natural Pre}}-{{Images}}},
  shorttitle = {Visualizing {{Deep Convolutional Neural Networks Using Natural Pre}}-{{Images}}},
  abstract = {Image representations, from SIFT and bag of visual words to Convolutional Neural Networks (CNNs), are a crucial component of almost all computer vision systems. However, our understanding of them remains limited. In this paper we study several landmark representations, both shallow and deep, by a number of complementary visualization techniques. These visualizations are based on the concept of "natural pre-image", namely a naturally-looking image whose representation has some notable property. We study in particular three such visualizations: inversion, in which the aim is to reconstruct an image from its representation, activation maximization, in which we search for patterns that maximally stimulate a representation component, and caricaturization, in which the visual patterns that a representation detects in an image are exaggerated. We pose this as a regularized energy-minimization framework and demonstrate its generality and effectiveness. In particular, we show that this method can invert representations such as HOG more accurately than recent alternatives while being applicable to CNNs too. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.},
  timestamp = {2017-02-09T19:11:19Z},
  journaltitle = {arXiv:1512.02017 [cs]},
  author = {Mahendran, Aravindh and Vedaldi, Andrea},
  date = {2015-12-07},
  keywords = {68T45,Computer Science - Computer Vision and Pattern Recognition,notes},
  file = {Mahendran and Vedaldi - 2015 - Visualizing Deep Convolutional Neural Networks Usi:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/Z42UPBHT/Mahendran and Vedaldi - 2015 - Visualizing Deep Convolutional Neural Networks Usi.pdf:application/pdf},
  groups = {review}
}

@article{makhzani_adversarial_2015,
  title = {Adversarial {{Autoencoders}}},
  shorttitle = {Adversarial {{Autoencoders}}},
  abstract = {In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named "adversarial autoencoder", uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no "holes" in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets.},
  timestamp = {2017-02-09T19:11:20Z},
  journaltitle = {arXiv:1511.05644 [cs]},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian},
  date = {2015-11-17},
  keywords = {Computer Science - Learning,notes},
  file = {Makhzani et al. - 2015 - Adversarial Autoencoders:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KT7Z3CTA/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:application/pdf},
  groups = {review}
}

@article{mamoshina_applications_2016,
  title = {Applications of {{Deep Learning}} in {{Biomedicine}}},
  volume = {13},
  issn = {1543-8392 (Electronic) 1543-8384 (Linking)},
  doi = {10.1021/acs.molpharmaceut.5b00982},
  shorttitle = {Applications of {{Deep Learning}} in {{Biomedicine}}},
  abstract = {Increases in throughput and installed base of biomedical research equipment led to a massive accumulation of -omics data known to be highly variable, high-dimensional, and sourced from multiple often incompatible data platforms. While this data may be useful for biomarker identification and drug discovery, the bulk of it remains underutilized. Deep neural networks (DNNs) are efficient algorithms based on the use of compositional layers of neurons, with advantages well matched to the challenges -omics data presents. While achieving state-of-the-art results and even surpassing human accuracy in many challenging tasks, the adoption of deep learning in biomedicine has been comparatively slow. Here, we discuss key features of deep learning that may give this approach an edge over other machine learning methods. We then consider limitations and review a number of applications of deep learning in biomedical studies demonstrating proof of concept and practical utility.},
  timestamp = {2017-02-09T19:11:20Z},
  eprinttype = {pubmed},
  eprint = {27007977},
  number = {5},
  journaltitle = {Mol Pharm},
  shortjournal = {Molecular pharmaceutics},
  author = {Mamoshina, P. and Vieira, A. and Putin, E. and Zhavoronkov, A.},
  date = {2016-05-02},
  pages = {1445--54},
  keywords = {Artificial Intelligence,biomarker development,Deep learning,deep neural networks,Genomics,Rbm,transcriptomics},
  groups = {review}
}

@article{mansimov_generating_2015,
  title = {Generating {{Images}} from {{Captions}} with {{Attention}}},
  shorttitle = {Generating {{Images}} from {{Captions}} with {{Attention}}},
  timestamp = {2017-02-09T19:11:20Z},
  journaltitle = {arXiv preprint arXiv:1511.02793},
  author = {Mansimov, Elman and Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  date = {2015},
  groups = {review}
}

@article{martens_predicting_2016,
  title = {Predicting Quantitative Traits from Genome and Phenome with near Perfect Accuracy},
  volume = {7},
  issn = {2041-1723 (Electronic) 2041-1723 (Linking)},
  doi = {10.1038/ncomms11512},
  shorttitle = {Predicting Quantitative Traits from Genome and Phenome with near Perfect Accuracy},
  abstract = {In spite of decades of linkage and association studies and its potential impact on human health, reliable prediction of an individual's risk for heritable disease remains difficult. Large numbers of mapped loci do not explain substantial fractions of heritable variation, leaving an open question of whether accurate complex trait predictions can be achieved in practice. Here, we use a genome sequenced population of approximately 7,000 yeast strains of high but varying relatedness, and predict growth traits from family information, effects of segregating genetic variants and growth in other environments with an average coefficient of determination R(2) of 0.91. This accuracy exceeds narrow-sense heritability, approaches limits imposed by measurement repeatability and is higher than achieved with a single assay in the laboratory. Our results prove that very accurate prediction of complex traits is possible, and suggest that additional data from families rather than reference cohorts may be more useful for this purpose.},
  timestamp = {2017-02-09T19:11:20Z},
  eprinttype = {pubmed},
  eprint = {27160605},
  langid = {english},
  journaltitle = {Nature communications},
  shortjournal = {Nat Commun},
  author = {Märtens, K. and Hallin, J. and Warringer, J. and Liti, G. and Parts, L.},
  date = {2016},
  pages = {11512},
  groups = {review}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  issn = {0007-4985},
  shorttitle = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  timestamp = {2017-02-09T19:11:20Z},
  number = {4},
  journaltitle = {The bulletin of mathematical biophysics},
  author = {McCulloch, Warren S and Pitts, Walter},
  date = {1943},
  pages = {115--133},
  groups = {review}
}

@article{menden_machine_2013,
  title = {Machine Learning Prediction of Cancer Cell Sensitivity to Drugs Based on Genomic and Chemical Properties},
  volume = {8},
  issn = {1932-6203 (Electronic) 1932-6203 (Linking)},
  doi = {10.1371/journal.pone.0061318},
  shorttitle = {Machine Learning Prediction of Cancer Cell Sensitivity to Drugs Based on Genomic and Chemical Properties},
  abstract = {Predicting the response of a specific cancer to a therapy is a major goal in modern oncology that should ultimately lead to a personalised treatment. High-throughput screenings of potentially active compounds against a panel of genomically heterogeneous cancer cell lines have unveiled multiple relationships between genomic alterations and drug responses. Various computational approaches have been proposed to predict sensitivity based on genomic features, while others have used the chemical properties of the drugs to ascertain their effect. In an effort to integrate these complementary approaches, we developed machine learning models to predict the response of cancer cell lines to drug treatment, quantified through IC(5)(0) values, based on both the genomic features of the cell lines and the chemical properties of the considered drugs. Models predicted IC(5)(0) values in a 8-fold cross-validation and an independent blind test with coefficient of determination R(2) of 0.72 and 0.64 respectively. Furthermore, models were able to predict with comparable accuracy (R(2) of 0.61) IC50s of cell lines from a tissue not used in the training stage. Our in silico models can be used to optimise the experimental design of drug-cell screenings by estimating a large proportion of missing IC(5)(0) values rather than experimentally measuring them. The implications of our results go beyond virtual drug screening design: potentially thousands of drugs could be probed in silico to systematically test their potential efficacy as anti-tumour agents based on their structure, thus providing a computational framework to identify new drug repositioning opportunities as well as ultimately be useful for personalized medicine by linking the genomic traits of patients to drug sensitivity.},
  timestamp = {2017-02-09T19:11:20Z},
  eprinttype = {pubmed},
  eprint = {23646105},
  number = {4},
  journaltitle = {PLoS One},
  shortjournal = {PloS one},
  author = {Menden, M. P. and Iorio, F. and Garnett, M. and McDermott, U. and Benes, C. H. and Ballester, P. J. and Saez-Rodriguez, J.},
  date = {2013},
  pages = {e61318},
  keywords = {*Artificial Intelligence,Analysis of Variance,Antineoplastic Agents/pharmacology/therapeutic use,Computer Simulation,Drug Resistance; Neoplasm/*genetics,Genomics/methods,Humans,Inhibitory Concentration 50,Neoplasms/drug therapy/*genetics,Pharmacogenetics/methods,Workflow},
  groups = {review}
}

@book{michalski_machine_2013,
  title = {Machine Learning: {{An}} Artificial Intelligence Approach},
  isbn = {3-662-12405-X},
  shorttitle = {Machine Learning: {{An}} Artificial Intelligence Approach},
  timestamp = {2017-02-09T19:11:20Z},
  publisher = {{Springer Science \& Business Media}},
  author = {Michalski, Ryszard S and Carbonell, Jaime G and Mitchell, Tom M},
  date = {2013},
  groups = {review}
}

@article{mikolov_statistical_2012,
  title = {Statistical Language Models Based on Neural Networks},
  shorttitle = {Statistical Language Models Based on Neural Networks},
  timestamp = {2017-02-09T19:11:20Z},
  journaltitle = {Presentation at Google, Mountain View, 2nd April},
  author = {Mikolov, Tomáš},
  date = {2012},
  groups = {review}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  volume = {518},
  issn = {0028-0836},
  doi = {10.1038/nature14236},
  shorttitle = {Human-Level Control through Deep Reinforcement Learning},
  abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  timestamp = {2017-02-09T19:11:20Z},
  langid = {english},
  journaltitle = {Nature},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02-26},
  pages = {529--533},
  keywords = {Computer science},
  file = {Mnih et al. - 2015 - Human-level control through deep reinforcement lea:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/GD39Q6QS/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf},
  groups = {review}
}

@article{montgomery_transcriptome_2010,
  title = {Transcriptome Genetics Using Second Generation Sequencing in a {{Caucasian}} Population},
  volume = {464},
  issn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  doi = {10.1038/nature08903},
  shorttitle = {Transcriptome Genetics Using Second Generation Sequencing in a {{Caucasian}} Population},
  abstract = {Gene expression is an important phenotype that informs about genetic and environmental effects on cellular state. Many studies have previously identified genetic variants for gene expression phenotypes using custom and commercially available microarrays. Second generation sequencing technologies are now providing unprecedented access to the fine structure of the transcriptome. We have sequenced the mRNA fraction of the transcriptome in 60 extended HapMap individuals of European descent and have combined these data with genetic variants from the HapMap3 project. We have quantified exon abundance based on read depth and have also developed methods to quantify whole transcript abundance. We have found that approximately 10 million reads of sequencing can provide access to the same dynamic range as arrays with better quantification of alternative and highly abundant transcripts. Correlation with SNPs (small nucleotide polymorphisms) leads to a larger discovery of eQTLs (expression quantitative trait loci) than with arrays. We also detect a substantial number of variants that influence the structure of mature transcripts indicating variants responsible for alternative splicing. Finally, measures of allele-specific expression allowed the identification of rare eQTLs and allelic differences in transcript structure. This analysis shows that high throughput sequencing technologies reveal new properties of genetic effects on the transcriptome and allow the exploration of genetic effects in cellular processes.},
  timestamp = {2017-02-09T19:11:22Z},
  eprinttype = {pubmed},
  eprint = {20220756},
  number = {7289},
  journaltitle = {Nature},
  shortjournal = {Nature},
  author = {Montgomery, S. B. and Sammeth, M. and Gutierrez-Arcelus, M. and Lach, R. P. and Ingle, C. and Nisbett, J. and Guigo, R. and Dermitzakis, E. T.},
  date = {2010-04-01},
  pages = {773--7},
  keywords = {Alleles,Alternative Splicing/genetics,European Continental Ancestry Group/*genetics,Exons/genetics,Gene Expression Profiling/*methods,Haplotypes/genetics,Homozygote,Humans,Polymorphism; Single Nucleotide/genetics,Quantitative Trait Loci/genetics,RNA; Messenger/*analysis/*genetics,Sequence Analysis; DNA/*methods},
  groups = {review}
}

@book{murphy_machine_2012,
  title = {Machine Learning: A Probabilistic Perspective},
  isbn = {0-262-01802-0},
  shorttitle = {Machine Learning: A Probabilistic Perspective},
  timestamp = {2017-02-09T19:11:22Z},
  publisher = {{MIT press}},
  author = {Murphy, Kevin P},
  date = {2012},
  groups = {review}
}

@article{nayebi_gruv:_????,
  title = {{{GRUV}}: {{Algorithmic Music Generation}} Using {{Recurrent Neural Networks}}},
  shorttitle = {{{GRUV}}},
  timestamp = {2017-02-09T19:11:22Z},
  author = {Nayebi, Aran and Vitelli, Matt},
  keywords = {notes},
  file = {NayebiAran:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4QD5J73C/NayebiAran.pdf:application/pdf},
  groups = {review}
}

@inproceedings{nesterov_method_1983,
  title = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}} (1/K2)},
  volume = {27},
  shorttitle = {A Method of Solving a Convex Programming Problem with Convergence Rate {{O}} (1/K2)},
  eventtitle = {Soviet Mathematics Doklady},
  timestamp = {2017-02-09T19:11:22Z},
  author = {Nesterov, Yurii},
  date = {1983},
  pages = {372--376},
  note = {2},
  groups = {review}
}

@book{nesterov_introductory_2013,
  title = {Introductory Lectures on Convex Optimization: {{A}} Basic Course},
  volume = {87},
  isbn = {1-4419-8853-X},
  shorttitle = {Introductory Lectures on Convex Optimization: {{A}} Basic Course},
  timestamp = {2017-02-09T19:11:22Z},
  publisher = {{Springer Science \& Business Media}},
  author = {Nesterov, Yurii},
  date = {2013},
  groups = {review}
}

@article{nguyen_multifaceted_2016,
  title = {Multifaceted {{Feature Visualization}}: {{Uncovering}} the {{Different Types}} of {{Features Learned By Each Neuron}} in {{Deep Neural Networks}}},
  shorttitle = {Multifaceted {{Feature Visualization}}},
  abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
  timestamp = {2017-02-09T19:11:22Z},
  journaltitle = {arXiv:1602.03616 [cs]},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2016-02-11},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Nguyen et al. - 2016 - Multifaceted Feature Visualization Uncovering the:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/X2WEWIM7/Nguyen et al. - 2016 - Multifaceted Feature Visualization Uncovering the.pdf:application/pdf},
  groups = {review}
}

@inproceedings{nielsen_convolutional_2015,
  title = {Convolutional {{LSTM Networks}} for {{Subcellular Localization}} of {{Proteins}}},
  shorttitle = {Convolutional {{LSTM Networks}} for {{Subcellular Localization}} of {{Proteins}}},
  eventtitle = {First Annual Danish Bioinformatics Conference},
  timestamp = {2017-02-09T19:11:23Z},
  author = {Nielsen, Henrik and Sønderby, Søren Kaae and Sønderby, Casper Kaae and Winther, Ole},
  date = {2015},
  groups = {review}
}

@article{ning_toward_2005,
  title = {Toward Automatic Phenotyping of Developing Embryos from Videos},
  volume = {14},
  issn = {1057-7149},
  shorttitle = {Toward Automatic Phenotyping of Developing Embryos from Videos},
  timestamp = {2017-02-09T19:11:23Z},
  number = {9},
  journaltitle = {Image Processing, IEEE Transactions on},
  author = {Ning, Feng and Delhomme, Damien and LeCun, Yann and Piano, Fabio and Bottou, Léon and Barbano, Paolo Emilio},
  date = {2005},
  pages = {1360--1371},
  groups = {review}
}

@article{oord_pixel_2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  shorttitle = {Pixel {{Recurrent Neural Networks}}},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  timestamp = {2017-02-09T19:11:23Z},
  journaltitle = {arXiv:1601.06759 [cs]},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  date = {2016-01-25},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  groups = {review}
}

@article{ordonez_deep_2016,
  title = {Deep {{Convolutional}} and {{LSTM Recurrent Neural Networks}} for {{Multimodal Wearable Activity Recognition}}},
  volume = {16},
  issn = {1424-8220},
  doi = {10.3390/s16010115},
  shorttitle = {Deep {{Convolutional}} and {{LSTM Recurrent Neural Networks}} for {{Multimodal Wearable Activity Recognition}}},
  timestamp = {2017-02-09T19:11:23Z},
  langid = {english},
  journaltitle = {Sensors},
  author = {Ordóñez, Francisco and Roggen, Daniel},
  date = {2016-01-18},
  pages = {115},
  file = {sensors-16-00115:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WTAZ7R7P/sensors-16-00115.pdf:application/pdf},
  groups = {review}
}

@inproceedings{pachitariu_extracting_2013,
  title = {Extracting Regions of Interest from Biological Images with Convolutional Sparse Block Coding},
  shorttitle = {Extracting Regions of Interest from Biological Images with Convolutional Sparse Block Coding},
  eventtitle = {Advances in Neural Information Processing Systems},
  timestamp = {2017-02-09T19:11:23Z},
  author = {Pachitariu, Marius and Packer, Adam M and Pettit, Noah and Dalgleish, Henry and Hausser, Michael and Sahani, Maneesh},
  date = {2013},
  pages = {1745--1753},
  groups = {review}
}

@article{park_deep_2015,
  title = {Deep Learning for Regulatory Genomics},
  volume = {33},
  issn = {1087-0156},
  doi = {10.1038/nbt.3313},
  shorttitle = {Deep Learning for Regulatory Genomics},
  abstract = {Computational modeling of DNA and RNA targets of regulatory proteins is improved by a deep-learning approach.},
  timestamp = {2017-02-09T19:11:23Z},
  langid = {english},
  journaltitle = {Nature Biotechnology},
  author = {Park, Yongjin and Kellis, Manolis},
  date = {2015-08},
  pages = {825--826},
  file = {Park and Kellis - 2015 - Deep learning for regulatory genomics:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MSJV5PDP/Park and Kellis - 2015 - Deep learning for regulatory genomics.pdf:application/pdf},
  groups = {review}
}

@article{parnamaa_accurate_2016,
  title = {Accurate Classification of Protein Subcellular Localization from High Throughput Microscopy Images Using Deep Learning},
  doi = {10.1101/050757},
  shorttitle = {Accurate Classification of Protein Subcellular Localization from High Throughput Microscopy Images Using Deep Learning},
  timestamp = {2017-02-09T19:11:24Z},
  journaltitle = {bioRxiv},
  author = {Pärnamaa, T. and Parts, L.},
  date = {2016-04-28},
  pages = {10.1101/050757},
  groups = {review}
}

@article{parts_heritability_2014,
  title = {Heritability and Genetic Basis of Protein Level Variation in an Outbred Population},
  volume = {24},
  issn = {1549-5469 (Electronic) 1088-9051 (Linking)},
  doi = {10.1101/gr.170506.113},
  shorttitle = {Heritability and Genetic Basis of Protein Level Variation in an Outbred Population},
  abstract = {The genetic basis of heritable traits has been studied for decades. Although recent mapping efforts have elucidated genetic determinants of transcript levels, mapping of protein abundance has lagged. Here, we analyze levels of 4084 GFP-tagged yeast proteins in the progeny of a cross between a laboratory and a wild strain using flow cytometry and high-content microscopy. The genotype of trans variants contributed little to protein level variation between individual cells but explained $>$50\% of the variance in the population's average protein abundance for half of the GFP fusions tested. To map trans-acting factors responsible, we performed flow sorting and bulk segregant analysis of 25 proteins, finding a median of five protein quantitative trait loci (pQTLs) per GFP fusion. Further, we find that cis-acting variants predominate; the genotype of a gene and its surrounding region had a large effect on protein level six times more frequently than the rest of the genome combined. We present evidence for both shared and independent genetic control of transcript and protein abundance: More than half of the expression QTLs (eQTLs) contribute to changes in protein levels of regulated genes, but several pQTLs do not affect their cognate transcript levels. Allele replacements of genes known to underlie trans eQTL hotspots confirmed the correlation of effects on mRNA and protein levels. This study represents the first genome-scale measurement of genetic contribution to protein levels in single cells and populations, identifies more than a hundred trans pQTLs, and validates the propagation of effects associated with transcript variation to protein abundance.},
  timestamp = {2017-02-09T19:11:24Z},
  eprinttype = {pubmed},
  eprint = {24823668},
  number = {8},
  journaltitle = {Genome Res},
  shortjournal = {Genome research},
  author = {Parts, L. and Liu, Y. C. and Tekkedil, M. M. and Steinmetz, L. M. and Caudy, A. A. and Fraser, A. G. and Boone, C. and Andrews, B. J. and Rosebrock, A. P.},
  date = {2014-08},
  pages = {1363--70},
  keywords = {Chromosome Mapping,Evolution; Molecular,Gene Expression,Gene Frequency,Genotype,Quantitative Trait Loci,RNA; Fungal/genetics/metabolism,RNA; Messenger/genetics/metabolism,Saccharomyces cerevisiae/*genetics/metabolism,Saccharomyces cerevisiae Proteins/genetics/*metabolism},
  groups = {review}
}

@article{parts_joint_2011,
  title = {Joint Genetic Analysis of Gene Expression Data with Inferred Cellular Phenotypes},
  volume = {7},
  issn = {1553-7404 (Electronic) 1553-7390 (Linking)},
  doi = {10.1371/journal.pgen.1001276},
  shorttitle = {Joint Genetic Analysis of Gene Expression Data with Inferred Cellular Phenotypes},
  abstract = {Even within a defined cell type, the expression level of a gene differs in individual samples. The effects of genotype, measured factors such as environmental conditions, and their interactions have been explored in recent studies. Methods have also been developed to identify unmeasured intermediate factors that coherently influence transcript levels of multiple genes. Here, we show how to bring these two approaches together and analyse genetic effects in the context of inferred determinants of gene expression. We use a sparse factor analysis model to infer hidden factors, which we treat as intermediate cellular phenotypes that in turn affect gene expression in a yeast dataset. We find that the inferred phenotypes are associated with locus genotypes and environmental conditions and can explain genetic associations to genes in trans. For the first time, we consider and find interactions between genotype and intermediate phenotypes inferred from gene expression levels, complementing and extending established results.},
  timestamp = {2017-02-09T19:11:24Z},
  eprinttype = {pubmed},
  eprint = {21283789},
  number = {1},
  journaltitle = {PLoS Genet},
  shortjournal = {PLoS genetics},
  author = {Parts, L. and Stegle, O. and Winn, J. and Durbin, R.},
  date = {2011},
  pages = {e1001276},
  keywords = {Algorithms,Biosynthetic Pathways/genetics,Databases; Protein/statistics & numerical data,Data Interpretation; Statistical,Environment,Epistasis; Genetic/genetics,Gene Expression/*genetics,Gene Regulatory Networks,Genetic Association Studies/*statistics & numerical data,Genetic Variation/genetics,Genotype,Models; Statistical,Phenotype,Quantitative Trait Loci/genetics,Saccharomyces cerevisiae/*cytology/*genetics},
  groups = {review}
}

@incollection{bengio_conditional_2009,
  title = {Conditional {{Neural Fields}}},
  shorttitle = {Conditional {{Neural Fields}}},
  timestamp = {2017-02-09T19:11:24Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  publisher = {{Curran Associates, Inc.}},
  author = {Peng, Jian and Bo, Liefeng and Xu, Jinbo},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  date = {2009},
  pages = {1419--1427},
  keywords = {notes},
  file = {Peng et al. - 2009 - Conditional Neural Fields:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/9EENW9RN/Peng et al. - 2009 - Conditional Neural Fields.pdf:application/pdf},
  groups = {review}
}

@article{pickrell_understanding_2010,
  title = {Understanding Mechanisms Underlying Human Gene Expression Variation with {{RNA}} Sequencing},
  volume = {464},
  issn = {1476-4687 (Electronic) 0028-0836 (Linking)},
  doi = {10.1038/nature08872},
  shorttitle = {Understanding Mechanisms Underlying Human Gene Expression Variation with {{RNA}} Sequencing},
  abstract = {Understanding the genetic mechanisms underlying natural variation in gene expression is a central goal of both medical and evolutionary genetics, and studies of expression quantitative trait loci (eQTLs) have become an important tool for achieving this goal. Although all eQTL studies so far have assayed messenger RNA levels using expression microarrays, recent advances in RNA sequencing enable the analysis of transcript variation at unprecedented resolution. We sequenced RNA from 69 lymphoblastoid cell lines derived from unrelated Nigerian individuals that have been extensively genotyped by the International HapMap Project. By pooling data from all individuals, we generated a map of the transcriptional landscape of these cells, identifying extensive use of unannotated untranslated regions and more than 100 new putative protein-coding exons. Using the genotypes from the HapMap project, we identified more than a thousand genes at which genetic variation influences overall expression levels or splicing. We demonstrate that eQTLs near genes generally act by a mechanism involving allele-specific expression, and that variation that influences the inclusion of an exon is enriched within and near the consensus splice sites. Our results illustrate the power of high-throughput sequencing for the joint analysis of variation in transcription, splicing and allele-specific expression across individuals.},
  timestamp = {2017-02-09T19:11:24Z},
  eprinttype = {pubmed},
  eprint = {20220758},
  number = {7289},
  journaltitle = {Nature},
  shortjournal = {Nature},
  author = {Pickrell, J. K. and Marioni, J. C. and Pai, A. A. and Degner, J. F. and Engelhardt, B. E. and Nkadori, E. and Veyrieras, J. B. and Stephens, M. and Gilad, Y. and Pritchard, J. K.},
  date = {2010-04-01},
  pages = {768--72},
  keywords = {*Gene Expression Profiling,African Continental Ancestry Group/genetics,Alleles,Consensus Sequence/genetics,DNA; Complementary/genetics,Exons/genetics,Gene Expression Regulation/*genetics,Genetic Variation/*genetics,Humans,Nigeria,Polymorphism; Single Nucleotide/genetics,Quantitative Trait Loci/genetics,RNA; Messenger/*analysis/*genetics,RNA Splice Sites/genetics,Sequence Analysis; RNA,Transcription; Genetic/*genetics},
  groups = {review}
}

@article{poon_sum-product_2011,
  title = {Sum-{{Product Networks}}: {{A New Deep Architecture}}},
  shorttitle = {Sum-{{Product Networks}}: {{A New Deep Architecture}}},
  timestamp = {2017-02-09T19:11:24Z},
  author = {Poon, Hoifung and Domingos, Pedro},
  date = {2011},
  keywords = {notes},
  file = {uai11-poon:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/B4RGTIWU/uai11-poon.pdf:application/pdf},
  groups = {review}
}

@article{radford_unsupervised_2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  shorttitle = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  timestamp = {2017-02-09T19:11:25Z},
  journaltitle = {arXiv:1511.06434 [cs]},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2015-11-19},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,notes},
  file = {Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/N8MDJ7BT/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
  groups = {review}
}

@article{rajendran_bridge_2015,
  title = {Bridge {{Correlational Neural Networks}} for {{Multilingual Multimodal Representation Learning}}},
  shorttitle = {Bridge {{Correlational Neural Networks}} for {{Multilingual Multimodal Representation Learning}}},
  abstract = {Recently there has been a lot of interest in learning common representations for multiple views of data. These views could belong to different modalities or languages. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V1 and V2) but parallel data is available between each of these views and a pivot view (V3). We propose a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3. The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) Transfer learning between languages L1,L2,...,Ln using a pivot language L and (ii) cross modal access between images and a language L1 using a pivot language L2. We evaluate our model using two datasets : (i) publicly available multilingual TED corpus and (ii) a new multilingual multimodal dataset created and released as a part of this work. On both these datasets, our model outperforms state of the art approaches.},
  timestamp = {2017-02-09T19:11:25Z},
  journaltitle = {arXiv:1510.03519 [cs]},
  author = {Rajendran, Janarthanan and Khapra, Mitesh M. and Chandar, Sarath and Ravindran, Balaraman},
  date = {2015-10-12},
  keywords = {Computer Science - Computation and Language,notes},
  file = {Rajendran et al. - 2015 - Bridge Correlational Neural Networks for Multiling:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZSP8ECVQ/Rajendran et al. - 2015 - Bridge Correlational Neural Networks for Multiling.pdf:application/pdf},
  groups = {review}
}

@article{rakitsch_modelling_2016,
  title = {Modelling Local Gene Networks Increases Power to Detect Trans-Acting Genetic Effects on Gene Expression},
  volume = {17},
  issn = {1474-760X (Electronic) 1474-7596 (Linking)},
  doi = {10.1186/s13059-016-0895-2},
  shorttitle = {Modelling Local Gene Networks Increases Power to Detect Trans-Acting Genetic Effects on Gene Expression},
  abstract = {Expression quantitative trait loci (eQTL) mapping is a widely used tool to study the genetics of gene expression. Confounding factors and the burden of multiple testing limit the ability to map distal trans eQTLs, which is important to understand downstream genetic effects on genes and pathways. We propose a two-stage linear mixed model that first learns local directed gene-regulatory networks to then condition on the expression levels of selected genes. We show that this covariate selection approach controls for confounding factors and regulatory context, thereby increasing eQTL detection power and improving the consistency between studies. GNet-LMM is available at: https://github.com/PMBio/GNetLMM .},
  timestamp = {2017-02-09T19:11:26Z},
  eprinttype = {pubmed},
  eprint = {26911988},
  number = {1},
  journaltitle = {Genome Biol},
  shortjournal = {Genome biology},
  author = {Rakitsch, B. and Stegle, O.},
  date = {2016},
  pages = {33},
  groups = {review}
}

@article{rampasek_tensorflow:_2016,
  title = {{{TensorFlow}}: {{Biology}}’s {{Gateway}} to {{Deep Learning}}?},
  volume = {2},
  issn = {24054712},
  doi = {10.1016/j.cels.2016.01.009},
  shorttitle = {{{TensorFlow}}},
  timestamp = {2017-02-09T19:11:26Z},
  langid = {english},
  journaltitle = {Cell Systems},
  author = {Rampasek, Ladislav and Goldenberg, Anna},
  date = {2016-01},
  pages = {12--14},
  file = {tensorflow:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4PMRUUX3/tensorflow.pdf:application/pdf},
  groups = {review}
}

@article{ramsundar_massively_2015,
  title = {Massively {{Multitask Networks}} for {{Drug Discovery}}},
  shorttitle = {Massively {{Multitask Networks}} for {{Drug Discovery}}},
  abstract = {Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process.},
  timestamp = {2017-02-09T19:11:26Z},
  journaltitle = {arXiv:1502.02072 [cs, stat]},
  author = {Ramsundar, Bharath and Kearnes, Steven and Riley, Patrick and Webster, Dale and Konerding, David and Pande, Vijay},
  date = {2015-02-06},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {Ramsundar et al. - 2015 - Massively Multitask Networks for Drug Discovery:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/WE2AU8F8/Ramsundar et al. - 2015 - Massively Multitask Networks for Drug Discovery.pdf:application/pdf},
  groups = {review}
}

@article{ranzato_sequence_2015,
  title = {Sequence {{Level Training}} with {{Recurrent Neural Networks}}},
  shorttitle = {Sequence {{Level Training}} with {{Recurrent Neural Networks}}},
  abstract = {Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster.},
  timestamp = {2017-02-09T19:11:27Z},
  journaltitle = {arXiv:1511.06732 [cs]},
  author = {Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
  date = {2015-11-20},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,notes},
  file = {Ranzato et al. - 2015 - Sequence Level Training with Recurrent Neural Netw:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VJRSEVAT/Ranzato et al. - 2015 - Sequence Level Training with Recurrent Neural Netw.pdf:application/pdf},
  groups = {review}
}

@article{rasmus_denoising_2014,
  title = {Denoising Autoencoder with Modulated Lateral Connections Learns Invariant Representations of Natural Images},
  shorttitle = {Denoising Autoencoder with Modulated Lateral Connections Learns Invariant Representations of Natural Images},
  abstract = {Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.},
  timestamp = {2017-02-09T19:11:28Z},
  journaltitle = {arXiv:1412.7210 [cs, stat]},
  author = {Rasmus, Antti and Raiko, Tapani and Valpola, Harri},
  date = {2014-12-22},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes,Statistics - Machine Learning},
  file = {Rasmus et al. - 2014 - Denoising autoencoder with modulated lateral conne:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5CTEKBMQ/Rasmus et al. - 2014 - Denoising autoencoder with modulated lateral conne.pdf:application/pdf},
  groups = {review}
}

@article{rasmus_semi-supervised_2015,
  title = {Semi-{{Supervised Learning}} with {{Ladder Network}}},
  shorttitle = {Semi-{{Supervised Learning}} with {{Ladder Network}}},
  abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.},
  timestamp = {2017-02-09T19:11:30Z},
  journaltitle = {arXiv:1507.02672 [cs, stat]},
  author = {Rasmus, Antti and Valpola, Harri and Honkala, Mikko and Berglund, Mathias and Raiko, Tapani},
  date = {2015-07-09},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes,Statistics - Machine Learning},
  file = {Rasmus et al. - 2015 - Semi-Supervised Learning with Ladder Network:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/DRRPBVTP/Rasmus et al. - 2015 - Semi-Supervised Learning with Ladder Network.pdf:application/pdf},
  groups = {review}
}

@inproceedings{razavian_cnn_2014,
  title = {{{CNN}} Features off-the-Shelf: An Astounding Baseline for Recognition},
  shorttitle = {{{CNN}} Features off-the-Shelf: An Astounding Baseline for Recognition},
  eventtitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  timestamp = {2017-02-09T19:11:30Z},
  author = {Razavian, Ali and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
  date = {2014},
  pages = {806--813},
  groups = {review}
}

@incollection{ronneberger_u-net:_2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  isbn = {3-319-24573-2},
  shorttitle = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  timestamp = {2017-02-09T19:11:30Z},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}–{{MICCAI}} 2015},
  publisher = {{Springer}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015},
  pages = {234--241},
  groups = {review}
}

@article{rosenblatt_perceptron:_1958,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  volume = {65},
  issn = {1939-1471},
  shorttitle = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  timestamp = {2017-02-09T19:11:30Z},
  number = {6},
  journaltitle = {Psychological review},
  author = {Rosenblatt, Frank},
  date = {1958},
  pages = {386},
  groups = {review}
}

@article{rumelhart_learning_1988,
  title = {Learning Representations by Back-Propagating Errors},
  volume = {5},
  shorttitle = {Learning Representations by Back-Propagating Errors},
  timestamp = {2017-02-09T19:11:30Z},
  number = {3},
  journaltitle = {Cognitive modeling},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  date = {1988},
  pages = {1},
  groups = {review}
}

@article{russakovsky_imagenet_2015,
  title = {Imagenet Large Scale Visual Recognition Challenge},
  volume = {115},
  issn = {0920-5691},
  shorttitle = {Imagenet Large Scale Visual Recognition Challenge},
  timestamp = {2017-02-09T19:11:30Z},
  number = {3},
  journaltitle = {International Journal of Computer Vision},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael},
  date = {2015},
  pages = {211--252},
  groups = {review}
}

@article{salakhutdinov_efficient_2012,
  title = {An Efficient Learning Procedure for Deep {{Boltzmann}} Machines},
  volume = {24},
  issn = {0899-7667},
  shorttitle = {An Efficient Learning Procedure for Deep {{Boltzmann}} Machines},
  timestamp = {2017-02-09T19:11:31Z},
  number = {8},
  journaltitle = {Neural computation},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  date = {2012},
  pages = {1967--2006},
  groups = {review}
}

@inproceedings{salakhutdinov_efficient_2010,
  title = {Efficient Learning of Deep {{Boltzmann}} Machines},
  shorttitle = {Efficient Learning of Deep {{Boltzmann}} Machines},
  eventtitle = {International Conference on Artificial Intelligence and Statistics},
  timestamp = {2017-02-09T19:11:31Z},
  author = {Salakhutdinov, Ruslan and Larochelle, Hugo},
  date = {2010},
  pages = {693--700},
  groups = {review}
}

@article{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  timestamp = {2017-02-09T19:11:31Z},
  journaltitle = {arXiv:1602.07868 [cs]},
  author = {Salimans, Tim and Kingma, Diederik P.},
  date = {2016-02-25},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  groups = {review}
}

@article{schmidhuber_deep_2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  volume = {61},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  timestamp = {2017-02-09T19:11:31Z},
  journaltitle = {Neural Networks},
  author = {Schmidhuber, Juergen},
  date = {2015-01},
  pages = {85--117},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/BDRZQZV4/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf},
  groups = {review}
}

@inproceedings{seltzer_multi-task_2013,
  title = {Multi-Task Learning in Deep Neural Networks for Improved Phoneme Recognition},
  shorttitle = {Multi-Task Learning in Deep Neural Networks for Improved Phoneme Recognition},
  timestamp = {2017-02-09T19:11:33Z},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}} ({{ICASSP}}), 2013 {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  author = {Seltzer, Michael L. and Droppo, Jasha},
  date = {2013},
  pages = {6965--6969},
  groups = {review}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  volume = {529},
  issn = {0028-0836},
  doi = {10.1038/nature16961},
  shorttitle = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  timestamp = {2017-02-09T19:11:33Z},
  langid = {english},
  journaltitle = {Nature},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01-28},
  pages = {484--489},
  keywords = {Computational science,Computer science,notes,Reward},
  options = {useprefix=true},
  file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2BUU3EU6/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
  groups = {review}
}

@article{simonyan_deep_2013,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  timestamp = {2017-02-09T19:11:34Z},
  journaltitle = {arXiv preprint arXiv:1312.6034},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2013-12-20},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notes},
  file = {Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8HMRBFZG/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
  groups = {review}
}

@article{simonyan_very_2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  shorttitle = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  timestamp = {2017-02-09T19:11:34Z},
  journaltitle = {arXiv preprint arXiv:1409.1556},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  groups = {review}
}

@inproceedings{snoek_practical_2012,
  title = {Practical Bayesian Optimization of Machine Learning Algorithms},
  shorttitle = {Practical Bayesian Optimization of Machine Learning Algorithms},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:11:34Z},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  date = {2012},
  pages = {2951--2959},
  groups = {review}
}

@article{snoek_scalable_2015,
  title = {Scalable {{Bayesian Optimization Using Deep Neural Networks}}},
  shorttitle = {Scalable {{Bayesian Optimization Using Deep Neural Networks}}},
  abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
  timestamp = {2017-02-09T19:11:34Z},
  journaltitle = {arXiv:1502.05700 [stat]},
  author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and {Prabhat} and Adams, Ryan P.},
  date = {2015-02-19},
  keywords = {Statistics - Machine Learning},
  file = {Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/B6KBMFHS/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N.pdf:application/pdf},
  groups = {review}
}

@article{socher_zero-shot_2013,
  title = {Zero-{{Shot Learning Through Cross}}-{{Modal Transfer}}},
  shorttitle = {Zero-{{Shot Learning Through Cross}}-{{Modal Transfer}}},
  abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.},
  timestamp = {2017-02-09T19:11:36Z},
  journaltitle = {arXiv:1301.3666 [cs]},
  author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
  date = {2013-01-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,notes},
  file = {Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/656BRDE2/Socher et al. - 2013 - Zero-Shot Learning Through Cross-Modal Transfer.pdf:application/pdf},
  groups = {review}
}

@article{sonderby_protein_2014,
  title = {Protein {{Secondary Structure Prediction}} with {{Long Short Term Memory Networks}}},
  shorttitle = {Protein {{Secondary Structure Prediction}} with {{Long Short Term Memory Networks}}},
  abstract = {Prediction of protein secondary structure from the amino acid sequence is a classical bioinformatics problem. Common methods use feed forward neural networks or SVMs combined with a sliding window, as these models does not naturally handle sequential data. Recurrent neural networks are an generalization of the feed forward neural network that naturally handle sequential data. We use a bidirectional recurrent neural network with long short term memory cells for prediction of secondary structure and evaluate using the CB513 dataset. On the secondary structure 8-class problem we report better performance (0.674) than state of the art (0.664). Our model includes feed forward networks between the long short term memory cells, a path that can be further explored.},
  timestamp = {2017-02-09T19:11:37Z},
  journaltitle = {ArXiv preprint arXiv:1412.7828},
  author = {Sønderby, Søren Kaae and Winther, Ole},
  date = {2014-12-25},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Quantitative Methods},
  file = {Sønderby and Winther - 2014 - Protein Secondary Structure Prediction with Long S:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/NV8EVF46/Sønderby and Winther - 2014 - Protein Secondary Structure Prediction with Long S.pdf:application/pdf},
  groups = {review}
}

@article{sordoni_hierarchical_2015,
  title = {A {{Hierarchical Recurrent Encoder}}-{{Decoder For Generative Context}}-{{Aware Query Suggestion}}},
  shorttitle = {A {{Hierarchical Recurrent Encoder}}-{{Decoder For Generative Context}}-{{Aware Query Suggestion}}},
  abstract = {Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a probabilistic suggestion model that is able to account for sequences of previous queries of arbitrary lengths. Our novel hierarchical recurrent encoder-decoder architecture allows the model to be sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that it outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our model is general enough to be used in a variety of other applications.},
  timestamp = {2017-02-09T19:11:37Z},
  journaltitle = {arXiv:1507.02221 [cs]},
  author = {Sordoni, Alessandro and Bengio, Yoshua and Vahabi, Hossein and Lioma, Christina and Simonsen, Jakob G. and Nie, Jian-Yun},
  date = {2015-07-08},
  keywords = {Computer Science - Information Retrieval,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Sordoni et al. - 2015 - A Hierarchical Recurrent Encoder-Decoder For Gener:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2F3G5MU3/Sordoni et al. - 2015 - A Hierarchical Recurrent Encoder-Decoder For Gener.pdf:application/pdf},
  groups = {review}
}

@article{spencer_deep_2015,
  title = {A Deep Learning Network Approach to Ab Initio Protein Secondary Structure Prediction},
  volume = {12},
  issn = {1545-5963},
  shorttitle = {A Deep Learning Network Approach to Ab Initio Protein Secondary Structure Prediction},
  timestamp = {2017-02-09T19:11:39Z},
  number = {1},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)},
  author = {Spencer, Matt and Eickholt, Jesse and Cheng, Jianlin},
  date = {2015},
  pages = {103--112},
  groups = {review}
}

@article{springenberg_unsupervised_2015,
  title = {Unsupervised and {{Semi}}-Supervised {{Learning}} with {{Categorical Generative Adversarial Networks}}},
  shorttitle = {Unsupervised and {{Semi}}-Supervised {{Learning}} with {{Categorical Generative Adversarial Networks}}},
  abstract = {In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).},
  timestamp = {2017-02-09T19:11:39Z},
  journaltitle = {arXiv:1511.06390 [cs, stat]},
  author = {Springenberg, Jost Tobias},
  date = {2015-11-19},
  keywords = {Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Springenberg - 2015 - Unsupervised and Semi-supervised Learning with Cat:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/G4IHNQ3B/Springenberg - 2015 - Unsupervised and Semi-supervised Learning with Cat.pdf:application/pdf},
  groups = {review}
}

@article{springenberg_striving_2014,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  timestamp = {2017-02-09T19:11:52Z},
  journaltitle = {ArXiv preprint arXiv:1412.6806},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2014-12-21},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/56TDER5S/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf:application/pdf},
  groups = {review}
}

@article{srinivas_gaussian_2012,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}: {{No Regret}} and {{Experimental Design}}},
  volume = {58},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2011.2182033},
  shorttitle = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  timestamp = {2017-02-09T19:11:52Z},
  journaltitle = {IEEE Transactions on Information Theory},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias},
  date = {2012-05},
  pages = {3250--3265},
  keywords = {Computer Science - Learning},
  file = {Srinivas et al. - 2012 - Gaussian Process Optimization in the Bandit Settin:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/JP9R9RDH/Srinivas et al. - 2012 - Gaussian Process Optimization in the Bandit Settin.pdf:application/pdf},
  groups = {review}
}

@article{srivastava_dropout:_2014,
  title = {Dropout: {{A}} Simple Way to Prevent Neural Networks from Overfitting},
  volume = {15},
  shorttitle = {Dropout},
  timestamp = {2017-02-09T19:11:54Z},
  journaltitle = {The Journal of Machine Learning Research},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  date = {2014},
  pages = {1929--1958},
  file = {srivastava14a:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/N9AVRF7C/srivastava14a.pdf:application/pdf},
  groups = {review}
}

@article{srivastava_unsupervised_2015,
  title = {Unsupervised {{Learning}} of {{Video Representations}} Using {{LSTMs}}},
  shorttitle = {Unsupervised {{Learning}} of {{Video Representations}} Using {{LSTMs}}},
  abstract = {We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations ("percepts") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.},
  timestamp = {2017-02-09T19:11:55Z},
  journaltitle = {arXiv:1502.04681 [cs]},
  author = {Srivastava, Nitish and Mansimov, Elman and Salakhutdinov, Ruslan},
  date = {2015-02-16},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Srivastava et al. - 2015 - Unsupervised Learning of Video Representations usi:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/V2HRW7BM/Srivastava et al. - 2015 - Unsupervised Learning of Video Representations usi.pdf:application/pdf},
  groups = {review}
}

@inproceedings{srivastava_multimodal_2012,
  title = {Multimodal Learning with Deep Boltzmann Machines},
  shorttitle = {Multimodal Learning with Deep Boltzmann Machines},
  timestamp = {2017-02-09T19:11:55Z},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Srivastava, Nitish and Salakhutdinov, Ruslan R.},
  date = {2012},
  pages = {2222--2230},
  keywords = {notes},
  file = {4683-multimodal-learning-with-deep-boltzmann-machines:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/5G5Z4IZ7/4683-multimodal-learning-with-deep-boltzmann-machines.pdf:application/pdf},
  groups = {review}
}

@article{srivastava_training_2015,
  title = {Training {{Very Deep Networks}}},
  shorttitle = {Training {{Very Deep Networks}}},
  abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
  timestamp = {2017-02-09T19:11:56Z},
  journaltitle = {arXiv:1507.06228 [cs]},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
  date = {2015-07-22},
  keywords = {68T01,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,G.1.6,I.2.6,notes},
  file = {Srivastava et al. - 2015 - Training Very Deep Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VKDJ32A3/Srivastava et al. - 2015 - Training Very Deep Networks.pdf:application/pdf},
  groups = {review}
}

@article{stegle_bayesian_2010,
  title = {A {{Bayesian}} Framework to Account for Complex Non-Genetic Factors in Gene Expression Levels Greatly Increases Power in {{eQTL}} Studies},
  volume = {6},
  issn = {1553-7358 (Electronic) 1553-734X (Linking)},
  doi = {10.1371/journal.pcbi.1000770},
  shorttitle = {A {{Bayesian}} Framework to Account for Complex Non-Genetic Factors in Gene Expression Levels Greatly Increases Power in {{eQTL}} Studies},
  abstract = {Gene expression measurements are influenced by a wide range of factors, such as the state of the cell, experimental conditions and variants in the sequence of regulatory regions. To understand the effect of a variable of interest, such as the genotype of a locus, it is important to account for variation that is due to confounding causes. Here, we present VBQTL, a probabilistic approach for mapping expression quantitative trait loci (eQTLs) that jointly models contributions from genotype as well as known and hidden confounding factors. VBQTL is implemented within an efficient and flexible inference framework, making it fast and tractable on large-scale problems. We compare the performance of VBQTL with alternative methods for dealing with confounding variability on eQTL mapping datasets from simulations, yeast, mouse, and human. Employing Bayesian complexity control and joint modelling is shown to result in more precise estimates of the contribution of different confounding factors resulting in additional associations to measured transcript levels compared to alternative approaches. We present a threefold larger collection of cis eQTLs than previously found in a whole-genome eQTL scan of an outbred human population. Altogether, 27\% of the tested probes show a significant genetic association in cis, and we validate that the additional eQTLs are likely to be real by replicating them in different sets of individuals. Our method is the next step in the analysis of high-dimensional phenotype data, and its application has revealed insights into genetic regulation of gene expression by demonstrating more abundant cis-acting eQTLs in human than previously shown. Our software is freely available online at http://www.sanger.ac.uk/resources/software/peer/.},
  timestamp = {2017-02-09T19:11:56Z},
  eprinttype = {pubmed},
  eprint = {20463871},
  number = {5},
  journaltitle = {PLoS Comput Biol},
  shortjournal = {PLoS computational biology},
  author = {Stegle, O. and Parts, L. and Durbin, R. and Winn, J.},
  date = {2010-05},
  pages = {e1000770},
  keywords = {*Bayes Theorem,*Gene Expression,*Models; Genetic,*Quantitative Trait Loci,*Software,Animals,Databases; Genetic,Humans,Internet,Markov Chains,Mice,Models; Statistical,Phenotype,Reproducibility of Results,Yeasts},
  groups = {review}
}

@article{stegle_using_2012,
  title = {Using Probabilistic Estimation of Expression Residuals ({{PEER}}) to Obtain Increased Power and Interpretability of Gene Expression Analyses},
  volume = {7},
  issn = {1750-2799 (Electronic) 1750-2799 (Linking)},
  doi = {10.1038/nprot.2011.457},
  shorttitle = {Using Probabilistic Estimation of Expression Residuals ({{PEER}}) to Obtain Increased Power and Interpretability of Gene Expression Analyses},
  abstract = {We present PEER (probabilistic estimation of expression residuals), a software package implementing statistical models that improve the sensitivity and interpretability of genetic associations in population-scale expression data. This approach builds on factor analysis methods that infer broad variance components in the measurements. PEER takes as input transcript profiles and covariates from a set of individuals, and then outputs hidden factors that explain much of the expression variability. Optionally, these factors can be interpreted as pathway or transcription factor activations by providing prior information about which genes are involved in the pathway or targeted by the factor. The inferred factors are used in genetic association analyses. First, they are treated as additional covariates, and are included in the model to increase detection power for mapping expression traits. Second, they are analyzed as phenotypes themselves to understand the causes of global expression variability. PEER extends previous related surrogate variable models and can be implemented within hours on a desktop computer.},
  timestamp = {2017-02-09T19:11:56Z},
  eprinttype = {pubmed},
  eprint = {22343431},
  number = {3},
  journaltitle = {Nat Protoc},
  shortjournal = {Nature protocols},
  author = {Stegle, O. and Parts, L. and Piipari, M. and Winn, J. and Durbin, R.},
  date = {2012-03},
  pages = {500--7},
  keywords = {*Algorithms,*Models; Statistical,*Software,Factor Analysis; Statistical,Gene Expression Profiling/*methods/statistics & numerical data,Genetic Association Studies/*methods,Sensitivity and Specificity},
  groups = {review}
}

@article{stormo_use_1982,
  title = {Use of the ‘{{Perceptron}}’Algorithm to Distinguish Translational Initiation Sites in {{E}}. Coli},
  volume = {10},
  issn = {0305-1048},
  shorttitle = {Use of the ‘{{Perceptron}}’Algorithm to Distinguish Translational Initiation Sites in {{E}}. Coli},
  timestamp = {2017-02-09T19:11:56Z},
  number = {9},
  journaltitle = {Nucleic Acids Research},
  author = {Stormo, Gary D and Schneider, Thomas D and Gold, Larry and Ehrenfeucht, Andrzej},
  date = {1982},
  pages = {2997--3011},
  groups = {review}
}

@inproceedings{sun_hybrid_2010,
  title = {Hybrid {{HMM}}/{{BLSTM}}-{{RNN}} for Robust Speech Recognition},
  shorttitle = {Hybrid {{HMM}}/{{BLSTM}}-{{RNN}} for Robust Speech Recognition},
  timestamp = {2017-02-09T19:11:56Z},
  booktitle = {Text, {{Speech}} and {{Dialogue}}},
  publisher = {{Springer}},
  author = {Sun, Yang and Ten Bosch, Louis and Boves, Lou},
  date = {2010},
  pages = {400--407},
  file = {Tsd211a:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/7FN8IGZZ/Tsd211a.pdf:application/pdf},
  groups = {review}
}

@thesis{sutskever_training_2013,
  title = {Training Recurrent Neural Networks},
  shorttitle = {Training Recurrent Neural Networks},
  timestamp = {2017-02-09T19:11:57Z},
  institution = {{University of Toronto}},
  author = {Sutskever, Ilya},
  date = {2013},
  groups = {review}
}

@incollection{koller_recurrent_2009,
  title = {The {{Recurrent Temporal Restricted Boltzmann Machine}}},
  shorttitle = {The {{Recurrent Temporal Restricted Boltzmann Machine}}},
  timestamp = {2017-02-09T19:11:57Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  publisher = {{Curran Associates, Inc.}},
  author = {Sutskever, Ilya and Hinton, Geoffrey E. and Taylor, Graham W.},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  date = {2009},
  pages = {1601--1608},
  keywords = {notes},
  file = {Sutskever et al. - 2009 - The Recurrent Temporal Restricted Boltzmann Machin:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/DDXUZTMU/Sutskever et al. - 2009 - The Recurrent Temporal Restricted Boltzmann Machin.pdf:application/pdf},
  groups = {review}
}

@inproceedings{sutskever_importance_2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  shorttitle = {On the Importance of Initialization and Momentum in Deep Learning},
  eventtitle = {Proceedings of the 30th international conference on machine learning (ICML-13)},
  timestamp = {2017-02-09T19:11:57Z},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  date = {2013},
  pages = {1139--1147},
  groups = {review}
}

@inproceedings{sutskever_sequence_2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  shorttitle = {Sequence to Sequence Learning with Neural Networks},
  eventtitle = {Advances in neural information processing systems},
  timestamp = {2017-02-09T19:11:57Z},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  date = {2014},
  pages = {3104--3112},
  groups = {review}
}

@article{swan_application_2013,
  title = {Application of Machine Learning to Proteomics Data: Classification and Biomarker Identification in Postgenomics Biology},
  volume = {17},
  issn = {1557-8100 (Electronic) 1536-2310 (Linking)},
  doi = {10.1089/omi.2013.0017},
  shorttitle = {Application of Machine Learning to Proteomics Data: Classification and Biomarker Identification in Postgenomics Biology},
  abstract = {Mass spectrometry is an analytical technique for the characterization of biological samples and is increasingly used in omics studies because of its targeted, nontargeted, and high throughput abilities. However, due to the large datasets generated, it requires informatics approaches such as machine learning techniques to analyze and interpret relevant data. Machine learning can be applied to MS-derived proteomics data in two ways. First, directly to mass spectral peaks and second, to proteins identified by sequence database searching, although relative protein quantification is required for the latter. Machine learning has been applied to mass spectrometry data from different biological disciplines, particularly for various cancers. The aims of such investigations have been to identify biomarkers and to aid in diagnosis, prognosis, and treatment of specific diseases. This review describes how machine learning has been applied to proteomics tandem mass spectrometry data. This includes how it can be used to identify proteins suitable for use as biomarkers of disease and for classification of samples into disease or treatment groups, which may be applicable for diagnostics. It also includes the challenges faced by such investigations, such as prediction of proteins present, protein quantification, planning for the use of machine learning, and small sample sizes.},
  timestamp = {2017-02-09T19:11:57Z},
  eprinttype = {pubmed},
  eprint = {24116388},
  number = {12},
  journaltitle = {OMICS},
  shortjournal = {Omics : a journal of integrative biology},
  author = {Swan, A. L. and Mobasheri, A. and Allaway, D. and Liddell, S. and Bacardit, J.},
  date = {2013-12},
  pages = {595--610},
  keywords = {*Artificial Intelligence,Biomarkers/metabolism,Humans,Mass Spectrometry,Proteome/classification/*metabolism,Proteomics,Tandem Mass Spectrometry},
  groups = {review}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  shorttitle = {Going Deeper with Convolutions},
  eventtitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  timestamp = {2017-02-09T19:11:57Z},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2015},
  pages = {1--9},
  groups = {review}
}

@article{szegedy_rethinking_2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  shorttitle = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  timestamp = {2017-02-09T19:11:57Z},
  journaltitle = {arXiv preprint arXiv:1512.00567},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  date = {2015},
  groups = {review}
}

@article{team_theano:_2016,
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  shorttitle = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  timestamp = {2017-02-09T19:11:57Z},
  journaltitle = {arXiv preprint arXiv:1605.02688},
  author = {Team, The Theano Development and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Frédéric and Bayer, Justin and Belikov, Anatoly},
  date = {2016},
  groups = {review}
}

@article{theis_generative_2015,
  title = {Generative {{Image Modeling Using Spatial LSTMs}}},
  shorttitle = {Generative {{Image Modeling Using Spatial LSTMs}}},
  abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
  timestamp = {2017-02-09T19:11:57Z},
  journaltitle = {arXiv:1506.03478 [cs, stat]},
  author = {Theis, Lucas and Bethge, Matthias},
  date = {2015-06-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Theis and Bethge - 2015 - Generative Image Modeling Using Spatial LSTMs:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/PP2NC8R4/Theis and Bethge - 2015 - Generative Image Modeling Using Spatial LSTMs.pdf:application/pdf},
  groups = {review}
}

@article{thornton_auto-weka:_2012,
  title = {Auto-{{WEKA}}: {{Combined Selection}} and {{Hyperparameter Optimization}} of {{Classification Algorithms}}},
  shorttitle = {Auto-{{WEKA}}},
  abstract = {Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.},
  timestamp = {2017-02-09T19:11:57Z},
  journaltitle = {arXiv:1208.3719 [cs]},
  author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  date = {2012-08-17},
  keywords = {Computer Science - Learning,D.2.10,I.2.2,I.2.6,notes},
  file = {Thornton et al. - 2012 - Auto-WEKA Combined Selection and Hyperparameter O:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/A9N75H9M/Thornton et al. - 2012 - Auto-WEKA Combined Selection and Hyperparameter O.pdf:application/pdf},
  groups = {review}
}

@article{tran_recurrent_2016,
  title = {Recurrent {{Memory Network}} for {{Language Modeling}}},
  shorttitle = {Recurrent {{Memory Network}} for {{Language Modeling}}},
  abstract = {Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform in-depth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2\% accuracy, surpassing the previous state-of-the-art by a large margin.},
  timestamp = {2017-02-09T19:11:59Z},
  journaltitle = {arXiv:1601.01272 [cs]},
  author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
  date = {2016-01-06},
  keywords = {Computer Science - Computation and Language,notes},
  file = {Tran et al. - 2016 - Recurrent Memory Network for Language Modeling:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/4PQWJKDQ/Tran et al. - 2016 - Recurrent Memory Network for Language Modeling.pdf:application/pdf},
  groups = {review}
}

@article{tu_coverage-based_2016,
  title = {Coverage-Based {{Neural Machine Translation}}},
  shorttitle = {Coverage-Based {{Neural Machine Translation}}},
  abstract = {Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.},
  timestamp = {2017-02-09T19:11:59Z},
  journaltitle = {arXiv:1601.04811 [cs]},
  author = {Tu, Zhaopeng and Lu, Zhengdong and Liu, Yang and Liu, Xiaohua and Li, Hang},
  date = {2016-01-19},
  keywords = {Computer Science - Computation and Language},
  file = {Tu et al. - 2016 - Coverage-based Neural Machine Translation:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/SCPZFB9I/Tu et al. - 2016 - Coverage-based Neural Machine Translation.pdf:application/pdf},
  groups = {review}
}

@inproceedings{unterthiner_deep_2015,
  title = {Deep Learning as an Opportunity in Virtual Screening},
  shorttitle = {Deep Learning as an Opportunity in Virtual Screening},
  timestamp = {2017-02-09T19:12:00Z},
  booktitle = {Deep {{Learning}} and {{Representation Learning Workshop}}, {{NIPS}}},
  author = {Unterthiner, Thomas and Mayr, Andreas and ünter Klambauer, G. and Steijaert, Marvin and Wegner, Jörg K. and Ceulemans, Hugo and Hochreiter, Sepp},
  date = {2015},
  options = {useprefix=true},
  file = {Deep Learning as an Opportunity in Virtual Screening:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/XM2HR5GG/Deep Learning as an Opportunity in Virtual Screening.pdf:application/pdf},
  groups = {review}
}

@article{vincent_stacked_2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  volume = {11},
  issn = {1532-4435},
  shorttitle = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  timestamp = {2017-02-09T19:12:00Z},
  journaltitle = {The Journal of Machine Learning Research},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  date = {2010},
  pages = {3371--3408},
  groups = {review}
}

@article{vinyals_pointer_2015,
  title = {Pointer {{Networks}}},
  shorttitle = {Pointer {{Networks}}},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  timestamp = {2017-02-09T19:12:00Z},
  journaltitle = {arXiv:1506.03134 [cs, stat]},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  date = {2015-06-09},
  keywords = {Computer Science - Computational Geometry,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes,Statistics - Machine Learning},
  file = {Vinyals et al. - 2015 - Pointer Networks:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/I45CD65T/Vinyals et al. - 2015 - Pointer Networks.pdf:application/pdf},
  groups = {review}
}

@inproceedings{vinyals_show_2015,
  title = {Show and Tell: {{A}} Neural Image Caption Generator},
  shorttitle = {Show and Tell: {{A}} Neural Image Caption Generator},
  eventtitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  timestamp = {2017-02-09T19:12:00Z},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  date = {2015},
  pages = {3156--3164},
  groups = {review}
}

@inproceedings{wang_chromatin_2015,
  title = {Chromatin and Genomic Determinants of Alternative Splicing},
  isbn = {1-4503-3853-4},
  shorttitle = {Chromatin and Genomic Determinants of Alternative Splicing},
  eventtitle = {Proceedings of the 6th ACM Conference on Bioinformatics, Computational Biology and Health Informatics},
  timestamp = {2017-02-09T19:12:00Z},
  publisher = {{ACM}},
  author = {Wang, Kun and Cao, Kan and Hannenhalli, Sridhar},
  date = {2015},
  pages = {345--354},
  groups = {review}
}

@article{wang_predicting_2013,
  title = {Predicting Drug-Target Interactions Using Restricted {{Boltzmann}} Machines},
  volume = {29},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btt234},
  shorttitle = {Predicting Drug-Target Interactions Using Restricted {{Boltzmann}} Machines},
  abstract = {Motivation: In silico prediction of drug-target interactions plays an important role toward identifying and developing new uses of existing or abandoned drugs. Network-based approaches have recently become a popular tool for discovering new drug-target interactions (DTIs). Unfortunately, most of these network-based approaches can only predict binary interactions between drugs and targets, and information about different types of interactions has not been well exploited for DTI prediction in previous studies. On the other hand, incorporating additional information about drug-target relationships or drug modes of action can improve prediction of DTIs. Furthermore, the predicted types of DTIs can broaden our understanding about the molecular basis of drug action.
Results: We propose a first machine learning approach to integrate multiple types of DTIs and predict unknown drug-target relationships or drug modes of action. We cast the new DTI prediction problem into a two-layer graphical model, called restricted Boltzmann machine, and apply a practical learning algorithm to train our model and make predictions. Tests on two public databases show that our restricted Boltzmann machine model can effectively capture the latent features of a DTI network and achieve excellent performance on predicting different types of DTIs, with the area under precision-recall curve up to 89.6. In addition, we demonstrate that integrating multiple types of DTIs can significantly outperform other predictions either by simply mixing multiple types of interactions without distinction or using only a single interaction type. Further tests show that our approach can infer a high fraction of novel DTIs that has been validated by known experiments in the literature or other databases. These results indicate that our approach can have highly practical relevance to DTI prediction and drug repositioning, and hence advance the drug discovery process.
Availability: Software and datasets are available on request.
Contact: zengjy321@tsinghua.edu.cn
Supplementary information: Supplementary data are available at Bioinformatics online.},
  timestamp = {2017-02-09T19:12:00Z},
  langid = {english},
  journaltitle = {Bioinformatics},
  author = {Wang, Yuhao and Zeng, Jianyang},
  date = {2013-01-07},
  pages = {i126--i134},
  file = {Wang and Zeng - 2013 - Predicting drug-target interactions using restrict:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZA7BUFDQ/Wang and Zeng - 2013 - Predicting drug-target interactions using restrict.pdf:application/pdf},
  groups = {review}
}

@article{waszak_population_2015,
  title = {Population {{Variation}} and {{Genetic Control}} of {{Modular Chromatin Architecture}} in {{Humans}}},
  volume = {162},
  issn = {1097-4172 (Electronic) 0092-8674 (Linking)},
  doi = {10.1016/j.cell.2015.08.001},
  shorttitle = {Population {{Variation}} and {{Genetic Control}} of {{Modular Chromatin Architecture}} in {{Humans}}},
  abstract = {Chromatin state variation at gene regulatory elements is abundant across individuals, yet we understand little about the genetic basis of this variability. Here, we profiled several histone modifications, the transcription factor (TF) PU.1, RNA polymerase II, and gene expression in lymphoblastoid cell lines from 47 whole-genome sequenced individuals. We observed that distinct cis-regulatory elements exhibit coordinated chromatin variation across individuals in the form of variable chromatin modules (VCMs) at sub-Mb scale. VCMs were associated with thousands of genes and preferentially cluster within chromosomal contact domains. We mapped strong proximal and weak, yet more ubiquitous, distal-acting chromatin quantitative trait loci (cQTL) that frequently explain this variation. cQTLs were associated with molecular activity at clusters of cis-regulatory elements and mapped preferentially within TF-bound regions. We propose that local, sequence-independent chromatin variation emerges as a result of genetic perturbations in cooperative interactions between cis-regulatory elements that are located within the same genomic domain.},
  timestamp = {2017-02-09T19:12:01Z},
  eprinttype = {pubmed},
  eprint = {26300124},
  number = {5},
  journaltitle = {Cell},
  shortjournal = {Cell},
  author = {Waszak, S. M. and Delaneau, O. and Gschwind, A. R. and Kilpinen, H. and Raghav, S. K. and Witwicki, R. M. and Orioli, A. and Wiederkehr, M. and Panousis, N. I. and Yurovsky, A. and Romano-Palumbo, L. and Planchon, A. and Bielser, D. and Padioleau, I. and Udin, G. and Thurnheer, S. and Hacker, D. and Hernandez, N. and Reymond, A. and Deplancke, B. and Dermitzakis, E. T.},
  date = {2015-08-27},
  pages = {1039--50},
  keywords = {*Gene Expression Regulation,*Genetic Variation,*Genome; Human,Chromatin/*chemistry/metabolism,Chromosomes; Human/chemistry,Genetics; Population,Humans,Quantitative Trait Loci,Regulatory Sequences; Nucleic Acid,Transcription Factors/metabolism},
  groups = {review}
}

@article{weyand_planet_2016,
  title = {{{PlaNet}} - {{Photo Geolocation}} with {{Convolutional Neural Networks}}},
  shorttitle = {{{PlaNet}} - {{Photo Geolocation}} with {{Convolutional Neural Networks}}},
  abstract = {Is it possible to build a system to determine the location where a photo was taken using just its pixels? In general, the problem seems exceptionally difficult: it is trivial to construct situations where no location can be inferred. Yet images often contain informative cues such as landmarks, weather patterns, vegetation, road markings, and architectural details, which in combination may allow one to determine an approximate location and occasionally an exact location. Websites such as GeoGuessr and View from your Window suggest that humans are relatively good at integrating these cues to geolocate images, especially en-masse. In computer vision, the photo geolocation problem is usually approached using image retrieval methods. In contrast, we pose the problem as one of classification by subdividing the surface of the earth into thousands of multi-scale geographic cells, and train a deep network using millions of geotagged images. While previous approaches only recognize landmarks or perform approximate matching using global image descriptors, our model is able to use and integrate multiple visible cues. We show that the resulting model, called PlaNet, outperforms previous approaches and even attains superhuman levels of accuracy in some cases. Moreover, we extend our model to photo albums by combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, we demonstrate that this model achieves a 50\% performance improvement over the single-image model.},
  timestamp = {2017-02-09T19:12:01Z},
  journaltitle = {arXiv:1602.05314 [cs]},
  author = {Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
  date = {2016-02-17},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  groups = {review}
}

@article{wu_investigating_2016,
  title = {Investigating Gated Recurrent Neural Networks for Speech Synthesis},
  shorttitle = {Investigating Gated Recurrent Neural Networks for Speech Synthesis},
  abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
  timestamp = {2017-02-09T19:12:01Z},
  journaltitle = {arXiv:1601.02539 [cs]},
  author = {Wu, Zhizheng and King, Simon},
  date = {2016-01-11},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  groups = {review}
}

@incollection{xie_beyond_2015,
  title = {Beyond {{Classification}}: {{Structured Regression}} for {{Robust Cell Detection Using Convolutional Neural Network}}},
  isbn = {3-319-24573-2},
  shorttitle = {Beyond {{Classification}}: {{Structured Regression}} for {{Robust Cell Detection Using Convolutional Neural Network}}},
  timestamp = {2017-02-09T19:12:01Z},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}–{{MICCAI}} 2015},
  publisher = {{Springer}},
  author = {Xie, Yuanpu and Xing, Fuyong and Kong, Xiangfei and Su, Hai and Yang, Lin},
  date = {2015},
  pages = {358--365},
  groups = {review}
}

@article{xiong_dynamic_2016,
  title = {Dynamic {{Memory Networks}} for {{Visual}} and {{Textual Question Answering}}},
  shorttitle = {Dynamic {{Memory Networks}} for {{Visual}} and {{Textual Question Answering}}},
  abstract = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the $\backslash$babi-10k text question-answering dataset without supporting fact supervision.},
  timestamp = {2017-02-09T19:12:01Z},
  journaltitle = {arXiv preprint arXiv:1603.01417},
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  date = {2016-03-04},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {Xiong et al. - 2016 - Dynamic Memory Networks for Visual and Textual Que:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3WA32R84/Xiong et al. - 2016 - Dynamic Memory Networks for Visual and Textual Que.pdf:application/pdf},
  groups = {review}
}

@article{xiong_human_2015,
  title = {The Human Splicing Code Reveals New Insights into the Genetic Determinants of Disease},
  volume = {347},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1254806},
  shorttitle = {The Human Splicing Code Reveals New Insights into the Genetic Determinants of Disease},
  abstract = {To facilitate precision medicine and whole-genome annotation, we developed a machine-learning technique that scores how strongly genetic variants affect RNA splicing, whose alteration contributes to many diseases. Analysis of more than 650,000 intronic and exonic variants revealed widespread patterns of mutation-driven aberrant splicing. Intronic disease mutations that are more than 30 nucleotides from any splice site alter splicing nine times as often as common variants, and missense exonic disease mutations that have the least impact on protein function are five times as likely as others to alter splicing. We detected tens of thousands of disease-causing mutations, including those involved in cancers and spinal muscular atrophy. Examination of intronic and exonic variants found using whole-genome sequencing of individuals with autism revealed misspliced genes with neurodevelopmental phenotypes. Our approach provides evidence for causal variants and should enable new discoveries in precision medicine.
INTRODUCTION Advancing whole-genome precision medicine requires understanding how gene expression is altered by genetic variants, especially those that are far outside of protein-coding regions. We developed a computational technique that scores how strongly genetic variants affect RNA splicing, a critical step in gene expression whose disruption contributes to many diseases, including cancers and neurological disorders. A genome-wide analysis reveals tens of thousands of variants that alter splicing and are enriched with a wide range of known diseases. Our results provide insight into the genetic basis of spinal muscular atrophy, hereditary nonpolyposis colorectal cancer, and autism spectrum disorder.
RATIONALE We used “deep learning” computer algorithms to derive a computational model that takes as input DNA sequences and applies general rules to predict splicing in human tissues. Given a test variant, which may be up to 300 nucleotides into an intron, our model can be used to compute a score for how much the variant alters splicing. The model is not biased by existing disease annotations or population data and was derived in such a way that it can be used to study diverse diseases and disorders and to determine the consequences of common, rare, and even spontaneous variants.
RESULTS Our technique is able to accurately classify disease-causing variants and provides insights into the role of aberrant splicing in disease. We scored more than 650,000 DNA variants and found that disease-causing variants have higher scores than common variants and even those associated with disease in genome-wide association studies (GWAS). Our model predicts substantial and unexpected aberrant splicing due to variants within introns and exons, including those far from the splice site. For example, among intronic variants that are more than 30 nucleotides away from any splice site, known disease variants alter splicing nine times as often as common variants; among missense exonic disease variants, those that least affect protein function are more than five times as likely as other variants to alter splicing. Autism has been associated with disrupted splicing in brain regions, so we used our method to score variants detected using whole-genome sequencing data from individuals with and without autism. Genes with high-scoring variants include many that have previously been linked with autism, as well as new genes with known neurodevelopmental phenotypes. Most of the high-scoring variants are intronic and cannot be detected by exome analysis techniques. When we scored clinical variants in spinal muscular atrophy and colorectal cancer genes, up to 94\% of variants found to alter splicing using minigene reporters were correctly classified.
CONCLUSION In the context of precision medicine, causal support for variants independent of existing whole-genome variant studies is greatly needed. Our computational model was trained to predict splicing from DNA sequence alone, without using disease annotations or population data. Consequently, its predictions are independent of and complementary to population data, GWAS, expression-based quantitative trait loci (QTL), and functional annotations of the genome. As such, our technique greatly expands the opportunities for understanding the genetic determinants of disease. View larger version: In this page In a new window Download PowerPoint Slide for Teaching “Deep learning” reveals the genetic origins of disease. A computational system mimics the biology of RNA splicing by correlating DNA elements with splicing levels in healthy human tissues. The system can scan DNA and identify damaging genetic variants, including those deep within introns. This procedure has led to insights into the genetics of autism, cancers, and spinal muscular atrophy.
Predicting defects in RNA splicing
Most eukaryotic messenger RNAs (mRNAs) are spliced to remove introns. Splicing generates uninterrupted open reading frames that can be translated into proteins. Splicing is often highly regulated, generating alternative spliced forms that code for variant proteins in different tissues. RNA-binding proteins that bind specific sequences in the mRNA regulate splicing. Xiong et al. develop a computational model that predicts splicing regulation for any mRNA sequence (see the Perspective by Guigó and Valcárcel). They use this to analyze more than half a million mRNA splicing sequence variants in the human genome. They are able to identify thousands of known disease-causing mutations, as well as many new disease candidates, including 17 new autism-linked genes.
Science, this issue 10.1126/science.1254806; see also p. 124},
  timestamp = {2017-02-09T19:12:01Z},
  langid = {english},
  journaltitle = {Science},
  author = {Xiong, Hui Y. and Alipanahi, Babak and Lee, Leo J. and Bretschneider, Hannes and Merico, Daniele and Yuen, Ryan K. C. and Hua, Yimin and Gueroussov, Serge and Najafabadi, Hamed S. and Hughes, Timothy R. and Morris, Quaid and Barash, Yoseph and Krainer, Adrian R. and Jojic, Nebojsa and Scherer, Stephen W. and Blencowe, Benjamin J. and Frey, Brendan J.},
  date = {2015-09-01},
  pages = {1254806},
  file = {Xiong et al. - 2015 - The human splicing code reveals new insights into:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZGGP3D5B/Xiong et al. - 2015 - The human splicing code reveals new insights into .pdf:application/pdf},
  groups = {review}
}

@article{xu_empirical_2015,
  title = {Empirical {{Evaluation}} of {{Rectified Activations}} in {{Convolutional Network}}},
  shorttitle = {Empirical {{Evaluation}} of {{Rectified Activations}} in {{Convolutional Network}}},
  abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart.},
  timestamp = {2017-02-09T19:12:03Z},
  journaltitle = {arXiv:1505.00853 [cs, stat]},
  author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
  date = {2015-05-04},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,notes,Statistics - Machine Learning},
  file = {Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/8FBVIMWD/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in C.pdf:application/pdf},
  groups = {review}
}

@article{xu_show_2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  timestamp = {2017-02-09T19:12:03Z},
  journaltitle = {arXiv preptin arXiv:1502.03044},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  date = {2015-02-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  file = {Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Genera:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/M2HUJRU4/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Genera.pdf:application/pdf},
  groups = {review}
}

@article{xu_inference_2007,
  title = {Inference of Genetic Regulatory Networks with Recurrent Neural Network Models Using Particle Swarm Optimization},
  volume = {4},
  issn = {1545-5963},
  shorttitle = {Inference of Genetic Regulatory Networks with Recurrent Neural Network Models Using Particle Swarm Optimization},
  timestamp = {2017-02-09T19:12:04Z},
  number = {4},
  journaltitle = {IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)},
  author = {Xu, Rui and Wunsch II, Donald and Frank, Ronald},
  date = {2007},
  pages = {681--692},
  groups = {review}
}

@inproceedings{xu_deep_2014,
  title = {Deep Learning of Feature Representation with Multiple Instance Learning for Medical Image Analysis},
  shorttitle = {Deep Learning of Feature Representation with Multiple Instance Learning for Medical Image Analysis},
  eventtitle = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
  timestamp = {2017-02-09T19:12:04Z},
  publisher = {{IEEE}},
  author = {Xu, Yan and Mo, Tao and Feng, Qiwei and Zhong, Peilin and Lai, Maode and Chang, Eric I},
  date = {2014},
  pages = {1626--1630},
  groups = {review}
}

@incollection{ghahramani_how_2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  shorttitle = {How Transferable Are Features in Deep Neural Networks?},
  timestamp = {2017-02-09T19:12:04Z},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  publisher = {{Curran Associates, Inc.}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  date = {2014},
  pages = {3320--3328},
  keywords = {notes},
  groups = {review}
}

@inproceedings{yu_roles_2010,
  title = {Roles of Pre-Training and Fine-Tuning in Context-Dependent {{DBN}}-{{HMMs}} for Real-World Speech Recognition},
  shorttitle = {Roles of Pre-Training and Fine-Tuning in Context-Dependent {{DBN}}-{{HMMs}} for Real-World Speech Recognition},
  timestamp = {2017-02-09T19:12:04Z},
  booktitle = {Proc. {{NIPS Workshop}} on {{Deep Learning}} and {{Unsupervised Feature Learning}}},
  author = {Yu, Dong and Deng, Li and Dahl, G.},
  date = {2010},
  file = {dbn4asr-nips2010:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/C8EDPQK5/dbn4asr-nips2010.pdf:application/pdf},
  groups = {review}
}

@article{zaremba_empirical_????,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  shorttitle = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  timestamp = {2017-02-09T19:12:04Z},
  author = {Zaremba, Wojciech and COM, GMAIL},
  keywords = {notes},
  file = {jozefowicz15:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/38ZB3BP8/jozefowicz15.pdf:application/pdf},
  groups = {review}
}

@article{zaremba_reinforcement_2015,
  title = {Reinforcement {{Learning Neural Turing Machines}}},
  shorttitle = {Reinforcement {{Learning Neural Turing Machines}}},
  abstract = {The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Reinforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first model that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.},
  timestamp = {2017-02-09T19:12:04Z},
  journaltitle = {arXiv:1505.00521 [cs]},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  date = {2015-05-04},
  keywords = {Computer Science - Learning},
  file = {Zaremba and Sutskever - 2015 - Reinforcement Learning Neural Turing Machines:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/FKN9RSIT/Zaremba and Sutskever - 2015 - Reinforcement Learning Neural Turing Machines.pdf:application/pdf},
  groups = {review}
}

@article{zaremba_recurrent_2014,
  title = {Recurrent {{Neural Network Regularization}}},
  shorttitle = {Recurrent {{Neural Network Regularization}}},
  abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
  timestamp = {2017-02-09T19:12:05Z},
  journaltitle = {arXiv:1409.2329 [cs]},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  date = {2014-09-08},
  keywords = {Computer Science - Neural and Evolutionary Computing,notes},
  file = {Zaremba et al. - 2014 - Recurrent Neural Network Regularization:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/56P8R87W/Zaremba et al. - 2014 - Recurrent Neural Network Regularization.pdf:application/pdf},
  groups = {review}
}

@article{zeiler_adadelta:_2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  timestamp = {2017-02-09T19:12:05Z},
  journaltitle = {arXiv:1212.5701 [cs]},
  author = {Zeiler, Matthew D.},
  date = {2012-12-22},
  keywords = {Computer Science - Learning},
  file = {Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/2CJD2NDW/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf:application/pdf},
  groups = {review}
}

@incollection{zeiler_visualizing_2014,
  title = {Visualizing and Understanding Convolutional Networks},
  isbn = {3-319-10589-2},
  shorttitle = {Visualizing and Understanding Convolutional Networks},
  timestamp = {2017-02-09T19:12:05Z},
  booktitle = {Computer Vision–{{ECCV}} 2014},
  publisher = {{Springer}},
  author = {Zeiler, Matthew D and Fergus, Rob},
  date = {2014},
  pages = {818--833},
  groups = {review}
}

@inproceedings{zhang_improving_2014,
  title = {Improving Multiview Face Detection with Multi-Task Deep Convolutional Neural Networks},
  shorttitle = {Improving Multiview Face Detection with Multi-Task Deep Convolutional Neural Networks},
  timestamp = {2017-02-09T19:12:05Z},
  booktitle = {Applications of {{Computer Vision}} ({{WACV}}), 2014 {{IEEE Winter Conference}} On},
  publisher = {{IEEE}},
  author = {Zhang, Cha and Zhang, Zhengyou},
  date = {2014},
  pages = {1036--1041},
  keywords = {notes},
  groups = {review}
}

@article{zhang_deep_2016,
  title = {A Deep Learning Framework for Modeling Structural Features of {{RNA}}-Binding Protein Targets},
  volume = {44},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkv1025},
  shorttitle = {A Deep Learning Framework for Modeling Structural Features of {{RNA}}-Binding Protein Targets},
  abstract = {RNA-binding proteins (RBPs) play important roles in the post-transcriptional control of RNAs. Identifying RBP binding sites and characterizing RBP binding preferences are key steps toward understanding the basic mechanisms of the post-transcriptional gene regulation. Though numerous computational methods have been developed for modeling RBP binding preferences, discovering a complete structural representation of the RBP targets by integrating their available structural features in all three dimensions is still a challenging task. In this paper, we develop a general and flexible deep learning framework for modeling structural binding preferences and predicting binding sites of RBPs, which takes (predicted) RNA tertiary structural information into account for the first time. Our framework constructs a unified representation that characterizes the structural specificities of RBP targets in all three dimensions, which can be further used to predict novel candidate binding sites and discover potential binding motifs. Through testing on the real CLIP-seq datasets, we have demonstrated that our deep learning framework can automatically extract effective hidden structural features from the encoded raw sequence and structural profiles, and predict accurate RBP binding sites. In addition, we have conducted the first study to show that integrating the additional RNA tertiary structural features can improve the model performance in predicting RBP binding sites, especially for the polypyrimidine tract-binding protein (PTB), which also provides a new evidence to support the view that RBPs may own specific tertiary structural binding preferences. In particular, the tests on the internal ribosome entry site (IRES) segments yield satisfiable results with experimental support from the literature and further demonstrate the necessity of incorporating RNA tertiary structural information into the prediction model. The source code of our approach can be found in https://github.com/thucombio/deepnet-rbp.},
  timestamp = {2017-02-09T19:12:05Z},
  langid = {english},
  journaltitle = {Nucleic Acids Research},
  author = {Zhang, Sai and Zhou, Jingtian and Hu, Hailin and Gong, Haipeng and Chen, Ligong and Cheng, Chao and Zeng, Jianyang},
  date = {2016-02-29},
  pages = {e32--e32},
  keywords = {notes},
  file = {Zhang et al. - 2016 - A deep learning framework for modeling structural:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KIDX48ZJ/Zhang et al. - 2016 - A deep learning framework for modeling structural .pdf:application/pdf},
  groups = {review}
}

@inproceedings{zhang_deep_2015,
  title = {Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis},
  isbn = {1-4503-3664-7},
  shorttitle = {Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis},
  eventtitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  timestamp = {2017-02-09T19:12:06Z},
  publisher = {{ACM}},
  author = {Zhang, Wenlu and Li, Rongjian and Zeng, Tao and Sun, Qian and Kumar, Sudhir and Ye, Jieping and Ji, Shuiwang},
  date = {2015},
  pages = {1475--1484},
  groups = {review}
}

@article{zhang_character-level_2015,
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  shorttitle = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  timestamp = {2017-02-09T19:12:06Z},
  journaltitle = {arXiv:1509.01626 [cs]},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  date = {2015-09-04},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,notes},
  file = {Zhang et al. - 2015 - Character-level Convolutional Networks for Text Cl:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/MK54NZ2E/Zhang et al. - 2015 - Character-level Convolutional Networks for Text Cl.pdf:application/pdf},
  groups = {review}
}

@article{zhang_sensitivity_2015,
  title = {A {{Sensitivity Analysis}} of (and {{Practitioners}}' {{Guide}} to) {{Convolutional Neural Networks}} for {{Sentence Classification}}},
  shorttitle = {A {{Sensitivity Analysis}} of (and {{Practitioners}}' {{Guide}} to) {{Convolutional Neural Networks}} for {{Sentence Classification}}},
  abstract = {Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on sentence classification tasks (Kim, 2014; Kalchbrenner et al., 2014). However, these models require practitioners to specify the exact model architecture and accompanying hyper-parameters, e.g., the choice of filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance (Kim, 2014). We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification. One important observation borne out by our experimental results is that researchers should report performance variances, as these can be substantial due to stochastic initialization and inference.},
  timestamp = {2017-02-09T19:12:06Z},
  journaltitle = {arXiv:1510.03820 [cs]},
  author = {Zhang, Ye and Wallace, Byron},
  date = {2015-10-13},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,notes},
  file = {Zhang and Wallace - 2015 - A Sensitivity Analysis of (and Practitioners' Guid:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/QICBGZ7R/Zhang and Wallace - 2015 - A Sensitivity Analysis of (and Practitioners' Guid.pdf:application/pdf},
  groups = {review}
}

@article{zhang_convex_2012,
  title = {A {{Convex Formulation}} for {{Learning Task Relationships}} in {{Multi}}-{{Task Learning}}},
  shorttitle = {A {{Convex Formulation}} for {{Learning Task Relationships}} in {{Multi}}-{{Task Learning}}},
  abstract = {Multi-task learning is a learning paradigm which seeks to improve the generalization performance of a learning task with the help of some other related tasks. In this paper, we propose a regularization formulation for learning the relationships between tasks in multi-task learning. This formulation can be viewed as a novel generalization of the regularization framework for single-task learning. Besides modeling positive task correlation, our method, called multi-task relationship learning (MTRL), can also describe negative task correlation and identify outlier tasks based on the same underlying principle. Under this regularization framework, the objective function of MTRL is convex. For efficiency, we use an alternating method to learn the optimal model parameters for each task as well as the relationships between tasks. We study MTRL in the symmetric multi-task learning setting and then generalize it to the asymmetric setting as well. We also study the relationships between MTRL and some existing multi-task learning methods. Experiments conducted on a toy problem as well as several benchmark data sets demonstrate the effectiveness of MTRL.},
  timestamp = {2017-02-09T19:12:07Z},
  journaltitle = {arXiv:1203.3536 [cs, stat]},
  author = {Zhang, Yu and Yeung, Dit-Yan},
  date = {2012-03-15},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  groups = {review}
}

@article{zhou_predicting_2015,
  title = {Predicting Effects of Noncoding Variants with Deep Learning-Based Sequence Model},
  volume = {12},
  issn = {1548-7105 (Electronic) 1548-7091 (Linking)},
  doi = {10.1038/nmeth.3547},
  shorttitle = {Predicting Effects of Noncoding Variants with Deep Learning-Based Sequence Model},
  abstract = {Identifying functional effects of noncoding variants is a major challenge in human genetics. To predict the noncoding-variant effects de novo from sequence, we developed a deep learning-based algorithmic framework, DeepSEA (http://deepsea.princeton.edu/), that directly learns a regulatory sequence code from large-scale chromatin-profiling data, enabling prediction of chromatin effects of sequence alterations with single-nucleotide sensitivity. We further used this capability to improve prioritization of functional variants including expression quantitative trait loci (eQTLs) and disease-associated variants.},
  timestamp = {2017-02-09T19:12:07Z},
  eprinttype = {pubmed},
  eprint = {26301843},
  number = {10},
  journaltitle = {Nat Methods},
  shortjournal = {Nature methods},
  author = {Zhou, J. and Troyanskaya, O. G.},
  date = {2015-10},
  pages = {931--4},
  keywords = {*Algorithms,*Polymorphism; Single Nucleotide,*Quantitative Trait Loci,Chromatin/*genetics,Epigenomics,Genome; Human,Hepatocyte Nuclear Factor 3-alpha/genetics,Humans,Models; Genetic,Mutation,Regulatory Sequences; Nucleic Acid,RNA; Untranslated,Support Vector Machine,Transcription Factors/genetics/metabolism},
  groups = {review}
}

@article{huang_profiling_2010,
  title = {Profiling {{DNA Methylomes}} from {{Microarray}} to {{Genome}}-{{Scale Sequencing}}},
  volume = {9},
  issn = {1533-0346},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3011833/},
  abstract = {DNA cytosine methylation is a central epigenetic modification which plays critical roles in cellular processes including genome regulation, development and disease. Here, we review current and emerging microarray and next-generation sequencing based technologies that enhance our knowledge of DNA methylation profiling. Each methodology has limitations and their unique applications, and combinations of several modalities may help build the entire methylome. With advances on next-generation sequencing technologies, it is now possible to globally map the DNA cytosine methylation at single-base resolution, providing new insights into the regulation and dynamics of DNA methylation in genomes.},
  timestamp = {2017-02-18T19:08:16Z},
  number = {2},
  journaltitle = {Technology in cancer research \& treatment},
  shortjournal = {Technol Cancer Res Treat},
  author = {Huang, Yi-Wen and Huang, Tim H.-M. and Wang, Li-Shu},
  urldate = {2017-02-18},
  date = {2010-04},
  pages = {139--147},
  file = {PubMed Central Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/ZK2KEC9W/Huang et al. - 2010 - Profiling DNA Methylomes from Microarray to Genome.pdf:application/pdf},
  groups = {bio},
  eprinttype = {pmid},
  eprint = {20218736},
  pmcid = {PMC3011833}
}

@article{yong_profiling_2016,
  title = {Profiling Genome-Wide {{DNA}} Methylation},
  volume = {9},
  issn = {1756-8935},
  url = {http://dx.doi.org/10.1186/s13072-016-0075-3},
  doi = {10.1186/s13072-016-0075-3},
  abstract = {DNA methylation is an epigenetic modification that plays an important role in regulating gene expression and therefore a broad range of biological processes and diseases. DNA methylation is tissue-specific, dynamic, sequence-context-dependent and trans-generationally heritable, and these complex patterns of methylation highlight the significance of profiling DNA methylation to answer biological questions. In this review, we surveyed major methylation assays, along with comparisons and biological examples, to provide an overview of DNA methylation profiling techniques. The advances in microarray and sequencing technologies make genome-wide profiling possible at a single-nucleotide or even a single-cell resolution. These profiling approaches vary in many aspects, such as DNA input, resolution, genomic region coverage, and bioinformatics analysis, and selecting a feasible method requires knowledge of these methods. We first introduce the biological background of DNA methylation and its pattern in plants, animals and fungi. We present an overview of major experimental approaches to profiling genome-wide DNA methylation and hydroxymethylation and then extend to the single-cell methylome. To evaluate these methods, we outline their strengths and weaknesses and perform comparisons across the different platforms. Due to the increasing need to compute high-throughput epigenomic data, we interrogate the computational pipeline for bisulfite sequencing data and also discuss the concept of identifying differentially methylated regions (DMRs). This review summarizes the experimental and computational concepts for profiling genome-wide DNA methylation, followed by biological examples. Overall, this review provides researchers useful guidance for the selection of a profiling method suited to specific research questions.},
  timestamp = {2017-02-18T19:08:26Z},
  journaltitle = {Epigenetics \& Chromatin},
  shortjournal = {Epigenetics \& Chromatin},
  author = {Yong, Wai-Shin and Hsu, Fei-Man and Chen, Pao-Yang},
  urldate = {2017-02-18},
  date = {2016},
  pages = {26},
  keywords = {bisulfite sequencing,DNA Methylation,good,Hydroxymethylation,Methylome,RRBS,Single-cell,WGBS},
  file = {notes.pptx:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/IMJW8D2W/notes.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation;Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/XMKNXGBI/Yong et al. - 2016 - Profiling genome-wide DNA methylation.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/VQGKQUA6/s13072-016-0075-3.html:text/html},
  groups = {bio}
}

@article{schwartzman_single-cell_2015,
  title = {Single-Cell Epigenomics: Techniques and Emerging Applications},
  volume = {16},
  rights = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v16/n12/full/nrg3980.html},
  doi = {10.1038/nrg3980},
  shorttitle = {Single-Cell Epigenomics},
  abstract = {Epigenomics is the study of the physical modifications, associations and conformations of genomic DNA sequences, with the aim of linking these with epigenetic memory, cellular identity and tissue-specific functions. While current techniques in the field are characterizing the average epigenomic features across large cell ensembles, the increasing interest in the epigenetics within complex and heterogeneous tissues is driving the development of single-cell epigenomics. We review emerging single-cell methods for capturing DNA methylation, chromatin accessibility, histone modifications, chromosome conformation and replication dynamics. Together, these techniques are rapidly becoming a powerful tool in studies of cellular plasticity and diversity, as seen in stem cells and cancer.},
  timestamp = {2017-02-20T13:39:13Z},
  langid = {english},
  number = {12},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Schwartzman, Omer and Tanay, Amos},
  urldate = {2017-02-20},
  date = {2015-12},
  pages = {716--726},
  keywords = {Chromatin,Chromosome conformation capture-based methods,DNA Methylation,Epigenetics analysis,Epigenomics,next-generation sequencing},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/6DIZW39N/Schwartzman and Tanay - 2015 - Single-cell epigenomics techniques and emerging a.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/DNZA436Z/nrg3980.html:text/html},
  groups = {bio}
}

@article{plongthongkum_advances_2014,
  title = {Advances in the Profiling of {{DNA}} Modifications: Cytosine Methylation and Beyond},
  volume = {15},
  rights = {© 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1471-0056},
  url = {http://www.nature.com/nrg/journal/v15/n10/full/nrg3772.html},
  doi = {10.1038/nrg3772},
  shorttitle = {Advances in the Profiling of {{DNA}} Modifications},
  abstract = {Chemical modifications of DNA have been recognized as key epigenetic mechanisms for maintenance of the cellular state and memory. Such DNA modifications include canonical 5-methylcytosine (5mC), 5-hydroxymethylcytosine (5hmC), 5-formylcytosine (5fC) and 5-carboxycytosine (5caC). Recent advances in detection and quantification of DNA modifications have enabled epigenetic variation to be connected to phenotypic consequences on an unprecedented scale. These methods may use chemical or enzymatic DNA treatment, may be targeted or non-targeted and may utilize array-based hybridization or sequencing. Key considerations in the choice of assay are cost, minimum sample input requirements, accuracy and throughput. This Review discusses the principles behind recently developed techniques, compares their respective strengths and limitations and provides general guidelines for selecting appropriate methods for specific experimental contexts.},
  timestamp = {2017-02-20T14:21:22Z},
  langid = {english},
  number = {10},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  author = {Plongthongkum, Nongluk and Diep, Dinh H. and Zhang, Kun},
  urldate = {2017-02-20},
  date = {2014-10},
  pages = {647--661},
  keywords = {DNA Methylation,epigenetics,Epigenetics analysis,Methylation analysis},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/3ZF2PXH2/Plongthongkum et al. - 2014 - Advances in the profiling of DNA modifications cy.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/XZRANVSM/nrg3772.html:text/html},
  groups = {bio}
}

@article{guo_profiling_2015,
  title = {Profiling {{DNA}} Methylome Landscapes of Mammalian Cells with Single-Cell Reduced-Representation Bisulfite Sequencing},
  volume = {10},
  rights = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1754-2189},
  url = {http://www.nature.com/nprot/journal/v10/n5/abs/nprot.2015.039.html},
  doi = {10.1038/nprot.2015.039},
  abstract = {The heterogeneity of DNA methylation within a population of cells necessitates DNA methylome profiling at single-cell resolution. Recently, we developed a single-cell reduced-representation bisulfite sequencing (scRRBS) technique in which we modified the original RRBS method by integrating all the experimental steps before PCR amplification into a single-tube reaction. These modifications enable scRRBS to provide digitized methylation information on ∼1 million CpG sites within an individual diploid mouse or human cell at single-base resolution. Compared with the single-cell bisulfite sequencing (scBS) technique, scRRBS covers fewer CpG sites, but it provides better coverage for CpG islands (CGIs), which are likely to be the most informative elements for DNA methylation. The entire procedure takes ∼3 weeks, and it requires strong molecular biology skills.
View full text},
  timestamp = {2017-02-20T19:17:12Z},
  langid = {english},
  number = {5},
  journaltitle = {Nature Protocols},
  shortjournal = {Nat. Protocols},
  author = {Guo, Hongshan and Zhu, Ping and Guo, Fan and Li, Xianlong and Wu, Xinglong and Fan, Xiaoying and Wen, Lu and Tang, Fuchou},
  urldate = {2017-02-20},
  date = {2015-05},
  pages = {645--659},
  keywords = {DNA Methylation,Epigenomics,Methylation analysis,next-generation sequencing},
  file = {Full Text PDF:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/KJBRHMW5/Guo et al. - 2015 - Profiling DNA methylome landscapes of mammalian ce.pdf:application/pdf;Snapshot:/Users/angermue/Library/Application Support/Zotero/Profiles/4wv6rzc5.default/zotero/storage/W8N47GAA/nprot.2015.039.html:text/html},
  groups = {seq,seq}
}

@comment{jabref-meta: databaseType:biblatex;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:bio\;0\;;
2 ExplicitGroup:seq\;0\;;
1 ExplicitGroup:seq\;0\;;
1 ExplicitGroup:DeepCpG\;0\;;
2 ExplicitGroup:methods\;0\;;
3 ExplicitGroup:CGI\;0\;;
3 ExplicitGroup:seq\;0\;;
1 ExplicitGroup:methods\;0\;;
2 ExplicitGroup:CGI\;0\;;
2 ExplicitGroup:seq\;0\;;
1 ExplicitGroup:CGI\;0\;;
1 ExplicitGroup:seq\;0\;;
1 ExplicitGroup:review\;0\;;
}

